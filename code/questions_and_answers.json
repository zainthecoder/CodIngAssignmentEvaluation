[
    {
        "file_name": "Intro2NLP_Assignment_1_Aylin_Gheisar.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef extract_words_tokens(any_string):\n    # To find the number of words we eliminate scpecial characters: ! , : ? . ( ) # \" from the string using the regex module\n    # We then split the resulting string by whitespace characters to get the word list\n    no_special = re.sub(r'[!,:\\?\\.\\(\\)#\\.\\\"]', ' ', any_string)\n    word_list = list(filter(bool, re.split(r\"\\s\", no_special)))\n    \n    # We simply store any non-whitespace character of the original string to get the token list\n    char_list = [ch for ch in list(any_string) if ch not in [\" \", \"\\t\"]]\n    \n    print(any_string, \":\", \"num_words:\", len(word_list), \"and\", \"num_tokens:\", len(char_list), \"respectively\")\n# Test for Task 1.1\ns = \"\"\"The first nine lines stores the first paragraph of the story, each line consisting of a series of character symbols. These elements don\u2019t contain any metadata or information to tell us which characters are words and which aren\u2019t: #Identifying these \"kinds\" of boundaries between words is where the process of tokenization comes in!\n\nIn tokenization, we take an input (a string) and a token type (a meaningful unit of text, such as a word) and split the input into pieces (tokens) that correspond to the type (Manning, Raghavan, and Sch\u00fctze 2008)? Figure 2.1 outlines this process!\"\"\"\nextract_words_tokens(s)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Aylin_Gheisar.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # This task can be better achieved by compiling the corpus into a more processable data-type (dict)\n    # We keep the words as the key and the lemma in the value of the dictionary\n    # We must note that as some of the words have multiple lemmas, we keep the lemmas in a list (\"belying\": [\"belie\", \"bely\"])\n    with open(file_name, 'r', encoding='utf-8-sig') as f:\n        corpus_dict = {}\n        for line in f.read().split('\\n'):\n            if line:\n                if line.split('\\t')[1] in corpus_dict:\n                    corpus_dict[line.split('\\t')[1]].append(line.split('\\t')[0])\n                else:\n                    corpus_dict[line.split('\\t')[1]] = [line.split('\\t')[0]]\n        \n    # We split the words by whitespaces\n    word_list = list(filter(bool, re.split(r\"\\s\", any_string)))\n    \n    # Lemmas of the words are looked up in the corpus, in case of multiple lemmas, they are concatinated by a |\n    lemmatized = {}\n    for word in word_list:\n        lemmatized[word] = '|'.join(corpus_dict[word])\n        \n    print(lemmatized)\n# Test for Task 1.2\nlemmatize(\"belying opening\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Aylin_Gheisar.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # Like Task 1.1 we get the word list\n    no_special = re.sub(r'[!,:\\?\\.\\(\\)#\\.\\\"]', ' ', any_string)\n    word_list = list(filter(bool, re.split(r\"\\s\", no_special)))\n    \n    stemmed_words = []\n    for word in word_list:\n        # A word is assumed to be the stem for itself, unless if it fits the following criteria\n        stemmed = word\n        \n        # infinitive:\n        # verbs ending with -y that do not proceed a vowel are replaced by -i (play -> play - study -> studi)\n        # -e postfixes are removed (decide -> decid)\n        if re.findall(r\"[^euioa]y$\", word):\n            stemmed = word[:-1] + \"i\"\n        elif re.findall(r\"e$\", word):\n            stemmed = word[:-1]\n\n        # simple third person:\n        # -es postfixes are removed\n        # -s postfixes are removed (pulls -> pull)\n        elif re.findall(r\"es$\", word):\n            stemmed = word[:-2]\n        elif re.findall(r\"s$\", word):\n            stemmed = word[:-1]\n\n        # continous\n        # -ing postfix is removed and then infinitive processing is redone (studying -> study -> studi)\n        # (agreeing -> agree -> agre)\n        elif re.findall(r\"ing$\", word):\n            tmp = word[:-3]\n\n            if re.findall(r\"[^euioa]y$\", tmp):\n                stemmed = tmp[:-1] + \"i\"\n            elif re.findall(r\"e$\", tmp):\n                stemmed = tmp[:-1]\n            else:\n                stemmed = tmp\n                \n        # past simple\n        # -ed postfix is removed\n        elif re.findall(r\"ed$\", word):\n            stemmed = word[:-2]\n\n        stemmed_words.append(stemmed)\n        \n    print(' '.join(stemmed_words))\n# Test for Task 1.3\nstemmer(\"decide decides decided deciding\")\nstemmer(\"study studies studying studied\")\nstemmer(\"love loves loved loving\")\nstemmer(\"raise raises raising raised\")\nstemmer(\"agree agrees agreeing agreed\")\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Aksa_Aksa.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef extract_words_tokens(any_string):\n    any_string = any_string.strip() #remove starting and trailing whitespace characters\n    any_string = re.sub('\\s+', ' ', any_string) #remove extra whitespace characters\n\n    filtered_string = re.sub('[^A-z0-9 ]', '', any_string).strip() #remove special characters to count number of words\n\n    words = filtered_string.split(' ')\n    num_words = len(words)\n\n    characters = list(re.sub(' ', '', any_string))\n    num_tokens = len(characters)\n\n    print('words', words)\n    print('characters', characters)\n    print()\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nsentence = 'This is a sample     string, and it counts the numbers of words and characters!'\nextract_words_tokens(sentence)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Aksa_Aksa.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    #here comes your code\n    any_string = any_string.strip() #remove starting and trailing whitespace characters\n    any_string = re.sub('\\s+', ' ', any_string) #remove extra whitespace characters\n\n    words = any_string.split(' ')\n    lemmatize_dict = {}\n\n    #read the file and create a mapping of word and its lemma\n    with open(file_name, encoding=\"utf-8-sig\") as f:\n      for line in f:\n        lemma, word = line.strip().split('\\t')\n        lemmatize_dict[word] = lemma\n\n    dictionary_of_lemmatized_words = {}\n\n    for word in words:\n      #get the lemma of the word from the dictionary, if it doesn't exist, then use the same word\n      lemma = lemmatize_dict.get(word.lower(), word)\n      dictionary_of_lemmatized_words[word] = lemma\n\n    return dictionary_of_lemmatized_words\nfilename = 'lemmatization-en.txt'\nsentences = [\n    'The striped bats are hanging on their feet for best',\n    'The cats are running in the garden',\n    'He better understands the process now',\n    'She happily danced through the meadow',\n    'The fox Jumps over the lazy dogs',\n]\n\nfor sentence in sentences:\n  print(lemmatize(sentence, filename))\n  print()"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Aksa_Aksa.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\ndef applyRules(word, rules):\n  base = word\n  for rule in rules:\n    if word.endswith(rule):\n      if rule in ['ies', 'ying', 'y', 'yed', 'ys']:\n        base = word[: -len(rule)]\n        base = base + 'i'\n      elif rule in ['ing', 'ed', 'es', 's']:\n        base = word[: -len(rule)]\n\n      return base\n  return base\ndef stemmer(any_string):\n    any_string = any_string.strip() #remove starting and trailing whitespace characters\n    any_string = re.sub('\\s+', ' ', any_string) #remove extra whitespace characters\n\n    filtered_string = re.sub('[^A-z0-9 ]', '', any_string).strip() #remove special character to count number of words\n    words = filtered_string.split(' ')\n\n    stems = {}\n    rules = ['ies', 'ying', 'ing', 'y', 'yed', 'ed', 'es', 'ys', 's']\n\n\n    # Description of the rules implemented\n\n    # ies --> keep i only\n    # ying --> replace it with y, example: playing -> plai, studying -> studi, flying -> fli\n    # ing --> remove ing, example: reading -> read,\n    # y --> replace y with i, example: play -> plai, study -> studi, fly -> fli\n    # yed --> replace it with y, example: played -> plai, stayed -> stai\n    # ed --> remove ed, example: studied -> studi, flied -> fli, asked -> ask\n    # es --> remove es, example: does -> do\n    # ys --> replace it with i, example: plays: play, stays: stay\n    # s --> remove s, example: reads -> read\n\n    #create a mapping of the word and its stem\n    for word in words:\n      stem = applyRules(word.lower(), rules)\n      stems[word.lower()] = stem\n\n    print('stems', stems)\n\n\n    #replace the actual string with the stem\n    stemmed_string = any_string\n    for word in words:\n      stemmed_string = re.sub(r\"\\b%s\\b\" % word , stems[word.lower()], stemmed_string)\n\n    return(print(stemmed_string))\nstemmer('study, studies, studying, studied')\nprint()\nstemmer('play, plays, played, playing')\nprint()\nstemmer('stay, stays, stayed, staying')\nprint()\nstemmer('fly, flies, flied, fly')\nprint()\nstemmer('read, reads, read, reading')\nprint()\nstemmer('I am studied yesterday. I am now studying, as well. I will study tommorrow as well, because amna also studies.')\nimport re\n\ndef is_vowel(ch):\n  vowels = ['a', 'e', 'i', 'o', 'u']\n  return (True if ch.lower() in vowels else False)\n\n\ndef applyRulesV2(word, rules):\n  base = word\n  for rule in rules:\n    if word.endswith(rule):\n      if rule == 'ies':\n        base = word[: -len(rule) + 1]\n      elif rule == 'ing':\n        base = word[: -len(rule)]\n        base = applyRulesV2(base, rules)\n      elif rule == 'y':\n        is2LastVowel = is_vowel(word[-2])\n        if is2LastVowel:\n          base = word\n        else:\n          base = word[: -len(rule)]\n          base = base + 'i'\n      elif rule == 'ied':\n        trimmed =  word[: -len(rule)]\n        if len(trimmed) == 1:\n          base = word[: -1]\n        else:\n          base = word[: -len(rule) + 1]\n      elif rule == 'ed':\n        trimmed =  word[: -len(rule)]\n        if len(trimmed) == 1:\n          base = word\n        else:\n          base = word[: -len(rule)]\n      elif rule == 'es':\n        base =  word[: -len(rule)]\n      elif rule == 's':\n        base = word[: -len(rule)]\n\n      return base\n  return base\ndef stemmerV2(any_string):\n    any_string = any_string.strip() #remove starting and trailing whitespace characters\n    any_string = re.sub('\\s+', ' ', any_string) #remove extra whitespace characters\n\n    filtered_string = re.sub('[^A-z0-9 ]', '', any_string) #remove special character to count number of words\n    words = filtered_string.split(' ')\n\n    stems = {}\n    rules = ['ies', 'ing', 'y', 'ied', 'ed', 'es', 's']\n\n\n    # Description of the rules implemented\n\n    #ies --> keep i only\n    # ing --> removing ing and then run the algorithm again on the basic word\n    # Vy --> Vy (play -> play, clay -> clay) V is a prefix ending with vowel\n    # Cy --> Ci (fry -> fri, study -> studi) C is a prefix ending with a consonant\n    # ied --> keep i only, except when length of the base word after removing is 1, then remove only d.\n      ## studied -> studi\n      ## died -> die\n    # ed --> remove ed, except when length of the base word after removing is 1, then keep ed.\n      ## played -> play\n      ## fed -> fed\n    #es --> remove es\n    #s --> remove s\n\n    for word in words:\n      stem = applyRulesV2(word.lower(), rules)\n      stems[word.lower()] = stem\n\n    stemmed_string = any_string\n    print('stems', stems)\n    for word in words:\n      stemmed_string = re.sub(r\"\\b%s\\b\" % word , stems[word.lower()], stemmed_string)\n\n    return(print(stemmed_string))\nstemmerV2('study, studies, studying, studied')\nprint()\nstemmerV2('play, plays, played, playing')\nprint()\nstemmerV2('plays, studies, does, reads, flies')\nprint()\nstemmerV2('I am studied yesterday. I am now studying, too. I will study tommorrow, because amna also studies.')"
    },
    {
        "file_name": "Assignment_1_Valdrin_Smakaj.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # Split the string into words\n    words = any_string.translate(str.maketrans('', '', '!,.:')).split()\n    num_tokens = len(words)\n\n    \n    #               Im not sure if this was meant for counting tokens or the one I currently implemented above\n    #               Im providing both examples as proof I can do\n\n    # Counting tokens by iterating over each character in the string\n    # but excluding punctuation marks as specified for character-based tokenization.\n    \n    #                   num_tokens = len([char for char in any_string if char.isalnum()])\n    \n    # Use a set to count the number of distinct word types\n    num_words = len(set(word.lower() for word in words))\n\n    # The structure of the return as specified\n    return print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\")"
    },
    {
        "file_name": "Assignment_1_Valdrin_Smakaj.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_path):\n    # create the lemma dictionary for the file input\n    lemma_dict = {}\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            # Split the line by tab to separate the inflected form from the lemma\n            parts = line.strip().split('\\t')\n            if len(parts) == 2:\n                lemma, inflected = parts\n                # If the lemma is already a key in the dictionary, append the inflected form to its list\n                if lemma in lemma_dict:\n                    lemma_dict[lemma].append(inflected)\n                else:\n                    # If the lemma is not in the dictionary, add it and set its value to a new list with the inflected form\n                    lemma_dict[lemma] = [inflected]\n\n    # Tokenize the input string\n    words = any_string.split()\n    \n    # Create a dictionary to store the words and their lemmas\n    dictionary_of_lemmatized_words = {}\n    \n    for word in words:\n    # Find the lemma for the word by searching through the dictionary values\n        for lemma, inflections in lemma_dict.items():\n            if word in inflections:\n                dictionary_of_lemmatized_words[word] = lemma\n                break\n        else:\n            # If the word is not found as an inflected form, keep it as is\n            dictionary_of_lemmatized_words[word] = word\n    \n    return print(dictionary_of_lemmatized_words)"
    },
    {
        "file_name": "Assignment_1_Valdrin_Smakaj.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # Define the stem rules\n    suffix_rules = {\n        'ying' : 'i',\n        'ies': 'i',   \n        'ing': 'i',  \n        'ied': 'i',\n        'y' : 'i',  \n        'ss': 's',\n        'es': '',\n        'ies': 'y',\n        'ed': '',\n        'ing': '',\n        'en': '',\n        'er': '',\n        'est': '',\n        'ize': '',\n        'ise': '',\n        'ify': '',\n        'ate': '',\n        'ble': '',\n        'al': '',\n        'ful': '',\n        'ish': '',\n        'less': '',\n        'ness': '',\n        'ment': '',\n        'tion': '',\n        'sion': '',\n        'ity': '',\n        'y': '',\n        'ly': '',\n    }\n\n    # Tokenize the input string\n    words = any_string.translate(str.maketrans('', '', '!,.:')).split()\n\n    # Create a list to store the stemmed words\n    stemmed_words = []\n\n    # Apply the stemming rules to each word\n    for word in words:\n        for suffix, replacement in suffix_rules.items():\n            if word.endswith(suffix):\n                # Cut the word from the end by the length of the suffix and add the replacement corresponding \n                word = word[:-len(suffix)] + replacement\n                break\n        stemmed_words.append(word)\n\n    # Join the stemmed words into a string\n    stemmed_string = ' '.join(stemmed_words)\n\n    # Return the stemmed string\n    return print(stemmed_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_SonaJabrayilova_LeylaHashimli.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # Count the number of words\n    # delete special characters (only keep letters and numbers) only for getting # of words\n    updated_any_string = re.sub('[^A-Za-z0-9]+', ' ', any_string)\n    num_words = len(set(updated_any_string.split()))\n\n    # Tokenize based on characters (including punctuation)\n    num_tokens = len(set(any_string))\n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_SonaJabrayilova_LeylaHashimli.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # tokenize string by using whitespace as the seperator\n    token_list = any_string.split()\n    \n    # read lemmatization file\n    lemmatization_df = pd.read_csv(file_name, sep='\\t', names=['lemmatized_version', 'original_version'])\n    \n    # get lemmatized words given only in input string and convert to dictionary format\n    filtered_lemmatization_df = lemmatization_df[lemmatization_df['original_version'].isin(token_list)]\n    dictionary_of_lemmatized_words = filtered_lemmatization_df.set_index('original_version')['lemmatized_version'].to_dict()\n    \n    return(print(dictionary_of_lemmatized_words))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_SonaJabrayilova_LeylaHashimli.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # define Stemmer (by nltk)\n    stemmer = PorterStemmer()\n    \n    # delete special characters (only keep letters and numbers) \n    any_string = re.sub('[^A-Za-z0-9]+', ' ', any_string)\n    # using whitespace as seperator, get word list and apply stemmer\n    stemmed_string_list = [stemmer.stem(word) for word in any_string.split()]\n    \n    # given stemmed words list, form string again\n    stemmed_string = ' '.join(stemmed_string_list)\n    \n    return(print(stemmed_string))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Bruno_Scheider.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from string import punctuation\n\ndef extract_words_tokens(any_string):\n    # list all words including special chars\n    words = any_string.split(\" \")\n    num_words = len(words)\n    chars_with_space = list(any_string)\n    #chars_without_space = list(any_string.replace(\" \", \"\"))\n    num_tokens = len(chars_with_space)\n    \n    ##version without special_chars\n    any_string_wo_punctation = any_string\n    for p in set(punctuation):\n        any_string_wo_punctation = any_string_wo_punctation.replace(p, \"\")\n    words_wo_p = any_string_wo_punctation.split(\" \")\n    num_words_wo_p = len(words_wo_p)\n    return num_words_wo_p, words_wo_p, num_tokens\n\nany_string = \"any string, test!\"\nnum_words, words, num_tokens = extract_words_tokens(any_string)\nprint(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Bruno_Scheider.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import numbers\n\ndef lemmatize(any_string, file_name):\n    #here comes your code\n    _, words, _ = extract_words_tokens(any_string)\n    dictionary_of_lemmatized_words = {}\n    \n    with open(file_name) as f:\n        for i, line in enumerate(f):\n                \n            (key, val) = line.split()\n            \n            if(val in words):\n                if (i==0) and len(key)==2:\n                    ## weird special case for the beginning of the document,\n                    ## this only works this way because we know, that the first entry key should be len=1\n                    key = key[1]\n                dictionary_of_lemmatized_words[val] = key\n                \n    return dictionary_of_lemmatized_words\nl = lemmatize(\"millions issued \u00f6alskdfj first second third hundred\", 'lemmatization-en.txt')\nprint(l)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Bruno_Scheider.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "#!pip install nltk\nfrom nltk.stem import PorterStemmer\n\ndef stemmer(any_string):\n    _, words, _ = extract_words_tokens(any_string)\n    porter_stemmer = PorterStemmer()\n    stemmed_words = [porter_stemmer.stem(w) for w in words]\n    return stemmed_words\n\nany_string = \"study, studies, studying, studied\"\nprint(stemmer(any_string))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Timon_Oerder.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef extract_words_tokens(any_string):\n    \n    cleaned_string = re.sub(r'[^a-zA-Z\\s]', '', any_string)\n    num_words = len(cleaned_string.split())\n    # I supposed words in this context are no numbers and tokens are all words and numbers included, I basically split at whitespaces every time.\n\n    num_tokens = len(re.findall(r'\\w+', any_string))\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Timon_Oerder.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name='./lemmatization-en.txt'):\n    any_string = any_string.lower()\n    dictionary_of_lemmatized_words = {}\n    with open(file_name, 'r', encoding='utf-8-sig') as file:\n        for line in file:\n            line = line.strip().split('\\t')\n            dictionary_of_lemmatized_words[line[1]] = line[0]\n\n    # newany_string = \"\"\n    # for word in re.findall(r'\\w+', any_string):\n    #     cleanedword = re.sub(r'[^a-zA-Z\\s]', '', word)\n    #     if cleanedword in dictionary_of_lemmatized_words.keys():\n    #         newany_string = newany_string + \" \" + dictionary_of_lemmatized_words[cleanedword]\n    #     else:\n    #         newany_string = newany_string + \" \" + cleanedword\n    # print(newany_string)\n    # return(newany_string)\n\n    # I am not sure if I should return the dictionary or the string, after the string is lemmatized\n    # Also I would return the dictionary not a print of the dictionary\n    # return(dictionary_of_lemmatized_words)\n\n    return(print(dictionary_of_lemmatized_words))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Timon_Oerder.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    cleaned_string = re.sub(r'[^a-zA-Z0-9\\s]', '', any_string)\n    \n    stemmed_string = \"\"\n   \n    for word in re.findall(r'\\w+', cleaned_string):\n        if word.endswith('ies'):\n            word = word[:-3] + 'i'\n        if word.endswith('ying'):\n            word = word[:-4] + 'i'\n        if word.endswith('ied'):\n            word = word[:-3] + 'i'\n        if word.endswith('y'):\n            word = word[:-1] + 'i'\n\n        stemmed_string = stemmed_string + \" \" + word\n\n    # obviously this rule is also catching nouns like \"cities\", \"countries\" or \"Mondays\" too \n    # But I am not sure how to exlcude every other word form except verbs\n            \n    return(print(stemmed_string))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_RaoRohilPrakash.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    #here comes your code\n    # Counting words\n    num_words = len(re.findall(r'\\b\\w+\\b', any_string))\n    \n    # Counting tokens using character-based tokenization\n    num_tokens = len(any_string)\n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nimport re\ninput_string = \"Hello, this is an example sentence!\"\nprint(len(input_string))\nextract_words_tokens(input_string)\n## Using nltk\n\nimport nltk\n\nnltk.download('punkt')  # Download the punkt tokenizer data if not already downloaded\n\ndef extract_words_tokens(any_string):\n    # Tokenizing words\n    words = nltk.word_tokenize(any_string)\n    num_words = len(words)\n    print(words)\n    \n    # Tokenizing characters\n    tokens = list(any_string)\n    num_tokens = len(tokens)\n\n    return print(f\"{any_string}: num_words: {num_words} and num_tokens: {num_tokens} respectively\")\n\n# Example usage:\ninput_string = \"Hello, this is an example sentence!\"\nprint(len(input_string))\nextract_words_tokens(input_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_RaoRohilPrakash.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import nltk\nfrom nltk.stem import WordNetLemmatizer\n\ndef load_lemma_dict(file_name):\n    lemma_dict = {}\n    with open(file_name, 'r', encoding='utf-8') as file:\n        for line in file:\n            word, lemma = line.strip().split('\\t')\n            lemma_dict[word] = lemma\n    return lemma_dict\n\ndef lemmatize(any_string, file_name):\n    # Load the lemma dictionary\n    lemma_dict = load_lemma_dict(file_name)\n    \n    # Tokenize the input string using whitespace as the separator\n    tokens = nltk.word_tokenize(any_string)\n    \n    #print(tokens)\n    \n    # Lemmatize each token\n    lemmatized_words = [lemma_dict.get(token.lower(), token) for token in tokens]\n\n    # Create a dictionary with original words as keys and lemmas as values\n    dictionary_of_lemmatized_words = dict(zip(tokens, lemmatized_words))\n\n    return print(dictionary_of_lemmatized_words)\n\n# Example usage:\ninput_string = \"The cat is asleep\"\nfile_name = \"lemmatization-en.txt\"\nlemmatize(input_string, file_name)\n\ninput_string = \"The text is lemmatised\"\nfile_name = \"lemmatization-en.txt\"\nlemmatize(input_string, file_name)\n\ninput_string = \"This is a test\"\nfile_name = \"lemmatization-en.txt\"\nlemmatize(input_string, file_name)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_RaoRohilPrakash.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\ndef stemmer(any_string):\n    # Create a Porter Stemmer object\n    porter = PorterStemmer()\n\n    # Tokenize the input string\n    words = word_tokenize(any_string)\n\n    # Apply stemming to each word\n    stemmed_words = [porter.stem(word) for word in words]\n\n    # Join the stemmed words into a string\n    stemmed_string = ' '.join(stemmed_words)\n\n    return print(stemmed_string)\n\n# Example usage:\ninput_string = \"It's important to study and so he studies. He never studied. He is always studying.\"\nstemmer(input_string)\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Rouaa_Maadanli__Hossam_Fawzy_Mohamed_Elsafty.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # split the string into words then count the number of words\n    num_words = len(any_string.split())\n    # count the number of tokens based on characters-based\n    # assume the special characters like (\"!\",\",\",\":\") are tokens\n    num_tokens = len(any_string.replace(\" \", \"\"))\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\n# Test the function\nany_string = \"Hello over!\"\nextract_words_tokens(any_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Rouaa_Maadanli__Hossam_Fawzy_Mohamed_Elsafty.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import csv\n\ndef lemmatize(any_string, file_name):\n    # Define the path of the file\n    file_path = 'lemmatization-en.txt'\n    # Create a dictionary to store the lemma map of the file\n    lemma_map = {}\n    with open(file_path, 'r',encoding='utf-8-sig') as file:\n        # Use the csv reader with tab delimiter\n        reader = csv.reader(file, delimiter='\\t')\n        for row in reader:\n            lemma_map[row[1]] = row[0]\n    \n    # tokenize the string using whitespace\n    tokens = any_string.split()\n    # create a dictionary to store the lemmatized words from the input string\n    dictionary_of_lemmatized_words = {}\n\n    for token in tokens:\n        # check if the token is in the lemma map\n\n        # if yes, then the key of the token will be the lemma\n        if token in lemma_map:\n            dictionary_of_lemmatized_words[token] = lemma_map[token]\n        # if no, then the key of the token will be the token itself\n        else:\n            dictionary_of_lemmatized_words[token] = token\n\n    return(print(dictionary_of_lemmatized_words))\n# Example\nlemmatize(\"cars crashing\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Rouaa_Maadanli__Hossam_Fawzy_Mohamed_Elsafty.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n# implment the stemmer function using regex for these 4 rules\ndef stemmer(any_string):\n    # remove the suffix Continuous from the string\n    any_string = re.sub(r'ing\\b', '', any_string)\n    # remove the suffix Present simple from the string\n    any_string = re.sub(r'(s|es)\\b', '', any_string)\n    # remove the suffix Past simple from the string\n    any_string = re.sub(r'ed\\b', '', any_string)\n    # remove the suffix Infinitive form from the string\n    # remove y with i if it is not preceded by a vowel\n    stemmed_string = re.sub(r'(?<![aeiouAEIOU])y\\b', 'i', any_string)\n    \n    return(print(stemmed_string))\nstemmer(\"studying studies reaches played study\")\n# !pip install nltk\nimport nltk\nfrom nltk.stem import PorterStemmer\nimport re\n\n# implement the stemmer function using nltk\n# Initialize the stemmer\nport_stemmer = PorterStemmer()\n\ndef stemmer(any_string):\n    # tokenize the string\n    words = re.findall(r'\\b\\w+\\b', any_string)\n    # stem the words\n    stemmed_words = [port_stemmer.stem(word) for word in words]\n    # convert the list of stemmed words into a string\n    return ' '.join(stemmed_words)\n\n# Example \ninput_string = \"He studies while she is studying and studied yesterday.\"\nstemmed_string = stemmer(input_string)\nprint(stemmed_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Muskaan_Chopra.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import nltk\nnltk.download('punkt')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nnltk.download('wordnet')\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\ndef extract_words_tokens(any_string):\n    # Split the string into words\n    words = any_string.split()\n    num_words = len(words)\n\n    # Split the string into tokens (individual characters)\n    tokens = list(any_string)\n    num_tokens = len(tokens)\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Muskaan_Chopra.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import nltk\n# nltk.download('all')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet\n\ndef lemmatize(any_string, file_name):\n    # Initialize an empty dictionary for the lemmatization mapping\n    lemma_dict = {}\n\n    # Read the lemmatization file\n    with open(file_name, 'r') as f:\n        for line in f:\n            # Each line contains a word and its lemma, separated by a tab\n            # word, lemma = line.strip().split('\\t')\n            lemma, word = line.strip().split('\\t')\n            lemma_dict[word] = lemma\n\n    # Tokenize the string using whitespace\n    tokens = any_string.split()\n\n    # Create a dictionary of lemmatized words\n    dictionary_of_lemmatized_words = {word: lemma_dict.get(word, word) for word in tokens}\n\n    # Print the dictionary\n\n\n    return print(dictionary_of_lemmatized_words)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Muskaan_Chopra.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef custom_stemmer(word):\n    # Define rules for stemming\n    rules = [\n        # (r'y$', 'i'),  # infinitive form\n        (r'ies$', 'i'),  # Present simple tense: Third person\n        (r'ing$', ''),   # Continuous tense\n        (r'ed$', ''),    # Past simple tense\n    ]\n\n    # Apply rules to the word\n    for pattern, replacement in rules:\n        if re.search(pattern, word):\n            return re.sub(pattern, replacement, word)\n\n    return word  # Return the original word if no rules match\n\ndef stemmer(any_string):\n    # Tokenize the input string using regular expression\n    tokens = re.findall(r'\\b\\w+\\b', any_string)\n\n    # Apply custom stemming to each token\n    stemmed_tokens = [custom_stemmer(token) for token in tokens]\n\n    # Reconstruct the string with stemmed tokens\n    stemmed_string = ' '.join(stemmed_tokens)\n\n    return print(stemmed_string)\n#tokenize\nprint('tokenization')\nextract_words_tokens('i am buying a pizza after I finish playing as I studied and finished my homework :)')\nprint(\"========================================================================================================\")\n#lemmatize\nprint('lemmaization')\nlemmatize(\"i am buying a pizza after I finish playing as I studied and finished my homework :)\", \"lemmatization-en.txt\")\nprint(\"========================================================================================================\")\n#stemming\nprint('stemming')\nstemmer(\"i am buying a pizza after I finish playing as I studied and finished my homework :)\")\n"
    },
    {
        "file_name": "Assignment_1_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef extract_words_tokens(any_string):\n    \n    # number of words by spliting string into words based on whitespace\n    num_words = len(any_string.split())\n    \n    # number of tokens, Tokenize the string using regular expression\n    num_tokens = len(re.findall(r'\\S', any_string))\n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
    },
    {
        "file_name": "Assignment_1_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    \n    # tokenize the input\n    words = any_string.split()\n    \n    # read the lemmatization file and create a dictionary\n    lemma_dict = {}\n    with open(file_name, 'r', encoding='utf-8') as file:\n        for line in file:\n            word, lemma = line.strip().split('\\t')\n            lemma_dict[word] = lemma\n\n    # lemmatize each word and create a dictionary\n    dictionary_of_lemmatized_words = {word: lemma_dict.get(word, word) for word in words}\n    \n    return(print(dictionary_of_lemmatized_words))"
    },
    {
        "file_name": "Assignment_1_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef stemmer(any_string):\n    \n    # remove special characters from the input string\n    cleaned_string = re.sub(r'[^A-Za-z0-9\\s]', '', any_string)\n    \n    # stemming rules\n    rules_list = [\n        ('ying', 'i'),\n        ('ing', ''),\n        ('ed', ''),\n        ('es', ''),\n        ('s', ''),\n        ('y', 'i'),\n    ]\n    \n    stemmed_words = []\n    \n    # apply stemming rules to each word\n    for word in cleaned_string.split():\n        for suffix, replacement in rules_list:\n            if word.endswith(suffix):\n                word = word[: -len(suffix)] + replacement\n                break\n        stemmed_words.append(word)\n\n    # join the stemmed words back into a string\n    stemmed_string = ' '.join(stemmed_words)    \n    \n    return(print(stemmed_string))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ayush_Mishra.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\ndef extract_words_tokens(any_string):\n    #here comes your code\n\n    # The regex below doesn't work when the string\n    # contains contractions like \"don't\" or \"We're\"\n    # To consider contractions as a single word,\n    # we could use the regex r'\\b\\w+(?:'\\w+)?\\b' instead\n    # words = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", any_string)\n    words = re.findall(r'\\w+', any_string)\n    tokens = re.findall(r'\\S', any_string)\n\n    print(\"List of words:\", words)\n    print(\"List of tokens:\", tokens)\n\n    return(print(any_string, \":\", \"num_words:\", len(words), \"and\", \"num_tokens:\", len(tokens), \"respectively\"))\n# Test\nextract_words_tokens(\"''''This!Is!$%@A!String!!!\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ayush_Mishra.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string: str, file_name: str) -> dict:\n    with open(file_name, encoding=\"utf8\") as f:\n        lemma = dict(line.strip().split(\"\\t\") for line in f)\n\n    # Lemmatize each word in the string and store the result in a dictionary\n    return {word: lemma.get(word, word) for word in any_string.strip().split()}\n# Test\nstring = \"The quick brown fox jumps over the lazy dog\"\nlemma = lemmatize(string, \"lemmatization-en.txt\")\nprint('\\n'.join(map(':'.join, lemma.items())))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ayush_Mishra.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(string):\n    rules = {\n        r\"ies$\": \"i\",\n        r\"(s|ing)$\": \"\",\n        r\"ying$\": \"i\",\n        r\"ed$\": \"\",\n        r\"y$\": \"i\",\n    }\n\n    # Replace until no more replacements are possible\n    while True:\n        for rule, replacement in rules.items():\n            if re.search(rule, string):\n                string = re.sub(rule, replacement, string)\n                break\n        else:\n            break\n\n    return string\n# Test\nfor word in [\"study\", \"studies\", \"studying\", \"studied\"]:\n    print(f\"{word} - {stemmer(word)}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # split sentennces into words to count them\n    num_words =len(any_string.split())\n    \n    # remove empty spaces in order not to be considred a single word and count characters only\n    num_tokens=len(list(set(any_string.replace(\" \",\"\"))))\n    \n    #returning the variables\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"this is a test, don't push it!!\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n\n  # d is used as the dictionary of the given file (lemmatization-en.txt) \n  my_dictionary = {}\n  \n  # dictionary_of_lemmatized_words is the dictionary which we will return to the user\n  dictionary_of_lemmatized_words = {}\n\n  # loading the given dictionary into \"d\"  from the file \"lemmatization-en.txt\"\n  with open(file_name, 'r', encoding='utf-8-sig') as my_file:\n    for line in my_file:\n        x = line.split()\n        my_dictionary[x[1]]=x[0]\n\n  # tokenizing the string \"any_string\" into the list \"l\" (we get every word of the string into the list)\n  my_list =any_string.split()\n\n\n  #checking if the words of the string \"any_string\" are in the values or keys of the given dictionary (file lemmatization-en.txt)\n  # if it is found in the dictionary the value is set to the found value/key\n  # if it isn't found in the dictionary the value is set to \"not found in file\"\n  for word in my_list:\n    if word in my_dictionary :\n      dictionary_of_lemmatized_words[word] = my_dictionary[word]\n    elif word in my_dictionary.values():\n      dictionary_of_lemmatized_words[word] = word\n    else :\n      dictionary_of_lemmatized_words[word] = \"Not found\"\n\n  #printing the dictionary_of_lemmatized_words to the user     \n  return(print(dictionary_of_lemmatized_words))\n\nany_string = str()\nlemmatize(\"swimming and surfing are nice activities\",\"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\na\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "nltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem import PorterStemmer\n\ndef stemmer(any_string):\n  # string which will be printed to the user with only the stemms\n  stemmed_string= str()\n  \n  # initializing the class used for stemming\n  stemmer = PorterStemmer()\n  \n  # removing every special charecter but the whitespace with regex and splitting the string into word tokens into the list \"l\"\n  tokenized_string=nltk.word_tokenize(any_string)\n  \n  # stemming every word and adding it to the string \"stemmed_string\"\n  for word in tokenized_string:\n    stemmed_string = stemmed_string +\" \"+ stemmer.stem(word)\n  \n  # printing the stemmed string to the user\n  return(print(stemmed_string))\n\nstemmer(\"Stemming is a rule-based methodology that displays multiple variants of the same base word. ! The approach reduces the base word to its stem word. This is simpler as it involves indiscriminate reduction of the word-ends. Stemming in Python normalizes the sentences and shortens the search result for a more transparent understanding.\")\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Abhishek_Murtuza.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    \n    #Creating list of words from given string\n    words = any_string.split()\n    #Calculating number of words\n    num_words = len(words)\n    #Tokening string according to character-based tokenization\n    tokens = list(any_string)\n    #Removing empty character from list of tokens\n    tokens = [item for item in tokens if item != \" \"]\n    #Calculating length of tokens in string\n    num_tokens = len(tokens)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"Hello, world! This is an example.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Abhishek_Murtuza.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import nltk\ndef lemmatize(any_string, file_name):\n    \n    #Generate lemma dictionary from corpus file provided\n    dict_lemma = {}\n    with open(file_name,'r',encoding='utf-8') as file:\n        for line in file:\n            lemma, word = line.strip().split('\\t')\n            dict_lemma[word] = lemma\n    # Tokenize input string into words\n    tokenized_words  = any_string.strip().split()\n    \n    #Create list with lemmatized_words using values from dict_lemma and keys of tokenized words from input string \n    lemmatized_words = [dict_lemma.get(word,word) for word in tokenized_words ]\n    \n    #Create dictionary with keys as tokenized words in input string and their corresponding value of lemmatized word\n    dictionary_of_lemmatized_words = dict(zip(tokenized_words,lemmatized_words))\n    \n    return(print(dictionary_of_lemmatized_words))\n\nlemmatize(\"billionth abnormalities aborted by academicians\",\"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Abhishek_Murtuza.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\ndef stemmer(any_string):\n    #Create list of tokenized words using regular expression to remove all special characters and get whole words only\n    tokenized_words = re.findall(r'\\b\\w+\\b', any_string)\n    # Define rules for stemming\n    stemmed_words = []\n    rules = [\n        (r'ying$', 'i'),        # Continuous tense\n        (r'es$', ''),            # Present simple tense: Third person\n        (r'ed$', ''),           # Past simple tense\n        (r'y$', 'i')            # Infinitive form\n    ]\n    \n    for word in tokenized_words:\n        # Apply rules to each word\n        for pattern, replacement in rules:\n            word = re.sub(pattern, replacement, word)\n        \n        # Store the stemmed word in the list\n        stemmed_words.append(word)\n        \n    # Reconstruct the string with stemmed words\n    stemmed_string = ' '.join(stemmed_words)\n    return(print(stemmed_string))\ninput_string = \"He is studying and has studied and will study and continue to do studies\"\noutput_string = stemmer(input_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Mauel_Maximilian.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    #a word is a word when it is surrounded by ' ' or at the beginning or end.\n    num_words = len(any_string.split(\" \"))\n    num_tokens = len(any_string)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"hello there\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Mauel_Maximilian.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "!wget https://raw.githubusercontent.com/michmech/lemmatization-lists/master/lemmatization-en.txt\nimport typing\ndef lemmatize(any_string : str,  file_name : str) -> typing.Dict[str,str]:\n    d = {}\n    # extract lookup table from provided file, format lemma\\tword is expected\n    with open(file_name, \"r\") as file:\n      for line in file:\n        lemma, word = line.split(\"\\t\")\n        word = word.strip()\n        lemma = lemma.strip()\n        d[word] = lemma\n\n    # do the lematization with the extracted lookup table\n    dictionary_of_lemmatized_words = {}\n    for word in any_string.split(\" \"):\n      dictionary_of_lemmatized_words[word] = d.get(word, None)\n\n    return(print(dictionary_of_lemmatized_words))\nfile_name = 'lemmatization-en.txt'\nlemmatize(\"first, accounts\", file_name)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Mauel_Maximilian.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    stemmed_string = \"\"\n\n    for word in any_string.split():\n      # remove special chars, numbers are not regardes as a special char\n      if not word.isalnum():\n        while len(word) > 0 and not word[0].isalnum():\n          word = word[1:]\n        while len(word) > 0 and not word[-1].isalnum():\n          word = word[:-1]\n\n\n      # extract the word stem via rules specified above\n      if word.endswith(\"y\"):\n        word = word[:-len(\"y\")] + \"i\"\n      elif word.endswith(\"ies\"):\n        word = word[:-len(\"es\")]\n      elif word.endswith(\"ing\"):\n        word = word[:-len(\"ying\")] + \"i\"\n      elif word.endswith(\"ed\"):\n        word = word[:-len(\"ed\")]\n\n      # append stem to result\n      # note: we do not reatattch the removed special chars, as the task left\n      # this part open for interpretation\n      stemmed_string += f\" {word}\"\n\n    return(print(stemmed_string))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # Split words by spaces.\n    # Remove any strings that are empty.\n    words = any_string.split(\" \")\n    words = [word for word in words if word != \"\"]\n    num_words = len(words)\n    # Split tokens by characters and count the number of unique tokens.\n    num_tokens = len(set(any_string.split(\"\")))\n    return print(\n        any_string,\n        \":\",\n        \"num_words:\",\n        num_words,\n        \"and\",\n        \"num_tokens:\",\n        num_tokens,\n        \"respectively\",\n    )"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # Read file line by line.\n    with open(file_name, \"r\", encoding=\"utf-8-sig\") as f:\n        lines = f.readlines()\n        # Tokenize each line.\n        lines = [line.replace(\"\\n\", \"\").split(\"\\t\") for line in lines]\n    # Create a dictionary of lemmatized words.\n    lemma_dict = {}\n    for line in lines:\n        lemma = line[0]\n        word = line[1]\n        lemma_dict[word] = lemma\n    # Tokenize the input string.\n    tokens = any_string.split(\" \")\n    # Lemmatize each token.\n    dictionary_of_lemmatized_words = {}\n    for token in tokens:\n        # If the token is in the dictionary, replace it with the lemma.\n        if token in lemma_dict.keys():\n            dictionary_of_lemmatized_words[token] = lemma_dict[token]\n        # If the token is not in the dictionary, keep it as it is.\n        else:\n            dictionary_of_lemmatized_words[token] = token\n    return dictionary_of_lemmatized_words\n\n\nlemmatize(\"I am playing with my cats\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\n\ndef stemmer(any_string):\n    words = re.findall(\"[a-zA-Z]+\", any_string)\n\n    # Stemming rules\n    stemmed_words = []\n    for word in words:\n        if word.endswith((\"ies\", \"ing\", \"ed\")):\n            stem = re.sub(\"(ies|ing|ed)$\", \"i\", word)\n            stem = re.sub(\"(yi)$\", \"i\", stem)\n            stemmed_words.append(stem)\n        else:\n            stemmed_words.append(word)\n\n    stemmed_string = \" \".join(stemmed_words)\n\n    return print(stemmed_string)\n"
    },
    {
        "file_name": "Assignment_1_Naman_Jain.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef extract_words_tokens(any_string):\n    # Tokenize\n    tokens = re.findall(r'\\S+|\\n', any_string)\n\n    # Count the number of words and tokens\n    num_words = len(tokens)\n    num_tokens = sum(len(token) for token in tokens)\n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\nany_string = \"Hey! How many words and tokens are in this string?\"\nextract_words_tokens(any_string)"
    },
    {
        "file_name": "Assignment_1_Naman_Jain.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\ndef lemmatize(any_string, file_name):\n    # Initialize\n    lemmatizer = WordNetLemmatizer()\n\n    # Read lemma mapping from the file\n    with open(file_name, 'r', encoding='utf-8') as file:\n        lemma_mapping = {word: lemma for word, lemma in (line.strip().split('\\t') for line in file)}\n\n    # Lemmatize each token\n    dictionary_of_lemmatized_words = {token: lemma_mapping.get(token, lemmatizer.lemmatize(token)) for token in any_string.split()}\n\n    return(print(dictionary_of_lemmatized_words))\n\nany_string = \"The quick brown foxes are jumping over the lazy dogs\"\nfile_name = \"lemmatization-en.txt\"\nlemmatize(any_string, file_name)"
    },
    {
        "file_name": "Assignment_1_Naman_Jain.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from nltk.stem import PorterStemmer\nimport re\n\ndef stemmer(any_string):\n    # Initialize\n    porter = PorterStemmer()\n    \n    # Split into words\n    words = re.findall(r'\\b\\w+\\b', any_string)\n\n    # Apply stemming to each word\n    stemmed_words = [porter.stem(word) for word in words]\n\n    # final stemmed string\n    stemmed_string = ' '.join(stemmed_words)\n    \n    return(print(stemmed_string))\n\n#any_string = \"study studies studying studied\"\nany_string = \"He studies programming and enjoys studying new langauges.\"\nstemmer(any_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Sinem_Dnmez.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "#without libraries\ndef extract_words_tokens(any_string):\n    #here comes your code\n    word_list = any_string.split(\" \")\n    num_words= len(word_list)\n    chars = sorted(list(set(any_string)))\n    num_tokens = len(chars)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\n\n\nany_string = \"bu bi cumle mi sence, aw!asdfghjkl\"\nextract_words_tokens(any_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Sinem_Dnmez.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    #here comes your code\n\n    #create a lookup of lemmatization\n    #key: token, value: lemma\n    look_up = {}\n    with open(file_name, 'r', encoding='utf-8') as my_file:\n      for line in my_file:\n        lemma_token = line.split(\"\\t\")\n        #remove \\n\n        look_up[lemma_token[1][:-1]] = lemma_token[0]\n\n    #tokenize the string first\n    word_list = any_string.split(\" \")\n\n    #words as keys and lemma as values\n    #repeated words?\n    dictionary_of_lemmatized_words = {}\n    for word in word_list:\n      #if its not in the look_up table its a word in itself\n      if word in look_up:\n        dictionary_of_lemmatized_words[word] = look_up[word]\n      else:\n        dictionary_of_lemmatized_words[word] = word\n\n    return(print(dictionary_of_lemmatized_words))\n\n\nany_string = \"this is accelerated very well and wellness and yes\"\nlemmatize(any_string, \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Sinem_Dnmez.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "#without using libraries\n# def stemmer(any_string):\n#     #here comes your code\n#     # Helper function to get the stem of a word\n#     def get_stem(word):\n#         if word.endswith('ies') and len(word) > 3:\n#             return word[:-3] + 'i'\n#         elif word.endswith('ing') and len(word) > 3:\n#             return word[:-3] + 'i'\n#         elif word.endswith('ied') and len(word) > 3:\n#             return word[:-3] + 'i'\n#         elif word.endswith('y') and len(word) > 1:\n#             return word[:-1] + 'i'\n#         else:\n#             return word\n\n#     # Splitting the string into words\n#     words = any_string.split()\n\n#     # Processing each word and reconstructing the sentence\n#     stemmed_words = []\n#     for word in words:\n#         # Remove punctuation from the end of the word, if present\n#         punctuations = \",.!:;?\"\n#         if word[-1] in punctuations:\n#             stemmed_word = get_stem(word[:-1]) + word[-1]\n#         else:\n#             stemmed_word = get_stem(word)\n\n#         stemmed_words.append(stemmed_word)\n#     stemmed_string = ' '.join(stemmed_words)\n\n#     return(print(stemmed_string))\n\n#with libraries\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\n\ndef stemmer(any_string):\n  ps = PorterStemmer()\n  words = any_string.split(\" \")\n  stemmed_words = []\n  for word in words:\n    stemmed_words.append(ps.stem(word))\n  stemmed_string = ' '.join(stemmed_words)\n  return(print(stemmed_string))\n\n\n\n# Example usage\nstemmer(\"Tom quickly modifies his plans, modifying them as needed. He modified his schedule last week.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Jan_Rogge.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    words = any_string.split(\" \")#first split the string into words\n    num_words = len(words)\n\n    tokens = [c for c in any_string]#character based tokenization means every character is a token, so make a character list\n    num_tokens = len(tokens)\n    print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\")\nstring = \"test this super-cool thingy !\"\nextract_words_tokens(string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Jan_Rogge.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    #first read in text file\n    with open(file_name, \"r\", encoding='utf-8') as f:\n        lines = f.readlines()\n        stems = []\n        lemmas = []\n        for line in range(1, len(lines)):\n            lines[line] = lines[line].strip().split(\"\\t\")#get rid of \\n and \\t characters, \\t is \" \"\n            stems.append(lines[line][0])#save all word stems (left column in text file)\n            lemmas.append(lines[line][1])#save all word lemmas (right column in text file)\n\n    words = any_string.split(\" \")#split the input string at \" \"\n    \n    dictionary_of_lemmatized_words = {}\n    for word in words:\n        if word in lemmas:\n            stem = stems[lemmas.index(word)]#find the index of lemma that belongs to word, and save its corresponding stem\n            dictionary_of_lemmatized_words.update({word : stem})#add the word and stem to dictionary\n    print(dictionary_of_lemmatized_words)\n\nlemmatize(\"barricades barricaded\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Jan_Rogge.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    words = any_string.split(\" \")# get words\n\n    for word in range(len(words)):\n        chars = [c for c in words[word]]#get characters of words for processing\n        #join the last n characters to a string and check for rules above:\n        if \"\".join(chars[-3:]) == \"ing\":#checking if a last three characters of a word are \"ing\"\n            chars = chars[:-3]\n        if chars[-1] == \"y\":\n            chars[-1] = \"i\"\n        elif \"\".join(chars[-2:]) == \"es\":\n            chars = chars[:-2]      \n  \n        elif \"\".join(chars[-2:]) == \"ed\":\n            chars = chars[:-2]  \n\n        words[word] = \"\".join(chars)#join all characters back into a word after processing\n    stemmed_string = \" \".join(words)#join all words back into a single string\n    print(stemmed_string)\n\nstemmer(\"studying something!\")"
    },
    {
        "file_name": "Assignment_1_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n\n    # Cutting the string at SPACES, len counts the words\n    num_words = len(any_string.split())\n        \n    # Character-based tokenization\n    tokens = list(any_string)\n    \n    # deleting the SPACES in the list\n    while ' ' in tokens:\n        tokens.remove(\" \")\n        \n    # Counting the number of tokens\n    num_tokens = len(tokens)\n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\nany_string = \"irgendwas cooles langes\"\n\nextract_words_tokens(\"irgendwas cooles langes\")"
    },
    {
        "file_name": "Assignment_1_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    lemmatization_dict = {}\n      \n    #open file a put in dictionary, split at the TBULATOR\n    with open(file_name, 'r', encoding='utf-8') as file:\n        for line in file:\n            lemma, word = line.strip().split('\\t')\n            lemmatization_dict[word] = lemma\n            \n    # Tokenize the input string using whitespace as separator\n    words = any_string.split()\n    \n    # Lemmatize each word and create a dictionary\n    dictionary_of_lemmatized_words = {word: lemmatization_dict.get(word, word) for word in words}\n    \n    return(print(dictionary_of_lemmatized_words))\n# Example usage:\nfile_name = 'lemmatization-en.txt'\nany_string = \"cats dogs running swimmings not_included\"\n\nlemmatize(any_string, file_name)"
    },
    {
        "file_name": "Assignment_1_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "# Stemming solution with NLTK Libary\n\nimport nltk\nimport re\nfrom nltk.stem import PorterStemmer\n#nltk.download(\"punkt\")\n\ndef stemmer(any_string):\n    # filter bad symbols\n    any_string = re.sub(r\"[!:'`,$]\", \"\", any_string)\n    \n    #split text at SPACES\n    words = any_string.split()\n    \n    # initiate an object of porter stemmer\n    ps = PorterStemmer()\n    \n    # initalize a empty string\n    stemmed_string = \"\"\n    \n    # iterate over evry word in the string and repace it with ists stem an separate them with SPACES\n    for element in words:      \n        stemmed_string = stemmed_string + \" \" + ps.stem(element)\n        \n    return(print(stemmed_string))\n\n# Example usage:\nany_string2 = \"studies studying studied caresses cat's dogs running swimming\"\nstemmer(any_string2)\n# Alternative stemming solution with its own rules, inspired by: https://github.com/anishLearnsToCode/porter-stemmer/blob/master/porter-stemmer-algorithm.ipynb\n\nimport re\n\ndef stemmer(input_string):\n    # Define stemming rules\n    rules = [\n        (r\"[!:'`,$]\", ''),  # Filter bad symbols\n        (r'sses$', 'xx'), # SSSE reduction rule with placeholder see last regex\n        (r'ss$', 'xx'),   # Double S reduction rule with placeholder see last regex\n        (r'ies$', 'y'),   # Present simple tense: Third person\n        (r'es$', ''),     # Present simple tense\n        (r'ing$', ''),    # Continuous tense\n        (r'ed$', ''),     # Past simple tense\n        (r's$', ''),      # Plural form\n        (r'y$', 'i'),     # Infinitive form\n        (r'xx$', 'ss'),   # Replace placeholder\n\n    ]\n\n    # Apply rules to the input string\n    for pattern, replacement in rules:\n        input_string = re.sub(pattern, replacement, input_string)\n\n    return input_string\n\n# Example usage:\ninput_string = \"studies studying studied caresses cat's dogs running swimming\"\nresult = \" \".join(stemmer(word) for word in input_string.split())\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Faris_Hajdarpasic.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string, regex):\n    word_list = re.split(regex, any_string)[:-1]\n    num_words = len(word_list)\n    tokens = sorted(list(set(word_list))) # unique elements in this list\n    num_tokens = len(tokens)\n    return word_list, tokens\nregex = r'[\\s!@.:;,#]+'\nwords, tokens = extract_words_tokens(\"This is is is a sample text for. testing RegexpTokenizer in NLTK.NLTK.NLTK.\", regex)\nprint('Number of words: ', len(words))\nprint('Number of tokens: ', len(tokens))\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Faris_Hajdarpasic.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Faris_Hajdarpasic.ipynb",
        "question": "### Note:\nIt is written dictionary of all $\\textbf{words}$, but since there can be many same words (which are basically represented by same token) that would mean that we want to create dictionary where all those words are the keys. But since key must be unique in dictionary, I suppose you wanted to say - dictionary with all $\\textbf{tokens}$ as keys and the $\\textbf{lemma}$ of the $\\textbf{tokens}$ as values.",
        "answer": "def lemmatize(any_string, file_name):\n    df = pd.read_csv(file_name, sep='\\t', header=None, names=['lemma', 'token'])\n    # Set the 'token' column as the index for easier search (token is unique, whereas lemma is not)\n    df.set_index('token', inplace=True)\n    \n    # Regex meaning: split based on:\n    # 1) [\\s]+ -> one or multiple occurences of whitespace , or (|)\n    # 2) $ -> end of the line\n    regex = r'[\\s]+|$'\n    words, tokens = extract_words_tokens(any_string, regex)\n    print(words)\n    \n    dictionary_of_lemmatized_tokens = {}\n    for token in tokens:\n        dictionary_of_lemmatized_tokens[token] = df.loc[token]['lemma']\n        \n    return dictionary_of_lemmatized_tokens\nfile_name = \"lemmatization-en.txt\"\ndictionary_of_lemmatized_tokens = lemmatize(\"bustards busies acclimated acclimates acclimating\", file_name)\ndictionary_of_lemmatized_tokens\n\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Faris_Hajdarpasic.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # Stemming rules as regex\n    rules = {\n        \"y$\" : 'i',\n        \"ies$\" : 'i',\n        \"ying$\" : 'i',\n        \"ed$\" : 'i',\n        \"ing$\": \"\",\n        \"ed$\": \"\",\n        \"ves$\": \"f\",\n        \"ied$\": \"y\",\n        \"er$\": \"\",\n        \"est$\": \"\",\n        \"en$\": \"\",\n        \"ly$\": \"\",\n        \"ful$\": \"\",\n        \"ment$\": \"\",\n        \"ness$\": \"\",\n        \"able$\": \"\",\n        \"ize$\": \"\",\n        \"ise$\": \"\",\n        \"ation$\": \"\",\n        \"ator$\": \"\",\n        \"ative$\": \"\",\n        \"al$\": \"\",\n        \"ence$\": \"\",\n        \"ance$\": \"\",\n        \"tion$\": \"\",\n        \"ion$\": \"\",\n        \"ity$\": \"\",\n        \"ous$\": \"\",\n        \"ify$\": \"\",\n        \"ible$\": \"\",\n        \"ism$\": \"\",\n        \"ist$\": \"\",\n        \"ite$\": \"\",\n        \"ship$\": \"\",\n        \"hood$\": \"\"\n    }\n\n    \n    stemmed_string = any_string\n    for rule, replacement in rules.items():\n        pattern = re.compile(rule)\n        if re.search(pattern, any_string):\n            stemmed_string = re.sub(pattern, replacement, any_string)\n            \n    return stemmed_string\nprint(stemmer('studies'))\nprint(stemmer('neighbourhood'))\nprint(stemmer('likelihood'))\nprint(stemmer('fence'))\nprint(stemmer('stance'))\nprint(stemmer('crazy'))\n\nprint(stemmer('play'))\nprint(stemmer('plays'))\nprint(stemmer('playing'))\nprint(stemmer('played'))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Gjergj_Plepi.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import string\n\ndef extract_words_tokens(any_string):\n    \"\"\"\n    For this implementation, I do not count as words standalone punctuations like \",\" which might happen after we use the function split.\n    However, as the tokenization is character base, I count those. \n    \"\"\"\n    num_words, num_tokens = 0, 0\n\n    elements = any_string.split()\n\n    for element in elements:\n        if element.strip(string.punctuation).isalpha(): #strip the punctuations in the beginning and in the end, if it then is a word, count it.\n            num_words += 1\n        \n        num_tokens += len(element)\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\ns = \"Hi! What , is your name ,Gjergj.\"    \nextract_words_tokens(s)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Gjergj_Plepi.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    words = any_string.split()\n\n    \"\"\"Create the dictionary from the word-lemma pairs stored in the .txt file\"\"\"\n    dictionary = {} #stores the word-lemma pairs from the file\n    counter = 0\n    with open(\"lemmatization-en.txt\", 'r', encoding='utf-8') as my_file:\n        for line in my_file:\n            lemma, word = line.split(\"\\t\")\n            if counter == 0:\n                dictionary[word[:-1]] = lemma[-1]\n                counter += 1\n            else:\n                dictionary[word[:-1]] = lemma\n\n    \n    dictionary_of_lemmatized_words = {}\n\n    for word in words:\n        word_lowercase = word.lower() #convert it to lowercase since all the words in the .txt file are on the lowercase\n        if word_lowercase in dictionary:\n            dictionary_of_lemmatized_words[word] = dictionary[word_lowercase]\n        else:\n            dictionary_of_lemmatized_words[word] = word #if it is not in the .txt file it means that it is a lemma itself\n\n    return(print(dictionary_of_lemmatized_words))\ns = \"I am participating on the Introduction to Natural Language Processing course\"\nlemmatize(s, 'lemmatization-en.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Gjergj_Plepi.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n\n    words = any_string.split() # assuming there might be a sentence as an input\n\n    stemmed_strings = []\n\n    for word in words:\n        if word.strip(string.punctuation).isalpha(): #strip the punctuations in the beginning and in the end, if it then is a word, process it.\n            word = word.strip(string.punctuation)\n            if word[-1] == 'y': # infinitive form: e.g study\n                temp = word[:-1] + 'i'\n            elif word[-1] == 's' and word[-2] != 's':\n                if word[-2:] == 'es':\n                    temp = word[:-2] # studies\n                else: \n                    temp = word[:-1] # works\n            elif  word[-3:] == 'ing': # continuous tense\n                if word[-4] == 'y': # e.g: studying\n                    temp = word[:-4] + 'i'\n                else: # e.g: working\n                    temp = word[:-3]\n            elif word[-2:] == 'ed': # past tense\n                temp = word[:-2]\n            else: # infinitive form\n                temp = word\n        \n            stemmed_strings.append(temp)\n\n    stemmed_string = \" \".join(stemmed_strings)\n    return(print(stemmed_string))\nstemmer(\"study, studies, studied, studying, working, joking, works, worked, dress, dresses.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Vella_Rosario.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # Tokenize the input string into characters\n    tokens = list(any_string)\n\n    # Count the number of words (assuming words are separated by spaces)\n    num_words = len(any_string.split())\n\n    # Count the number of tokens\n    num_tokens = len(tokens)\n\n    return print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\")\n\nextract_words_tokens(\"hallo test zwei zwei drei\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Vella_Rosario.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # Read the lemmatization file and create a dictionary\n    with open(file_name, 'r') as file:\n        lemmatization_dict = {}\n        for line in file:\n            key, value = line.strip().split('\\t')\n            lemmatization_dict[key] = value\n\n    # Tokenize the input string using whitespace as a separator\n    words = any_string.split()\n\n    # Lemmatize the words using the lemmatization dictionary\n    lemmatized_words = [lemmatization_dict.get(word, word) for word in words]\n\n    # Create a dictionary of lemmatized words\n    dictionary_of_lemmatized_words = dict(zip(words, lemmatized_words))\n\n    return print(dictionary_of_lemmatized_words)\n\n# Example usage:\nfile_name = \"./lemmatization-en.txt\"\ninput_string = \"10 millionth second twentieth drei  sieben swimming\"\nlemmatize(input_string, file_name)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Vella_Rosario.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import nltk\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\n\ndef stemmer(any_string):\n    #Tokenize the input string into words\n    words = word_tokenize(any_string)\n\n    #Initialize the Porter Stemmer\n    porter_stemmer = PorterStemmer()\n\n    #Apply stemming to each word\n    stemmed_words = [porter_stemmer.stem(word) for word in words]\n\n    #Join the stemmed words into a string\n    stemmed_string = ' '.join(stemmed_words)\n\n    return print(stemmed_string)\n\nexample_string = \"studies studying studied study\"\nstemmer(example_string)"
    },
    {
        "file_name": "Assignment_1_Ali_Sehran.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    splited_words = any_string.split()\n    num_words = len(splited_words)\n    tokens = [t for t in list(any_string) if t != \" \"]\n    num_tokens = len(tokens)\n\n    print(\"num_words:\", num_words, \" Words:\", splited_words,\"\\nnum_tokens:\", num_tokens, \"Tokens:\", tokens)\n    return (num_words, num_tokens)"
    },
    {
        "file_name": "Assignment_1_Ali_Sehran.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def create_lemma_dict (file_name):\n    dictionary_of_lemmatized_words = {}\n    with open(file_name, 'r') as f:\n        for line in f:\n            lemma, word = line.strip().split('\\t') \n            dictionary_of_lemmatized_words[word] = lemma\n            \n    return dictionary_of_lemmatized_words\n \ndef lemmatize(any_string, file_name):\n    dictionary_of_lemmatized_words = create_lemma_dict(file_name)\n    words = any_string.split()\n    lemma_string = {}\n\n    # Lemmatizing the string using the dictionary.\n    for word in words:\n        lemma_string[word] = dictionary_of_lemmatized_words.get(word, word)\n\n    return lemma_string"
    },
    {
        "file_name": "Assignment_1_Ali_Sehran.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef stemmer(any_string):\n    rule = r'(ies|y|ing|ed)$' \n    words = any_string.split()\n\n    for i in range(len(words)):\n        if re.search(rule, words[i]):\n            # If the word ends with 'ies', y, ing, or ed, replace it with 'i'.\n            words[i] = re.sub(rule, 'i', words[i])\n\n    return ' '.join(words)\n# main\nlemma_file_path = \"./lemmatization-en.txt\"\ninput_string = \"This is a sample sentence for testing purposes. I am testing the lemmatization and stemming of words.\"\nnum_words, num_tokens = extract_words_tokens(input_string)\nlemmatized_words = lemmatize(input_string, lemma_file_path)\nstemmed_string = stemmer(input_string)\n\n\nprint(\"lemmatized_words:\", lemmatized_words)\nprint(\"stemmed_string:\", stemmed_string)\n"
    },
    {
        "file_name": "Assignment_1_Mahdi_Rahimianaraki_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # Splitting the string by spaces to get words\n    words = any_string.split()\n    num_words = len(words)\n\n    # Counting every character (including spaces and special characters) as a token\n    num_tokens = len(any_string)\n\n    return print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\")\n# Example usage:\nextract_words_tokens(\"Hello, world! This is a sample sentence. Please return the number of words and the number of tokens in this sentence.\")"
    },
    {
        "file_name": "Assignment_1_Mahdi_Rahimianaraki_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # Read the lemmatization file and build a dictionary\n    lemmas = {}\n    with open(file_name, 'r') as file:\n        for line in file:\n            word, lemma = line.strip().split('\\t')\n            lemmas[word] = lemma\n\n    # Tokenize the input string using whitespace\n    words = any_string.split()\n\n    # Create a dictionary to store the lemmas of the words in the input string\n    dictionary_of_lemmatized_words = {}\n    for word in words:\n        # Use the lemma if it's in the dictionary, otherwise use the word itself\n        dictionary_of_lemmatized_words[word] = lemmas.get(word, word)\n\n    return(print(dictionary_of_lemmatized_words))\n# Example usage:\nany_string = \"susceptibility spook disembowel invoice weedkiller talk traduce construct crib crumble\"\nfile_name = \"lemmatization-en.txt\"\nlemmatize(any_string, file_name)"
    },
    {
        "file_name": "Assignment_1_Mahdi_Rahimianaraki_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')  # Download the punkt tokenizer\n\ndef stemmer(any_string):\n    # Initialize the Porter Stemmer\n    stemmer = PorterStemmer()\n\n    # Tokenize the input string into words\n    words = word_tokenize(any_string)\n\n    # Apply stemming to each word\n    stemmed_words = [stemmer.stem(word) for word in words]\n\n    # Join the stemmed words into a string\n    stemmed_string = ' '.join(stemmed_words)\n\n    return(print(stemmed_string))\n# Example usage:\ninput_string = \"He studies programming and is currently studying Python at the University of Bonn.\"\nstemmer(input_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Weiberg.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # To find the number of words, we count the number of elements we get when we split at spaces\n    # Special characters should not matter as long as spaces are used were necessary\n    words = any_string.split(\" \")\n    num_words = len(words)\n    # The number of tokens is just the number of characters in the string\n    num_tokens = len(any_string)\n    #here comes your code\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\nextract_words_tokens(\"Hello my name is tobi.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Weiberg.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # Create a dictionary that given a word returns it's lemma according to the file\n    lemmatization_dict = {}\n    # No idea why I need to specify the encoding here, but this makes it work ;)\n    with open(file_name, \"r\", encoding='utf-8-sig') as f:\n        for line in f:\n            line = line.strip().split(\"\\t\")\n            assert len(line) == 2\n            lemmatization_dict[line[1]] = line[0]\n\n    # Split the input string at spaces\n    tokenized_string = any_string.split(\" \")\n    # Create a dictionary with the lemma as value if existent, else put the word again\n    dictionary_of_lemmatized_words = {w: lemmatization_dict[w] if w in lemmatization_dict else w for w in tokenized_string}\n\n    #here comes your code\n    return(print(dictionary_of_lemmatized_words))\n\nlemmatize(\"Hello my name is tobi\", \"./lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Weiberg.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "# I split this up into two functions:\n\n# This function returns the stem of a single word\ndef word_stemmer(word_string):\n    # Infinitive form:\n    if word_string.endswith(\"y\"):\n        word_string = word_string.removesuffix(\"y\") + \"i\"\n    # Present simple tense:\n    elif word_string.endswith(\"ies\"):\n        word_string = word_string.removesuffix(\"ies\") + \"i\"\n    # Continuous tense:\n    elif word_string.endswith(\"ying\"):\n        word_string = word_string.removesuffix(\"ying\") + \"i\"\n    # Past simple tense\n    elif word_string.endswith(\"ied\"):\n        word_string = word_string.removesuffix(\"ied\") + \"i\"\n    return word_string\n\nprint(word_stemmer(\"study\"))\nprint(word_stemmer(\"studies\"))\nprint(word_stemmer(\"studying\"))\nprint(word_stemmer(\"studied\"))\n\n# This function returns the stemming of an entire sentence with special characters\ndef stemmer(any_string):\n    # We first put one space before every special character so we can easily split them off\n    any_string = any_string.replace(\"!\", \" !\")\n    any_string = any_string.replace(\",\", \" ,\")\n    any_string = any_string.replace(\".\", \" .\")\n\n    # Split the string at the spaces, put every word through the word_stemmer and put it back together\n    tokenized_string = any_string.split(\" \")\n    stemmed_tokens = [word_stemmer(w) for w in tokenized_string]\n    stemmed_string = \" \".join(stemmed_tokens)\n\n    # Now we remove the added spaces again\n    any_string = any_string.replace(\" !\", \"!\")\n    any_string = any_string.replace(\" ,\", \",\")\n    any_string = any_string.replace(\" .\", \".\")\n\n    return(print(stemmed_string))\n\nstemmer(\"study now studies later studying soon studied\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from nltk.tokenize import word_tokenize\nimport string\ndef extract_words_tokens(any_string):\n    #here comes your code\n    words = word_tokenize(any_string.lower())\n    words_without_punctuation = [word for word in words if not any(char in string.punctuation for char in word)]\n    num_words=len(words_without_punctuation)\n    num_tokens=len(any_string)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\ninput_text = \"He studies programming and enjoys studying different languages.\"\nresult = extract_words_tokens(input_text)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    #here comes your code\n    with open(file_name, 'r',encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        dict = {} # word-to-lemma mapping\n        for line in lines:\n            lemma, word = line.split()\n            dict[word] = lemma\n        dictionary_of_lemmatized_words = {}\n        words = any_string.split()\n        for word in words:\n            if word in dict.keys():\n                dictionary_of_lemmatized_words[word] = dict[word]\n            else:\n                dictionary_of_lemmatized_words[word] =word\n    return(print(dictionary_of_lemmatized_words))\ninput_text = \"He studies programming and enjoys studying different languages\"\nresult = lemmatize(input_text,\"lemmatization-en.txt\")\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from nltk.stem import PorterStemmer\ndef stemmer(any_string):\n    # Initialize the Porter Stemmer\n    stemmer = PorterStemmer()\n\n    # Tokenize the input string into words\n    words = word_tokenize(any_string)\n\n    # Apply stemming to each word\n    stemmed_words = [stemmer.stem(word) for word in words]\n\n    # Join the stemmed words back into a string\n    stemmed_string = ' '.join(stemmed_words)\n\n    return stemmed_string\ninput_text = \"He studies programming and enjoys studying different languages. Now he studies German.\"\nresult = stemmer(input_text)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ilaha_Manafova.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef extract_words_tokens(any_string):\n    \n    # \\w matches any alphanumeric character ('+' for one or more occurrences) \n    # findall returns a list of all occurrences of the pattern\n    num_words = len(re.findall(r'\\w+', any_string))\n       \n    \n    # \\S matches any character which is not a whitespace\n    num_tokens = len(re.findall(r'\\S', any_string))\n    \n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"Hello @@ world!\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ilaha_Manafova.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    \n    \n    # list of tokens separated by white space\n    tokens = any_string.split() \n    \n    \n    # Intialize dictionary with the tokens as keys and empty string as values\n    dictionary_of_lemmatized_words = {token: \"\" for token in tokens}\n    \n    \n    # Look for the tokens in each line of the file and update the value of the token (key) the dictionary\n    with open(file_name, 'r', encoding='utf-8-sig') as my_file:\n        for line in my_file:\n            l = line.strip().split('\\t')\n            for token in tokens:\n                if token.lower() == l[1]:\n                    if token.islower():  # to keep the case of the word as it is\n                        dictionary_of_lemmatized_words[token] = l[0]\n                    else:  \n                        dictionary_of_lemmatized_words[token] = l[0].capitalize()\n               \n    \n    return(print(dictionary_of_lemmatized_words))\nlemmatize(\"Barak Obama was born in Hawai\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ilaha_Manafova.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef stemmer(any_string):\n    \n    # tokenize the string into words\n    words = re.findall(r'\\w+', any_string)  \n    \n    \n    # define rules for verbs\n    rules = {r'ing$': 'i', r'ed$': '', r'es$': '', r'y$': 'i'}\n    \n    \n    stemmed_words = []\n    \n    \n    # for each word, look for the pattern and apply the rule\n    for word in words:\n        for pattern, replacement in rules.items():\n            word = re.sub(pattern, replacement, word)\n        stemmed_words.append(word)\n    \n\n    stemmed_string = ' '.join(stemmed_words)\n    \n    \n    return(print(stemmed_string))\nstemmer(\"study studies talking @@ looked\")\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # remove special characters from the string, and convert it to lowercase\n    any_string = ''.join(e.lower() for e in any_string if e.isalnum() or e == \" \")\n\n    # split the string into words and create a set of tokens\n    words = any_string.split()\n    tokens = set(words)\n\n    print(any_string, \":\", \"num_words:\", len(words), \"and\", \"num_tokens:\", len(tokens), \"respectively\", \"words:\", words, \"tokens:\", tokens)\n\n    return len(words), len(tokens), words, tokens\n\n_ = extract_words_tokens(\n    'This is a string! It contains some words and some tokens. Tokens are the same as words, but they are not.'\n)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # load the lemmatization dictionary\n    with open(file_name, 'r') as f:\n        lemmatization_dict = {line.split('\\t')[1].strip(): line.split('\\t')[0] for line in f}\n\n    # extract tokens from the string\n    _, _, _, tokens = extract_words_tokens(any_string)\n\n    # create a dictionary with the lemmatized words\n    dictionary_of_lemmatized_words = {}\n\n    # for each token in the string, check if it's in the lemmatization dictionary\n    # if it is, add it to the dictionary with the lemma as the value\n    # if it's not, add it to the dictionary with the token as the value\n    for token in tokens:\n        if token in lemmatization_dict:\n            dictionary_of_lemmatized_words[token] = lemmatization_dict[token]\n        else:\n            dictionary_of_lemmatized_words[token] = token\n\n    return dictionary_of_lemmatized_words\n\nlemmatize('This is a string! It contains some words and some tokens. Tokens are the same as words, but they are not.', 'lemmatization-en.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # extract tokens from the string\n    _, _, _, tokens = extract_words_tokens(any_string)\n\n    # create a list of stemmed words\n    stemmed_words = []\n    for token in tokens:\n       # if the token ends with 'y', remove the 'y' and add 'i' to the end\n        if token.endswith('y'):\n            stemmed_words.append(token[:-1] + 'i')\n        # if the token ends with 'es', remove the 'es'\n        elif token.endswith('es'):\n            stemmed_words.append(token[:-2])\n        # if the token ends with 'ying', remove the 'ying' and add 'i' to the end\n        elif token.endswith('ying'):\n            stemmed_words.append(token[:-4] + 'i')\n        # if the token ends with 'ed', remove the 'ed'\n        elif token.endswith('ed'):\n            stemmed_words.append(token[:-2])\n        else:\n            stemmed_words.append(token)\n    return(print(stemmed_words))\n\nstemmer('Studying, studies, studied, study')\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # Split the input string into words using the default whitespace delimiter.\n    splitted_string = any_string.split()\n\n    # Initialize an empty list to hold individual character-based tokens.\n    char_based_tokens = []\n    \n    # Iterate over each word in the list of split words.\n    for word in splitted_string:\n        # Unpack the characters of the word and add them to the char_based_tokens list.\n        char_based_tokens += ([*word])\n    \n    # Calculate the number of character-based tokens.\n    num_tokens = len(char_based_tokens)\n\n    # Calculate the number of words.\n    num_words = len(splitted_string)\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def create_lemmatization_dict(file_name):\n    # Create an empty dictionary to store lemmatization mappings.\n    my_dict = {}\n\n    # Open the specified file in read mode with UTF-8 encoding.\n    with open(file_name, 'r', encoding='utf-8') as my_file:\n        # Iterate through each line in the file.\n        for line in my_file:\n            # Split the line into words.\n            splitted_words = line.split()\n            # Map the lemmatized word (second word) to its original form (first word).\n            my_dict[splitted_words[1]] = splitted_words[0]\n    \n    # Return the constructed dictionary.\n    return my_dict\n\ndef lemmatize(any_string, file_name):\n    # Create a dictionary for lemmatization using the specified file.\n    my_lemmatized_dict = create_lemmatization_dict(file_name)\n    \n    # Split the input string into individual words (tokens).\n    tokenized_sentence = any_string.split(\" \")\n    \n    # Initialize a dictionary to store the lemmatized form of each token.\n    dictionary_of_lemmatized_words = {}\n\n    # Iterate through each token in the tokenized sentence.\n    for token in tokenized_sentence:\n        # Check if the token is in the lemmatization dictionary.\n        if token in my_lemmatized_dict:\n            # If so, map the token to its lemmatized form.\n            dictionary_of_lemmatized_words[token] = my_lemmatized_dict[token]\n        else:\n            # If the token is not in the dictionary, keep it as it is.\n            dictionary_of_lemmatized_words[token] = token\n\n    return(print(dictionary_of_lemmatized_words))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "# A dictionary containing the rules for stemming words.\nRULES = {\n    'sation': '',\n    'zation': '',\n    'yes': 'yes',\n    'tion': 't',\n    'less': '',\n    'ness': '',\n    'ies': 'i',\n    'ying': 'i',\n    'ied': 'i',\n    'ings': '',\n    'est': '',\n    'ly': '',\n    'es': '',\n    'ing': '',\n    'er': '',\n    'ed': '',\n    's': '',\n    'y': 'i',\n}\n\ndef tokenize_sentence(any_string):\n    # Split the input string into words based on whitespace.\n    splitted_string = any_string.split()\n    \n    # Initialize a list to store the characters of each word.\n    char_based_tokens = []\n    \n    # Iterate over each word in the list.\n    for word in splitted_string:\n        # Convert each word into a list of its characters.\n        char_based_tokens.append([*word])\n    \n    # Process each word to remove non-alphabetic and non-digit characters.\n    for index, character_list in enumerate(char_based_tokens):\n        new_list = [*character_list]\n        \n        # Iterate over each character in the word.\n        for character in character_list:\n            # Remove non-alphabetic and non-digit characters.\n            if not (character.isalpha() or character.isdigit()):\n                new_list.remove(character)\n            continue\n        \n        # Join the characters back into a string and update the list.\n        char_based_tokens[index] = \"\".join(new_list)\n    \n    # Return the processed list of words.\n    return char_based_tokens\n\ndef stemmer(any_string):\n    # Tokenize the input string.\n    tokenized_sentence = tokenize_sentence(any_string)\n    \n    # Initialize a list to store the stemmed words.\n    stemmed_sentence = []\n    \n    # Iterate over each tokenized word.\n    for tokenized_word in tokenized_sentence:\n        stemmed_sentence_length = len(stemmed_sentence)\n        \n        # Iterate over the stemming rules.\n        for rule, addition in RULES.items():\n            word_length = len(tokenized_word)\n            rule_length = len(rule)\n            \n            # Extract the ending of the word based on the rule's length.\n            dropper = tokenized_word[word_length-rule_length:]\n\n            # Apply the stemming rule if the ending matches.\n            if dropper == rule:\n                stem = tokenized_word[0:word_length-rule_length] + addition\n                stemmed_sentence.append(stem)\n                break\n        \n        # If no rule was applied, append the original word.\n        if stemmed_sentence_length == len(stemmed_sentence):\n            stemmed_sentence.append(tokenized_word)\n\n    return(print(' '.join(stemmed_sentence)))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Jing_Wu.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    \n    words = any_string.split()\n    num_words = len(words)\n    tokens = ''.join(any_string.split())\n    num_tokens = len(tokens)\n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Jing_Wu.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # Load the lemmatization data from the file into a dictionary\n    lemma_dict = {}\n    with open(file_name, 'r', encoding='utf-8') as file:\n        for line in file:\n            word, lemma = line.strip().split('\\t')\n            lemma_dict[word] = lemma\n    \n    # Tokenize the input string by whitespace\n    words = any_string.split()\n    \n    # Create a dictionary of lemmatized words\n    dictionary_of_lemmatized_words = {}\n    for word in words:\n        # Lemmatize words if they exist in the lemma dictionary, else keep the word itself\n        lemma = lemma_dict.get(word, word)\n        dictionary_of_lemmatized_words[word] = lemma\n \n    return(print(dictionary_of_lemmatized_words))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Jing_Wu.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # List of rules for verb stemming\n    rules = [\n        (\"ies\", \"y\"),  # Present simple tense: Third person\n        (\"ing\", \"\"),   # Continuous tense\n        (\"ed\", \"\"),    # Past simple tense\n    ]\n    \n    # Tokenize the input string by whitespace\n    words = any_string.split()\n    \n    # Apply stemming rules to each word\n    stemmed_words = []\n    for word in words:\n        # Applying rules sequentially to find the stem\n        for rule in rules:\n            if word.endswith(rule[0]):\n                stemmed_word = word[: -len(rule[0])] + rule[1]\n                stemmed_words.append(stemmed_word)\n                break  # Break after first matching rule to prioritize rules\n                \n        # If no rule matches, use the original word\n        if word not in stemmed_words:\n            stemmed_words.append(word)\n    \n    # Create the stemmed string\n    stemmed_string = \" \".join(stemmed_words)\n    \n    return(print(stemmed_string))\nextract_words_tokens(\"yfuhu57687gfrdtcfyguil\")\nlemmatize(\"10\",'lemmatization-en.txt')\nstemmer(\"tasted\")\n"
    },
    {
        "file_name": "Assignment_1_Mahdi_Rahimianaraki_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # Splitting the string by spaces to get words\n    words = any_string.split()\n    num_words = len(words)\n\n    # Counting every character (including spaces and special characters) as a token\n    num_tokens = len(any_string)\n\n    return print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\")\n# Example usage:\nextract_words_tokens(\"Hello, world! This is a sample sentence. Please return the number of words and the number of tokens in this sentence.\")"
    },
    {
        "file_name": "Assignment_1_Mahdi_Rahimianaraki_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # Read the lemmatization file and build a dictionary\n    lemmas = {}\n    with open(file_name, 'r') as file:\n        for line in file:\n            word, lemma = line.strip().split('\\t')\n            lemmas[word] = lemma\n\n    # Tokenize the input string using whitespace\n    words = any_string.split()\n\n    # Create a dictionary to store the lemmas of the words in the input string\n    dictionary_of_lemmatized_words = {}\n    for word in words:\n        # Use the lemma if it's in the dictionary, otherwise use the word itself\n        dictionary_of_lemmatized_words[word] = lemmas.get(word, word)\n\n    return(print(dictionary_of_lemmatized_words))\n# Example usage:\nany_string = \"susceptibility spook disembowel invoice weedkiller talk traduce construct crib crumble\"\nfile_name = \"lemmatization-en.txt\"\nlemmatize(any_string, file_name)"
    },
    {
        "file_name": "Assignment_1_Mahdi_Rahimianaraki_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')  # Download the punkt tokenizer\n\ndef stemmer(any_string):\n    # Initialize the Porter Stemmer\n    stemmer = PorterStemmer()\n\n    # Tokenize the input string into words\n    words = word_tokenize(any_string)\n\n    # Apply stemming to each word\n    stemmed_words = [stemmer.stem(word) for word in words]\n\n    # Join the stemmed words into a string\n    stemmed_string = ' '.join(stemmed_words)\n\n    return(print(stemmed_string))\n# Example usage:\ninput_string = \"He studies programming and is currently studying Python at the University of Bonn.\"\nstemmer(input_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Suyash_Thapa.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):  \n    #here comes your code\n    num_words = len(any_string.split()) \n    num_tokens = len(any_string)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\")) \n\nextract_words_tokens(\"Suyash is handsome !\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Suyash_Thapa.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # Load lemmatization dictionary\n    lemmatization_dict = {}\n    with open(file_name, 'r', encoding='utf-8') as file:\n        for line in file:\n            word, lemma = line.strip().split('\\t')\n            lemmatization_dict[word] = lemma\n    \n    # Tokenize the input string using whitespace separator\n    string_sep = any_string.split() \n    \n    # Lemmatize each word\n    result_dict = {word: lemmatization_dict.get(word, word) for word in string_sep}\n    \n    print(result_dict)\n\n# Example usage:\nlemmatize(\"accede accede academics\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Suyash_Thapa.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from ntkl.stem import PorterStemmer\nimport re\n\ndef stemmer(input_string):\n    # Create a Porter Stemmer instance\n    porter_stemmer = PorterStemmer()\n\n    # Tokenize the input string using whitespace as the separator\n    words = re.findall(r'\\b\\w+\\b', input_string)  # Extract words while ignoring special characters\n\n    # Apply stemming to each word\n    stemmed_words = [porter_stemmer.stem(word) for word in words]\n\n    # Reconstruct the string with stemmed words\n    stemmed_string = ' '.join(stemmed_words)\n\n    return stemmed_string\n\n# Example usage:\ninput_string = \"He studies while studying and then studied the material.\"\nresult = stemmer(input_string)\nprint(result)"
    },
    {
        "file_name": "Assignment_1_Rouaa_Maadanli__Hossam_Fawzy_Mohamed_Elsafty.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # split the string into words then count the number of words\n    num_words = len(any_string.split())\n    # count the number of tokens based on characters-based\n    # assume the special characters like (\"!\",\",\",\":\") are tokens\n    num_tokens = len(any_string.replace(\" \", \"\"))\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\n# Test the function\nany_string = \"Hello over!\"\nextract_words_tokens(any_string)"
    },
    {
        "file_name": "Assignment_1_Rouaa_Maadanli__Hossam_Fawzy_Mohamed_Elsafty.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import csv\n\ndef lemmatize(any_string, file_name):\n    # Define the path of the file\n    file_path = 'lemmatization-en.txt'\n    # Create a dictionary to store the lemma map of the file\n    lemma_map = {}\n    with open(file_path, 'r',encoding='utf-8-sig') as file:\n        # Use the csv reader with tab delimiter\n        reader = csv.reader(file, delimiter='\\t')\n        for row in reader:\n            lemma_map[row[1]] = row[0]\n    \n    # tokenize the string using whitespace\n    tokens = any_string.split()\n    # create a dictionary to store the lemmatized words from the input string\n    dictionary_of_lemmatized_words = {}\n\n    for token in tokens:\n        # check if the token is in the lemma map\n\n        # if yes, then the key of the token will be the lemma\n        if token in lemma_map:\n            dictionary_of_lemmatized_words[token] = lemma_map[token]\n        # if no, then the key of the token will be the token itself\n        else:\n            dictionary_of_lemmatized_words[token] = token\n\n    return(print(dictionary_of_lemmatized_words))\n# Example\nlemmatize(\"cars crashing\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Assignment_1_Rouaa_Maadanli__Hossam_Fawzy_Mohamed_Elsafty.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n# implment the stemmer function using regex for these 4 rules\ndef stemmer(any_string):\n    # remove the suffix Continuous from the string\n    any_string = re.sub(r'ing\\b', '', any_string)\n    # remove the suffix Present simple from the string\n    any_string = re.sub(r'(s|es)\\b', '', any_string)\n    # remove the suffix Past simple from the string\n    any_string = re.sub(r'ed\\b', '', any_string)\n    # remove the suffix Infinitive form from the string\n    # remove y with i if it is not preceded by a vowel\n    stemmed_string = re.sub(r'(?<![aeiouAEIOU])y\\b', 'i', any_string)\n    \n    return(print(stemmed_string))\nstemmer(\"studying studies reaches played study\")\n# !pip install nltk\nimport nltk\nfrom nltk.stem import PorterStemmer\nimport re\n\n# implement the stemmer function using nltk\n# Initialize the stemmer\nport_stemmer = PorterStemmer()\n\ndef stemmer(any_string):\n    # tokenize the string\n    words = re.findall(r'\\b\\w+\\b', any_string)\n    # stem the words\n    stemmed_words = [port_stemmer.stem(word) for word in words]\n    # convert the list of stemmed words into a string\n    return ' '.join(stemmed_words)\n\n# Example \ninput_string = \"He studies while she is studying and studied yesterday.\"\nstemmed_string = stemmer(input_string)\nprint(stemmed_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef extract_words_tokens(any_string):\n    # To find the number of words we eliminate scpecial characters: ! , : ? . ( ) # \" from the string using the regex module\n    # We then split the resulting string by whitespace characters to get the word list\n    no_special = re.sub(r'[!,:\\?\\.\\(\\)#\\.\\\"]', ' ', any_string)\n    word_list = list(filter(bool, re.split(r\"\\s\", no_special)))\n    \n    # We simply store any non-whitespace character of the original string to get the token list\n    char_list = [ch for ch in list(any_string) if ch not in [\" \", \"\\t\"]]\n    \n    print(any_string, \":\", \"num_words:\", len(word_list), \"and\", \"num_tokens:\", len(char_list), \"respectively\")\n# Test for Task 1.1\ns = \"\"\"The first nine lines stores the first paragraph of the story, each line consisting of a series of character symbols. These elements don\u2019t contain any metadata or information to tell us which characters are words and which aren\u2019t: #Identifying these \"kinds\" of boundaries between words is where the process of tokenization comes in!\n\nIn tokenization, we take an input (a string) and a token type (a meaningful unit of text, such as a word) and split the input into pieces (tokens) that correspond to the type (Manning, Raghavan, and Sch\u00fctze 2008)? Figure 2.1 outlines this process!\"\"\"\nextract_words_tokens(s)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # This task can be better achieved by compiling the corpus into a more processable data-type (dict)\n    # We keep the words as the key and the lemma in the value of the dictionary\n    # We must note that as some of the words have multiple lemmas, we keep the lemmas in a list (\"belying\": [\"belie\", \"bely\"])\n    with open(file_name, 'r', encoding='utf-8-sig') as f:\n        corpus_dict = {}\n        for line in f.read().split('\\n'):\n            if line:\n                if line.split('\\t')[1] in corpus_dict:\n                    corpus_dict[line.split('\\t')[1]].append(line.split('\\t')[0])\n                else:\n                    corpus_dict[line.split('\\t')[1]] = [line.split('\\t')[0]]\n        \n    # We split the words by whitespaces\n    word_list = list(filter(bool, re.split(r\"\\s\", any_string)))\n    \n    # Lemmas of the words are looked up in the corpus, in case of multiple lemmas, they are concatinated by a |\n    lemmatized = {}\n    for word in word_list:\n        lemmatized[word] = '|'.join(corpus_dict[word])\n        \n    print(lemmatized)\n# Test for Task 1.2\nlemmatize(\"belying opening\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # Like Task 1.1 we get the word list\n    no_special = re.sub(r'[!,:\\?\\.\\(\\)#\\.\\\"]', ' ', any_string)\n    word_list = list(filter(bool, re.split(r\"\\s\", no_special)))\n    \n    stemmed_words = []\n    for word in word_list:\n        # A word is assumed to be the stem for itself, unless if it fits the following criteria\n        stemmed = word\n        \n        # infinitive:\n        # verbs ending with -y that do not proceed a vowel are replaced by -i (play -> play - study -> studi)\n        # -e postfixes are removed (decide -> decid)\n        if re.findall(r\"[^euioa]y$\", word):\n            stemmed = word[:-1] + \"i\"\n        elif re.findall(r\"e$\", word):\n            stemmed = word[:-1]\n\n        # simple third person:\n        # -es postfixes are removed\n        # -s postfixes are removed (pulls -> pull)\n        elif re.findall(r\"es$\", word):\n            stemmed = word[:-2]\n        elif re.findall(r\"s$\", word):\n            stemmed = word[:-1]\n\n        # continous\n        # -ing postfix is removed and then infinitive processing is redone (studying -> study -> studi)\n        # (agreeing -> agree -> agre)\n        elif re.findall(r\"ing$\", word):\n            tmp = word[:-3]\n\n            if re.findall(r\"[^euioa]y$\", tmp):\n                stemmed = tmp[:-1] + \"i\"\n            elif re.findall(r\"e$\", tmp):\n                stemmed = tmp[:-1]\n            else:\n                stemmed = tmp\n                \n        # past simple\n        # -ed postfix is removed\n        elif re.findall(r\"ed$\", word):\n            stemmed = word[:-2]\n\n        stemmed_words.append(stemmed)\n        \n    print(' '.join(stemmed_words))\n# Test for Task 1.3\nstemmer(\"decide decides decided deciding\")\nstemmer(\"study studies studying studied\")\nstemmer(\"love loves loved loving\")\nstemmer(\"raise raises raising raised\")\nstemmer(\"agree agrees agreeing agreed\")\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef extract_words_tokens(any_string):\n    #here comes your code\n    lines = any_string.splitlines()\n    num_words = 0\n    chars = set()\n    \n    # NOTE: as the problem statement is uclear, I'm assuming that \"words\" are not unique and the statement\n    # \"This is a a sentence\" has num_words = 5\n    \n    # NOTE: We can also use CountVectoriser but making a count vector is not the requirement here so not using it\n    \n    for line in lines:\n        # not considering new line character as a token\n        chars = chars.union(set(line))\n        \n        # not considering special characters as separate words.\n        # also, considering currency as a word; that is $40.30 is one word. Same for time: 12:45 is one word\n        # words joined by a special character are considered as single word: example \"play-acting\" is one word\n        words = line.strip().split(\" \")\n        \n        #remove special characters\n        for word in words:\n            word = re.sub(r'\\W+', '', word).strip()\n            # filter empty strings\n            if word:\n                num_words += 1\n    num_tokens = len(chars)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    #here comes your code\n    tokens = any_string.split(\" \")\n    word_dict = {}\n    final_dict = {}\n    with open(file_name, 'r', encoding='utf-8-sig') as file:\n        for line in file:\n            # remove a stray \\n character at the end of every line\n            words = line.splitlines()[0].split(\"\\t\")\n            word_dict[words[1]] = words[0]\n    \n    for token in tokens:\n        if token in word_dict:\n            final_dict[token] = word_dict[token]\n        else:\n            # if lemma does not exist in corpus, keep the original word\n            final_dict[token] = token\n    return(print(final_dict))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from nltk.stem import PorterStemmer\n\ndef stemmer(any_string):\n    stemmer = PorterStemmer()\n    new_lines = []\n    for line in any_string.splitlines():\n        words = line.strip().split(\" \")\n        stemmed = []\n        #remove stray special characters\n        for word in words:\n            word = re.sub(r'\\W+', '', word).strip()\n            if word:\n                stemmed.append(stemmer.stem(word))\n        # make sentence from stemmed words\n        new_lines.append(' '.join(w for w in stemmed))\n    # make text from stemmed sentences\n    stemmed_string = '\\n'.join(s for s in new_lines)\n    return(print(stemmed_string))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # Split words by spaces.\n    # Remove any strings that are empty.\n    words = any_string.split(\" \")\n    words = [word for word in words if word != \"\"]\n    num_words = len(words)\n    # Split tokens by characters and count the number of unique tokens.\n    num_tokens = len(set(any_string.split(\"\")))\n    return print(\n        any_string,\n        \":\",\n        \"num_words:\",\n        num_words,\n        \"and\",\n        \"num_tokens:\",\n        num_tokens,\n        \"respectively\",\n    )"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # Read file line by line.\n    with open(file_name, \"r\", encoding=\"utf-8-sig\") as f:\n        lines = f.readlines()\n        # Tokenize each line.\n        lines = [line.replace(\"\\n\", \"\").split(\"\\t\") for line in lines]\n    # Create a dictionary of lemmatized words.\n    lemma_dict = {}\n    for line in lines:\n        lemma = line[0]\n        word = line[1]\n        lemma_dict[word] = lemma\n    # Tokenize the input string.\n    tokens = any_string.split(\" \")\n    # Lemmatize each token.\n    dictionary_of_lemmatized_words = {}\n    for token in tokens:\n        # If the token is in the dictionary, replace it with the lemma.\n        if token in lemma_dict.keys():\n            dictionary_of_lemmatized_words[token] = lemma_dict[token]\n        # If the token is not in the dictionary, keep it as it is.\n        else:\n            dictionary_of_lemmatized_words[token] = token\n    return dictionary_of_lemmatized_words\n\n\nlemmatize(\"I am playing with my cats\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\n\ndef stemmer(any_string):\n    words = re.findall(\"[a-zA-Z]+\", any_string)\n\n    # Stemming rules\n    stemmed_words = []\n    for word in words:\n        if word.endswith((\"ies\", \"ing\", \"ed\")):\n            stem = re.sub(\"(ies|ing|ed)$\", \"i\", word)\n            stem = re.sub(\"(yi)$\", \"i\", stem)\n            stemmed_words.append(stem)\n        else:\n            stemmed_words.append(word)\n\n    stemmed_string = \" \".join(stemmed_words)\n\n    return print(stemmed_string)\n"
    },
    {
        "file_name": "Assignment_1_ZainulAbedin_ShahzebQamar.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    \"\"\"\n    Args:\n      any_string: Python String\n\n    Returns:\n      Python String\n    \"\"\"\n    # using split method to get a list of words present in the sentence.\n    word_list = any_string.split(\" \")\n\n    #here len method returns the number of words present in the sentence.\n    num_words = len(word_list)\n\n    #here len method returns the number of characters present in the sentence.\n    num_tokens = len(any_string)\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"my name is zain ul\")"
    },
    {
        "file_name": "Assignment_1_ZainulAbedin_ShahzebQamar.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n\n    vocab = dict() # declaring our vocabulary of words and lemmas with words as keys and lemma as values\n    dictionary_of_lemmatized_words = dict() # a map to store the lemmatized words\n\n    with open(file_name, 'r',encoding='utf-8-sig') as file:\n      for line in file:\n        key,value = line.strip().split('\\t') # stripping(removing any extra spaces at the beginning or end) and splitting the line on the basis of tab\n        vocab[value.lower()] = key.lower() # storing the word (lowercase) as key and lemma(lowercase) as value\n\n    for string in any_string.split(' '):\n      if string.lower() in vocab: # checking whether the word of a sentence(in lowercase) is present in the lemma vocabulary\n        dictionary_of_lemmatized_words[string] = vocab[string.lower()] # if present, assigned lemma of it\n      else:\n        dictionary_of_lemmatized_words[string] = string # else the word is lemma itself\n\n    return(print(dictionary_of_lemmatized_words))\nlemmatize('The striped bats are hanging on their feet for best','lemmatization-en.txt')"
    },
    {
        "file_name": "Assignment_1_ZainulAbedin_ShahzebQamar.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef stemmer(sentence):\n    # Define rules for stemming\n    rules = [\n        (r'y$', r'i'),             # Infinitive form\n        (r'ies$', r'i'),           # Present simple tense: Third person\n        (r'ying$', r'i'),           # Continuous tense\n        (r'ing$', r'i'),           # Continuous tense\n        (r'ied$', r'i'),            # Past simple tense\n    ]\n\n    # Split the sentence into words\n    words =  re.split(r'[,\\s!:]', sentence) # split on the basis of ' ','!',',',':'\n\n    # Apply rules to each word\n    stemmed_words = []\n    for word in words:\n      if len(word) == 0: # incase of empty strings after splitting\n        continue\n      for pattern, replacement in rules:\n          if len(word) <= 3: # assuming that words with length less than 3 are already stem\n            continue\n          word = re.sub(pattern, replacement, word)\n      stemmed_words.append(word)\n\n    # Join the stemmed words back into a sentence\n    stemmed_string = ' '.join(stemmed_words)\n\n    return(print(stemmed_string))\nstemmer(\"My boy is not studying and sleeping\")\nstemmer(\"My boy is eating\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Shruti_Nair_Ahmed_Javed_1.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    \"\"\"\n    This function takes a string as input and performs two operations: word extraction and character-based tokenization.\n    Parameters:\n    any_string:The input string from which tokens will be extracted\n    Returns:\n    None\n    The function prints the input string, the number of words in the string, and the number of tokens (non-space characters)\n    \"\"\"\n    # Extract words\n    words = any_string.split()\n    num_words = len(words)\n\n    # Tokenize the string using character-based tokenization\n    tokens = list(any_string)\n    tokens = [character for character in tokens if character != \" \"]\n    num_tokens = len(tokens)\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens('Hello, there!, This is a test.')\nextract_words_tokens(\"Hello, All!! This is NLP Assignment.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Shruti_Nair_Ahmed_Javed_1.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    \"\"\"\n    This function performs lemmatization on a given input string using a provided lemma file.\n    Parameters:\n    any_string: The input string to be lemmatized.\n    file_name: The name of the lemma file containing mappings of words to their lemmatized forms.\n\n    Returns:\n    None\n    The results are printed as a dictionary of original words and their\n    lemmatized forms.\n    \"\"\"\n    #encoding reference : https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n    with open(file_name, 'r', encoding='utf-8-sig') as file:\n        lines = file.readlines()\n        lemma_dict = {}\n        for line in lines:\n            word, lemma = line.split('\\t')\n            lemma = lemma.strip()\n            lemma_dict[lemma] = word\n\n    word_list = any_string.split()\n\n    # Lemmatize each word using the provided lemma dictionary\n    dictionary_of_lemmatized_words = {word:lemma_dict.get(word, word) for word in word_list}\n    return(print(dictionary_of_lemmatized_words))\nlemmatize(\"craws crazier first environmentalists\", 'lemmatization-en.txt')\nlemmatize('The accountant was accounting the records and it accumulated to sixteenth abnormalities', 'lemmatization-en.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Shruti_Nair_Ahmed_Javed_1.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef stemmer(any_string):\n    \"\"\"\n    This function performs stemming on a given input string using a set of predefined rules.\n    Parameters:\n    any_string: The input string to be stemmed.\n\n    Returns:\n    None\n    The function tokenizes the input string using regular expressions to extract words and applies a set of rules for stemming.\n    \"\"\"\n\n    #Create a list of tokenized words using regular expression to remove all special characters and get only words\n    tokenized_words = re.findall(r'\\b\\w+\\b', any_string)\n    # Rules for stemming using regular expression\n    stemmed_words = []\n    rules = [\n        (r'y$', 'i'),            # Infinitive form\n        (r'es$', ''),           # Present simple tense: Third person\n        (r'ying$', 'i'),        # Continuous tense\n        (r'ed$', ''),           # Past simple tense\n    ]\n\n    for word in tokenized_words:\n        # Applying rules to every word\n        for pattern, replace_with in rules:\n            word = re.sub(pattern, replace_with, word)\n\n        # Store the stemmed word in the list\n        stemmed_words.append(word)\n\n    # Rephrasing the string with stemmed words\n    stemmed_string = ' '.join(stemmed_words)\n\n    return(print(stemmed_string))\nstemmer(\"I am studying the subject that he studied and will study so as to continue with studies\")\n"
    },
    {
        "file_name": "Assignment_1_Ilhom_Khalimov.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string: str):\n    #here comes your code\n    words = any_string.split() # we could use word_tokenize from NLTK, but then we'd need to filter punctuation\n    num_words = len(words)\n    num_tokens = len(any_string)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\ns = \"12345!   qwer. U.S.\"\nextract_words_tokens(s)"
    },
    {
        "file_name": "Assignment_1_Ilhom_Khalimov.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string: str, file_name: str):\n    #here comes your code\n    with open(file_name, 'r', encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        dict = {} # word-to-lemma mapping\n        for line in lines:\n            lemma, word = line.split()\n            dict[word] = lemma\n        dictionary_of_lemmatized_words = {}\n        words = any_string.split()\n        # find lemma for each word\n        for word in words:\n            dictionary_of_lemmatized_words[word] = dict[word]\n    return(print(dictionary_of_lemmatized_words))\n\nlemmatize(\"algorithms are alienated analysers best appalled computers\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Assignment_1_Ilhom_Khalimov.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport string\n\n# I was not sure if we were to use libraries like NLTK\n# or implement our own functions, so I provided both\n\nPUNCTUATION = string.punctuation\nVERB_CODES = {\n    'VB',  # Verb, base form\n    'VBD',  # Verb, past tense\n    'VBG',  # Verb, gerund or present participle\n    'VBN',  # Verb, past participle\n    'VBP',  # Verb, non-3rd person singular present\n    'VBZ',  # Verb, 3rd person singular present\n}\n\n# Porter Stemmer\ndef stemmer(any_string: str):\n    stm = PorterStemmer()\n    stemmed = [stm.stem(word) for word in word_tokenize(any_string)]\n    stemmed_string = ' '.join(stemmed)\n    return(print(stemmed_string))\n\n# NLTK for POS guessing\ndef verb_stemmer(any_string: str):\n    tokens = word_tokenize(any_string)\n    tagged = nltk.pos_tag(tokens)\n    stemmed: list[str] = []\n    for word, pos in tagged:\n        if pos not in VERB_CODES:\n            stemmed.append(word)\n        else:\n            stem = word\n            match pos:\n                case 'VB' | 'VBP': # study, make, create\n                    if word[-1] == 'y':\n                        stem = word[:-1]\n                        stem += 'i'\n                case 'VBD' | 'VPN': # studied, created\n                    if word[-2:] == 'ed':\n                        stem = word[:-2]\n                case 'VBG': # studying, making, creating\n                    stem = word[:-3]\n                    if stem[-1] == 'y': # study[ing]\n                        stem = stem[:-1]\n                        stem += 'i'\n                case 'VBZ': # studies, makes, creates, reads, hits\n                    if word[-2:] == 'es': # studies, makes, creates\n                        stem = word[:-2]\n                    elif word[-1] == 's': # reads, hits\n                        stem = word[:-1]\n                case _:\n                    stem = word\n            stemmed.append(stem)\n        \n    stemmed_string = ' '.join(stemmed)\n    return(print(stemmed_string))\n\n\n# NLTK only for tokenization\n# disregards POS\ndef naive_stemmer(any_string: str):\n    tokens = word_tokenize(any_string)\n    stemmed: list[str] = []\n    for word in tokens:\n            # get rid of punctuation at the end\n            # if we can't use NLTK's tokenizer\n            # if word[-1] in PUNCTUATION:\n            #     word = word[:-1]\n\n            # study, make, create\n            if word[-1] == 'y':\n                stem = word[:-1]\n                stem += 'i'\n            # studied, created\n            elif word[-2:] == 'ed':\n                stem = word[:-2]\n            # studying, making, creating\n            elif word[-3:] == 'ing':\n                stem = word[:-3]\n                if stem[-1] == 'y':\n                    stem = stem[:-1]\n                    stem += 'i'\n            # studies, makes, creates\n            elif word[-2:] == 'es':\n                stem = word[:-2]\n            # reads, hits\n            elif word[-1] == 's':\n                stem = word[:-1]\n            else:\n                stem = word\n            stemmed.append(stem)\n        \n    stemmed_string = ' '.join(stemmed)\n    return(print(stemmed_string))\n\ns = \"I studied, I study, am studying just like he studies\"\nstemmer(s) # Porter stemmer\nverb_stemmer(s) # Rule-based stemmer for verbs, gets POS from NLTK\nnaive_stemmer(s) # Purely rule-based stemmer, disregards POS"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    num_words = len(any_string.split(\" \"))\n    num_tokens = len(any_string)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nmy_string = \"hello world!\"\nextract_words_tokens(my_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # initializing the dictionary which will have all the mapping\n    dictionary_of_lemmatized_words = {}\n    with open(file_name, 'r', encoding='utf-8') as my_file:\n        # populating the dictionary with the contents of the file\n        for line in my_file:\n            # using strip() to exclude irrelevant white spaces\n            lemma, word = line.strip().split(\"\\t\")\n            dictionary_of_lemmatized_words[word] = lemma\n    \n    # separating the input string\n    tokens = any_string.split(\" \")\n\n    # creating a dictionary corresponding to the input string\n    lemmatized_dict = { token:dictionary_of_lemmatized_words[token] for token in tokens }\n\n    return(print(lemmatized_dict))\ninput_string = \"emphasising an abandoning energizing\"\nlemmatization_file = \"lemmatization-en.txt\"\n\nresult = lemmatize(input_string, lemmatization_file)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef remove_special_characters(input_string):\n    # defining a regex pattern to match special characters\n    pattern = r'[^a-zA-Z0-9\\s]'\n    return re.sub(pattern, ' ', input_string)\n\n\ndef stemmer(any_string):\n    rules = [\n        ('y', 'i'),\n        ('ies', 'i'),          # Present simple tense: Third person\n        ('ying', 'i'),           # Continuous tense\n        ('ied', 'i'),            # Past simple tense\n    ]\n\n    # replacing special characters with a white space\n    input_string = remove_special_characters(any_string)\n\n    # tokenize the input string using white spaces as separators\n    tokens = input_string.split(\" \")\n\n    # filter the empty string form the list\n    tokens = [token for token in tokens if token != '']\n\n    print(tokens)\n\n    # Apply stemming rules to each token\n    stemmed_tokens = []\n    for token in tokens:\n        for pattern, replacement in rules:\n            if token.endswith(pattern):\n                token = token[:-len(pattern)] + replacement\n                \n        stemmed_tokens.append(token)\n\n    # Reconstruct the stemmed string\n    stemmed_string = ' '.join(stemmed_tokens)\n    return(print(stemmed_string))\ninput_string = \"study: studies studying,studied!\"\nstemmed_string = stemmer(input_string)"
    },
    {
        "file_name": "Assignment_1_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\ndef extract_words_tokens(any_string):\n    ## Get number of words using the len() on the split() method.\n    num_words = len(any_string.split())\n\n    ## Get number of tokens using the len() on the word_tokenize() method from the nltk library.\n    num_tokens = len(word_tokenize(any_string))\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"The quick briwn fox jumps over the lazy dog.\")\nextract_words_tokens(\"Hello, World!\")\nextract_words_tokens(\"Hello\")"
    },
    {
        "file_name": "Assignment_1_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n\n    ## Creating the lemmatization corpus from the file\n    ## For this I read all the lines of the file and create a dictionary of the corpus\n    ## To handle the case for the same word having multiple lemas I changed the value to be a list of all the lemmas of the word\n    lemmatization_corpus = {}\n    with open(file_name, 'r') as corpus_file:\n      for line in corpus_file:\n        word, lemma = line.strip().split('\\t')\n        if word not in lemmatization_corpus:\n          lemmatization_corpus[word] = []\n        lemmatization_corpus[word].append(lemma)\n\n    ## Create a dictionary of lemmas for each word in the provided string\n    dictionary_of_lemmatized_words={}\n\n    ## Use the nltk tokenizer to get words\n    words = word_tokenize(any_string.lower())\n\n    ## Find the lemma of the words from the lemmatization_corpus\n    for word in words:\n      if word in lemmatization_corpus:\n        dictionary_of_lemmatized_words[word] = lemmatization_corpus[word]\n      else:\n        dictionary_of_lemmatized_words[word] = [word]\n\n    return(print(dictionary_of_lemmatized_words))\nlemmatize(\"a quick brown fox jumos over the lazy dog.\", 'lemmatization-en.txt')\nlemmatize(\"Hello, World!\", 'lemmatization-en.txt')\nlemmatize(\"hello\", 'lemmatization-en.txt')"
    },
    {
        "file_name": "Assignment_1_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def rules(string):\n  if string.endswith('y'):\n    string = string[:-1] + 'i'\n\n  # Present simple tense: Third person:\n  if string.endswith('es'):\n    string = string[:-2]\n\n  # Continuous tense:\n  if string.endswith('ing'):\n    string = string[:-3]\n\n  # Past simple tense:\n  if string.endswith('ed'):\n    string = string[:-2]\n\n  return string\ndef stemmer(any_string):\n    #here comes your code\n    tokens = word_tokenize(any_string)\n    stemmed_string = \"\"\n    for token in tokens:\n      word = rules(token)\n      stemmed_string = stemmed_string + \" \" + word\n    return(print(stemmed_string))\nstemmer(\"I am studying CS\")\nstemmer(\"a quick brown fox is jumping over the lazy dogs\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Yagmur_Caglar_1.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    #here comes your code\n    num_words = len(any_string.split())     #break the sentence wrt spaces\n    tokens = ''.join(any_string.split())    #break the sentence wrt spaces then concat without space\n    num_tokens = len(tokens)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"can contain some special charecters, such as:\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Yagmur_Caglar_1.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    #here comes your code\n    lem_dict = {}\n    f = open(file_name, 'r')\n    for line in f.readlines():\n      lemma, word = line.strip().split('\\t')  #file in form of lemma, word. taking values from file\n      lem_dict[word] = lemma\n\n    words = any_string.split()  #seperating input wrt space\n    dictionary_of_lemmatized_words = {}\n    for word in words:\n      lemma = lem_dict.get(word, word)  #get the word of input from the file dictionary\n      dictionary_of_lemmatized_words[word] = lemma  #assign the lemma to word\n\n    return(print(dictionary_of_lemmatized_words))\nlemmatize(\"abased\", \"/content/lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Yagmur_Caglar_1.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    #here comes your code\n    #assigning rules: transform the key into value\n    rules = {\"y\":\"i\", \"ies\":\"i\", \"ying\":\"i\", \"ing\": \"\", \"ed\":\"\"}\n\n    words = any_string.split()\n    stem = []\n    for word in words:\n      flag = 0\n      for suffix, replacement in rules.items():\n        if word.endswith(suffix):    #if the input has the suffix that matches with the rule list, stem the word with the new version\n           stem.append(word[: -len(suffix)] + replacement)\n           flag=1                      #the first rule is prioritized, oes not look at the rest\n           break\n      if not flag:            #word does not fit any rules, add as it is\n        stem.append(word)\n    print(stem)\n    stemmed_string = \" \".join(stem)\n    return(print(stemmed_string))\nstemmer('studies and studying is hard')\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Abhishek_Murtuza.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    \n    #Creating list of words from given string\n    words = any_string.split()\n    #Calculating number of words\n    num_words = len(words)\n    #Tokening string according to character-based tokenization\n    tokens = list(any_string)\n    #Removing empty character from list of tokens\n    tokens = [item for item in tokens if item != \" \"]\n    #Calculating length of tokens in string\n    num_tokens = len(tokens)\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"Hello, world! This is an example.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Abhishek_Murtuza.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import nltk\ndef lemmatize(any_string, file_name):\n    \n    #Generate lemma dictionary from corpus file provided\n    dict_lemma = {}\n    with open(file_name,'r',encoding='utf-8') as file:\n        for line in file:\n            lemma, word = line.strip().split('\\t')\n            dict_lemma[word] = lemma\n    # Tokenize input string into words\n    tokenized_words  = any_string.strip().split()\n    \n    #Create list with lemmatized_words using values from dict_lemma and keys of tokenized words from input string \n    lemmatized_words = [dict_lemma.get(word,word) for word in tokenized_words ]\n    \n    #Create dictionary with keys as tokenized words in input string and their corresponding value of lemmatized word\n    dictionary_of_lemmatized_words = dict(zip(tokenized_words,lemmatized_words))\n    \n    return(print(dictionary_of_lemmatized_words))\n\nlemmatize(\"billionth abnormalities aborted by academicians\",\"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Abhishek_Murtuza.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\ndef stemmer(any_string):\n    #Create list of tokenized words using regular expression to remove all special characters and get whole words only\n    tokenized_words = re.findall(r'\\b\\w+\\b', any_string)\n    # Define rules for stemming\n    stemmed_words = []\n    rules = [\n        (r'ying$', 'i'),        # Continuous tense\n        (r'es$', ''),            # Present simple tense: Third person\n        (r'ed$', ''),           # Past simple tense\n        (r'y$', 'i')            # Infinitive form\n    ]\n    \n    for word in tokenized_words:\n        # Apply rules to each word\n        for pattern, replacement in rules:\n            word = re.sub(pattern, replacement, word)\n        \n        # Store the stemmed word in the list\n        stemmed_words.append(word)\n        \n    # Reconstruct the string with stemmed words\n    stemmed_string = ' '.join(stemmed_words)\n    return(print(stemmed_string))\ninput_string = \"He is studying and has studied and will study and continue to do studies\"\noutput_string = stemmer(input_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # split sentennces into words to count them\n    num_words =len(any_string.split())\n    \n    # remove empty spaces in order not to be considred a single word and count characters only\n    num_tokens=len(list(set(any_string.replace(\" \",\"\"))))\n    \n    #returning the variables\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"this is a test, don't push it!!\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n\n  # d is used as the dictionary of the given file (lemmatization-en.txt) \n  my_dictionary = {}\n  \n  # dictionary_of_lemmatized_words is the dictionary which we will return to the user\n  dictionary_of_lemmatized_words = {}\n\n  # loading the given dictionary into \"d\"  from the file \"lemmatization-en.txt\"\n  with open(file_name, 'r', encoding='utf-8-sig') as my_file:\n    for line in my_file:\n        x = line.split()\n        my_dictionary[x[1]]=x[0]\n\n  # tokenizing the string \"any_string\" into the list \"l\" (we get every word of the string into the list)\n  my_list =any_string.split()\n\n\n  #checking if the words of the string \"any_string\" are in the values or keys of the given dictionary (file lemmatization-en.txt)\n  # if it is found in the dictionary the value is set to the found value/key\n  # if it isn't found in the dictionary the value is set to \"not found in file\"\n  for word in my_list:\n    if word in my_dictionary :\n      dictionary_of_lemmatized_words[word] = my_dictionary[word]\n    elif word in my_dictionary.values():\n      dictionary_of_lemmatized_words[word] = word\n    else :\n      dictionary_of_lemmatized_words[word] = \"Not found\"\n\n  #printing the dictionary_of_lemmatized_words to the user     \n  return(print(dictionary_of_lemmatized_words))\n\nany_string = str()\nlemmatize(\"swimming and surfing are nice activities\",\"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\na\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "nltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem import PorterStemmer\n\ndef stemmer(any_string):\n  # string which will be printed to the user with only the stemms\n  stemmed_string= str()\n  \n  # initializing the class used for stemming\n  stemmer = PorterStemmer()\n  \n  # removing every special charecter but the whitespace with regex and splitting the string into word tokens into the list \"l\"\n  tokenized_string=nltk.word_tokenize(any_string)\n  \n  # stemming every word and adding it to the string \"stemmed_string\"\n  for word in tokenized_string:\n    stemmed_string = stemmed_string +\" \"+ stemmer.stem(word)\n  \n  # printing the stemmed string to the user\n  return(print(stemmed_string))\n\nstemmer(\"Stemming is a rule-based methodology that displays multiple variants of the same base word. ! The approach reduces the base word to its stem word. This is simpler as it involves indiscriminate reduction of the word-ends. Stemming in Python normalizes the sentences and shortens the search result for a more transparent understanding.\")\n"
    },
    {
        "file_name": "Assignment_1_Bondarenko_Nikita.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re \n\ndef extract_words_tokens(any_string):\n    \n    words = any_string.replace('!', ' ').replace(',', ' ').replace(':', ' ').replace(\"'\",' ').replace('?', ' ').split()\n    # or we could use regex to replace any special character:\n    #words = re.sub(r'\\W+', ' ', any_string).split()\n    \n    # ! In this case we treat contractions (e.g. 'Let's', 'They're') as 2 words, if we want to treat them as 1, we would\n    # probably need to remove \"'\" as special character. Or have a dictionary of all contractions ready to exclude the count.\n    num_words = len(words)\n    \n    # Anything is a token besides whitespace.\n    num_tokens = len(any_string.replace(' ', ''))\n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
    },
    {
        "file_name": "Assignment_1_Bondarenko_Nikita.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    \n    # first we read the lemmatization file and build a mapping dictionary\n    lemma_dict = {}\n    with open(file_name, 'r', encoding='utf-8-sig') as file:\n        for line in file:\n            parts = line.strip().split('\\t')\n            lemma, word = parts\n            lemma_dict[word] = lemma\n    \n    # tokenazing the input string\n    tokens = any_string.split(' ')\n    \n    # make a dict with words as keys and lemmas as values\n    dictionary_of_lemmatized_words = {word: lemma_dict.get(word, word) for word in tokens}\n    \n    #* Also not sure, but in the file there is a pair of \"i - i's\" what word is it supposed to be? \"I'm\"?\n    return(print(dictionary_of_lemmatized_words))"
    },
    {
        "file_name": "Assignment_1_Bondarenko_Nikita.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    \n    suffixes = {'y': 'i',  'ies': 'i',  'ying': 'i','ied': 'i'}\n    \n    # tokenize the string on whitespace and special characters\n    tokens = re.findall(r\"\\b\\w+\\b\", any_string)\n    \n    # for each token try to stem based on the defined suffixes\n    stemmed_tokens = []\n    for token in tokens:\n        stemmed_token = token\n        for suffix in suffixes:\n            if stemmed_token.endswith(suffix):\n                stemmed_token = re.sub(suffix + \"$\", suffixes[suffix], token)\n                break  # If we find a matching suffix, break out of the loop\n        stemmed_tokens.append(stemmed_token)\n        \n    # join back to string\n    stemmed_string = ' '.join(stemmed_tokens)\n    \n    #* Here the problem is that nouns or any words ending with y will be transformed: today -> todai. So we need to consider\n    # context, using stem library of some sorts.\n    return(print(stemmed_string))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Dobberstein_Niklas.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\ndef clean_string(txt : str) -> str:\n    cleaned_string = re.sub(r'[!,:\\.]', '', txt)\n    cleaned_string = re.sub(\"\\\\s+\", \" \",cleaned_string).strip()\n    return cleaned_string\n\ndef extract_words_tokens(any_string : str):\n\n    #NOTE: assuming that also \"!\", \",\", \":\" and \" \" are valid tokens \n    num_tokens = len(set(any_string))\n    cleaned_string = clean_string(any_string)\n    num_words = len(cleaned_string.split(\" \"))\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\nextract_words_tokens(\"test this is .\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Dobberstein_Niklas.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "import pandas as pd\n\n\ndef lemmatize(any_string : str, file_name : str):\n    df = pd.read_csv(file_name, sep=\"\\t\", names=[\"lemma\", \"word\"])\n    cleaned_string = clean_string(any_string)\n    words = cleaned_string.split(\" \")\n    matches = df[df[\"word\"].isin(words)]\n    dictionary_of_lemmatized_words = {\n        row[\"word\"]: row[\"lemma\"] for _,row in matches.iterrows()\n    }\n\n    return(print(dictionary_of_lemmatized_words))\n\nlemmatize(\"abjures accessed\", \"lemmatization-en.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Dobberstein_Niklas.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "from nltk.stem import PorterStemmer\n\ndef stemmer(any_string : str):\n    cleaned_str = clean_string(any_string)\n    rules = [\n        (r'(.*)(y)$', r'\\1i'),  \n        (r'(.*)(ies)$', r'\\1i'),         \n        (r'(.*)(ying)$', r'\\1i'),        \n        (r'(.*)(ied)$', r'\\1i'),         \n    ]\n\n    # stemmer = PorterStemmer()\n    \n    # stemmed_string_list = [stemmer.stem(word) for word in cleaned_str.split(\" \")]\n    # Apply the rules to the word\n    \n    stemmed_string_list = []\n    for word in cleaned_str.split(\" \"):\n        for pattern, replacement in rules:\n            word = re.sub(pattern, replacement, word)\n        \n        stemmed_string_list.append(word)\n\n    stemmed_string = \" \".join(stemmed_string_list)\n    #here comes your code\n    return(print(stemmed_string))\n\nstemmer(\"He studies, while she is studying, and they studied.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Carl_Jacobsen.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    #here comes your code\n    num_words = len(any_string.split()) #If we assume correct grammar, then any ., !, ... is allways direct after a word.\n    num_tokens = len(any_string)\n    \n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"Test of test.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Carl_Jacobsen.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    #here comes your code\n    with open(file_name) as file:\n        dictionary_of_lemmatized_words = {}\n    return(print(dictionary_of_lemmatized_words))"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Carl_Jacobsen.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    #here comes your code\n    stemmed_string = \"\"\n    words = any_string.split()\n    for word in words:\n        if word[-1] in [\"!\", \",\", \".\"]: #remove !, , and ..\n            word = word[:-1]\n        if word[-1] == \"y\": #Infinitive form\n            word = word[:-1] + \"i\"\n        elif word[-3:] == \"ies\":#Present simple tense\n            word = word[:-2]\n        elif word[-4:] == \"ying\":#Continuous tense\n            word = word[:-4] + \"i\"\n        elif word[-3:] == \"ied\":#Past simple tense\n            word = word[:-2]\n        stemmed_string += word + \" \"\n    return(print(stemmed_string))\nstemmer(\"study studied.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    # remove special characters from the string, and convert it to lowercase\n    any_string = ''.join(e.lower() for e in any_string if e.isalnum() or e == \" \")\n\n    # split the string into words and create a set of tokens\n    words = any_string.split()\n    tokens = set(words)\n\n    print(any_string, \":\", \"num_words:\", len(words), \"and\", \"num_tokens:\", len(tokens), \"respectively\", \"words:\", words, \"tokens:\", tokens)\n\n    return len(words), len(tokens), words, tokens\n\n_ = extract_words_tokens(\n    'This is a string! It contains some words and some tokens. Tokens are the same as words, but they are not.'\n)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    # load the lemmatization dictionary\n    with open(file_name, 'r') as f:\n        lemmatization_dict = {line.split('\\t')[1].strip(): line.split('\\t')[0] for line in f}\n\n    # extract tokens from the string\n    _, _, _, tokens = extract_words_tokens(any_string)\n\n    # create a dictionary with the lemmatized words\n    dictionary_of_lemmatized_words = {}\n\n    # for each token in the string, check if it's in the lemmatization dictionary\n    # if it is, add it to the dictionary with the lemma as the value\n    # if it's not, add it to the dictionary with the token as the value\n    for token in tokens:\n        if token in lemmatization_dict:\n            dictionary_of_lemmatized_words[token] = lemmatization_dict[token]\n        else:\n            dictionary_of_lemmatized_words[token] = token\n\n    return dictionary_of_lemmatized_words\n\nlemmatize('This is a string! It contains some words and some tokens. Tokens are the same as words, but they are not.', 'lemmatization-en.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # extract tokens from the string\n    _, _, _, tokens = extract_words_tokens(any_string)\n\n    # create a list of stemmed words\n    stemmed_words = []\n    for token in tokens:\n       # if the token ends with 'y', remove the 'y' and add 'i' to the end\n        if token.endswith('y'):\n            stemmed_words.append(token[:-1] + 'i')\n        # if the token ends with 'es', remove the 'es'\n        elif token.endswith('es'):\n            stemmed_words.append(token[:-2])\n        # if the token ends with 'ying', remove the 'ying' and add 'i' to the end\n        elif token.endswith('ying'):\n            stemmed_words.append(token[:-4] + 'i')\n        # if the token ends with 'ed', remove the 'ed'\n        elif token.endswith('ed'):\n            stemmed_words.append(token[:-2])\n        else:\n            stemmed_words.append(token)\n    return(print(stemmed_words))\n\nstemmer('Studying, studies, studied, study')\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 1.1 (3 points)\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def remove_punctuation(input_string):\n    punctuation_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n    cleaned_string = ''.join(char for char in input_string if char not in punctuation_chars)\n\n    return cleaned_string\n\n# check non alphabetic characters and keep track of their index\ndef has_special_characters(word):\n    for i, char in enumerate(word):\n        if not char.isalnum():\n            return i, char\n    return False, False\ndef extract_words_tokens(any_string):\n    # count the number of words\n    # we remove the sepcial characters because they are not supposed to count as words\n    num_words = len(remove_punctuation(any_string).split())\n\n    # tokenize the input based on characters and count the number of tokens\n    num_tokens = len(list(any_string))\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n\nexample_string = \"Hello, World!. How is your day ?\"\nextract_words_tokens(example_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 1.2 (4 points)\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n    lemmatization_dict = {}\n    with open(file_name, 'r', encoding='utf-8') as file:\n        for line in file:\n            # I am not sure if we should swap (lemma and word)\n            lemma, word = line.strip().split('\\t')\n            lemmatization_dict[word] = lemma\n\n    words = any_string.split()\n    lemmatized_words = [lemmatization_dict.get(word, word) for word in words]\n    dictionary_of_lemmatized_words = dict(zip(words, lemmatized_words))\n    return(print(dictionary_of_lemmatized_words))\n\nfile_name = \"lemmatization-en.txt\"\nexample_string = \"cats runnings running better\"\nlemmatize(example_string, file_name)"
    },
    {
        "file_name": "Intro2NLP_Assignment_1_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 1.3 (3 points)\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\nCreate rules for the following forms of the verbs, Here is one example:\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def stemmer(any_string):\n    # define stemming rules for verb forms\n    stemming_rules = [\n        (\"ied\", \"i\"), # past simple\n        (\"ed\", \"\"), # past simple\n        (\"ying\", \"i\"), # continuous\n        (\"ing\", \"\"), # continuous\n        (\"ies\", \"i\"), # present simple\n        (\"s\", \"\"), # present simple\n        (\"y\", \"i\") # infinitive\n    ]\n\n    # tokenize words\n    words = any_string.split()\n\n    # apply stemming rules to each word\n    stemmed_words = []\n    for word in words:\n        # find if the word has special characters and keep track of it\n        sc_position, sc = has_special_characters(word)\n        # check each rule\n        for rule in stemming_rules:\n            # if word contains special_character remove it\n            if sc_position:\n                word = remove_punctuation(word)\n            # apply the rule\n            if word.endswith(rule[0]):\n                stemmed_word = word[: -len(rule[0])] + rule[1]\n                # add back the special character if exist\n                if sc_position:\n                    stemmed_word = stemmed_word[:sc_position] + sc + stemmed_word[sc_position:]\n                stemmed_words.append(stemmed_word)\n                break\n        else:\n            # If no rule applies, keep the original word\n            stemmed_words.append(word)\n\n    # join the stemmed words\n    stemmed_string = \" \".join(stemmed_words)\n\n    print(stemmed_string)\n\nexample_string = \"study studies studying, studied eats eating eat.\"\nstemmer(example_string)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_AylinGheisar.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# We just need to read the csv file and specify a custom delimeter for pandas to parse the file correctly\npolarity = pd.read_csv(\"polarity.txt\", delimiter='\t', names=[\"Text\", \"Label\"])\npolarity.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_AylinGheisar.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "polarity[\"Label_int\"] = polarity[\"Label\"].apply(lambda x: 0 if x==\"neg\" else 1)\npolarity.drop(\"Label\", axis=1, inplace=True)\npolarity.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_AylinGheisar.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "from collections import Counter\nimport re\n\n\ndef word_seperator(string):\n    # This function returns a list of all words in a string as a list\n    words_list = re.findall(r\"[\\w']+\", string)\n    \n    return words_list\n\ndef create_count_and_probability(file_name):\n    # We need all the unique words in the document to construct the word vectors because we are not aware of the missing words\n    # just by getting a single documents\n    with open(file_name, 'r') as f:\n        word_list = [word.lower() for word in word_seperator(f.read())]\n        unique_words = sorted(list(set(word_list)))\n\n    # The results are stored in this dictionary as we can easily convert it into a dataframe object\n    Result = {\n        \"Document\": [],\n        \"Word_Vector\": [],\n        \"Probability\": []\n    }\n\n    # We will need this dataframe as a template to sort word vectors and fill in the missing words as 0\n    count_vectors = pd.DataFrame(columns=unique_words)\n\n    with open(\"corpus.txt\", 'r') as f:\n        for line in f.readlines():\n            word_list = [word.lower() for word in word_seperator(line)]\n            word_freq = Counter(word_list)\n\n            # Calculating the probability\n            word_num = len(word_list)\n            word_prob = [f\"{word_freq[word]}/{word_num}\" for word in word_list]\n\n            # Constructing the word vector\n            word_freq = pd.Series(word_freq, dtype=\"int64\")\n            \n            # We merge the word frequencies to the template to put them in the same order and make room for missing words in document\n            merged_vector = pd.concat([count_vectors, word_freq.to_frame().T])\n            \n            # The missing values get a value of 0\n            merged_vector = merged_vector.fillna(0)\n            word_vector = merged_vector.iloc[0, :].to_list()\n\n            Result[\"Document\"].append(line.strip('\\n'))\n            Result[\"Word_Vector\"].append(str(word_vector))\n            Result[\"Probability\"].append(str(word_prob))\n\n    Result = pd.DataFrame(Result)\n    \n    return(Result.to_csv())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_AylinGheisar.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "random_seed = 11\n\n# we construct the dataset with documents and the class labels\nwith open(\"rural.txt\", 'r') as f:\n    stripped_lines = [line.strip('\\n') for line in f.readlines()]\n    rural = pd.DataFrame({\"Document\": stripped_lines, \"Class\": \"rural\"})\n    \nwith open(\"science.txt\", 'r') as f:\n    stripped_lines = [line.strip('\\n') for line in f.readlines()]\n    science = pd.DataFrame({\"Document\": stripped_lines, \"Class\": \"science\"})\ndocs = pd.concat([rural, science]).reset_index(drop=True)\ndocs\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nvectorizer = TfidfVectorizer()\n# We use the dense reprentation of the vectorized documents as out ttraining data\nX = vectorizer.fit_transform(docs[\"Document\"]).toarray()\ny = docs[\"Class\"].apply(lambda x: 1 if x==\"science\" else 0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\n# We fit a naive bayes classifier with default hyper parameters and evaluate its performance\nnb_clf = GaussianNB()\nnb_clf.fit(X_train, y_train)\nnb_pred = nb_clf.predict(X_test)\nprint(\"Metrics for Naive Bayes Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, nb_pred))\nprint(\"Recall:\", recall_score(y_test, nb_pred))\nprint(\"Precision:\", precision_score(y_test, nb_pred))\nprint(\"F1_score:\", f1_score(y_test, nb_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_pred))\n# We fit a support vector machine classifier with default hyper parameters and evaluate its performance\nsvm_clf = LinearSVC()\nsvm_clf.fit(X_train, y_train)\nsvm_pred = svm_clf.predict(X_test)\nprint(\"Metrics for Support Vector Machine Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, svm_pred))\nprint(\"Recall:\", recall_score(y_test, svm_pred))\nprint(\"Precision:\", precision_score(y_test, svm_pred))\nprint(\"F1_score:\", f1_score(y_test, svm_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, svm_pred))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Aksa_Aksa.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\ndata = pd.read_csv(\"polarity.txt\", sep=\"\\t\", names=['Text', 'Label'], header=None)\ndata"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Aksa_Aksa.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n\nlabels_to_numbers = {}\nlabels = data['Label'].unique() # get unique label values\n\n# convert labels to numerical values, i.e., 0 for pos, 1 for neg\nfor index, label in enumerate(labels):\n  labels_to_numbers[label] = index\n\ndef get_number_from_label(value):\n  return labels_to_numbers[value]\n\ndata['numerical_label'] = data['Label'].apply(get_number_from_label)\ndata = data.drop(columns=['Label']) # drop the original column\ndata = data.rename(columns={\"numerical_label\": \"Label\"}) # rename numerical label for readability\ndata"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Aksa_Aksa.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\n\n# get unique words in the corpus\ndef getUniqueWords(allWords):\n    uniqueWords = Counter()\n\n    for word in allWords:\n        uniqueWords[word] += 1\n    return uniqueWords.keys()\n\ndef create_count_and_probability(file_name):\n    # here comes your code\n    df = pd.DataFrame(columns=['Text', 'Count_Vector', 'Probability'])\n    words_list = [] # this will contain the all the words present in the file\n    docs = [] # this will be a 2-d array containing tokens in each document.\n    originals_docs = [] # this will be an array of string contain original docs.\n\n\n    # read the corpus.txt file and get the data.\n    with open(file_name, 'r') as f:\n      for index, line in enumerate(f):\n        original_line = line.strip()\n        line = original_line.lower()\n\n        doc_words = re.findall(r\"[\\w']+\", line) # get the words in a document\n\n        docs.append(doc_words)\n        words_list.extend(doc_words)\n        originals_docs.append(original_line)\n\n    # get the unique words/vocabulary from all words\n    vocab = getUniqueWords(words_list)\n\n    # count words and their probabilities\n    for index, doc in enumerate(docs):\n      words_count = Counter(doc)\n\n      count_arr = []\n\n      # calculate of number of words w.r.t to the complete vocab of the corpus\n      for word in vocab:\n        count_word = words_count.get(word, 0)\n        count_arr.append(count_word)\n\n      # calculate the probability w.r.t to document\n      total_words = len(doc)\n      probability = [round(words_count[word]/total_words, 3) for word in doc ]\n\n      df.loc[index] = [originals_docs[index], count_arr, probability]\n\n    display(df)\n\n    fn = ''.join(file_name.split('.')[0:-1]) # get the original filename without extension\n\n    # generate csv file from the dataframe\n    # the generated filename will be original filename with count_and_probability.csv.\n    df.to_csv(f'{fn}_count_and_probability.csv', index=False)\ncreate_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Aksa_Aksa.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n#here comes your code\n#read rural and science text files and combine them into a df\ndf_1 = pd.read_csv(\"rural.txt\", sep=\"\\t\", names=['Document'], header=None)\ndf_1['Class'] = 'rural'\ndf_2 = pd.read_csv(\"science.txt\", sep=\"\\t\", names=['Document'], header=None)\ndf_2['Class'] = 'science'\ndf = pd.concat([df_1, df_2])\n# separate input and class labels\nX = df['Document']\n\n# convert class labels to numerical values\ncat_class = pd.Categorical(df['Class'])\ny = cat_class.codes\n#split training and testing data with 70% and 30%, respectivily\ntext_train, text_test, label_train, label_test = train_test_split(X, y,\n                                                                  test_size=0.30,\n                                                                  random_state=1234, shuffle=True)\n\ntext_train.shape, text_test.shape\n# Apply TF-IDF\ntf_idf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\nX_idf = tf_idf_vectorizer.fit_transform(text_train)\n\n# create df containing X_idf corresponding each token\ntfidf_tokens = tf_idf_vectorizer.get_feature_names_out()\ndf_tfidfvect = pd.DataFrame(data = X_idf.toarray(),columns = tfidf_tokens)\ndf_tfidfvect"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Aksa_Aksa.ipynb",
        "question": "#### SVM Classifier",
        "answer": "# SVM classifier\nsvm_classifier = svm.LinearSVC()\n# SVM model training\nsvm_classifier.fit(X_idf, label_train);\n# SVM predict output\nX_test_idf = tf_idf_vectorizer.transform(text_test)\ntest_svm = svm_classifier.predict(X_test_idf)\n# SVM different performance metrics\nprint('Test performance SVM')\nprint('accuracy_score:', accuracy_score(label_test, test_svm))\nprint('f1_score:', f1_score(label_test, test_svm))\nprint('precision_score:', precision_score(label_test, test_svm))\nprint('recall_score:', recall_score(label_test, test_svm))\nprint('confusion matrix')\ncm_svm = confusion_matrix(label_test, test_svm)\ndisp_cm_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=cat_class.categories)\ndisp_cm_svm.plot()\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Aksa_Aksa.ipynb",
        "question": "#### Naive Bayes Classifier",
        "answer": "# Naive bayes Classifier\nnb_classifier = GaussianNB()\n# NB model training\nnb_classifier.fit(X_idf.toarray(), label_train)\n# NB Predict Output\nX_test_idf = tf_idf_vectorizer.transform(text_test)\ntest_nb = nb_classifier.predict(X_test_idf.toarray())\n# NB different performance metrics\nprint('Test performance NB')\nprint('accuracy_score:', accuracy_score(label_test, test_nb))\nprint('f1_score:', f1_score(label_test, test_nb))\nprint('precision_score:', precision_score(label_test, test_nb))\nprint('recall_score:', recall_score(label_test, test_nb))\nprint('confusion matrix')\ncm_nb = confusion_matrix(label_test, test_nb)\ndisp_cm_nb = ConfusionMatrixDisplay(confusion_matrix=cm_nb, display_labels=cat_class.categories)\ndisp_cm_nb.plot()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Valdrin_Smakaj.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Path\nfile_path = './polarity.txt'\n\n# Read the file into a DataFrame\ndf = pd.read_csv(file_path, sep='\\\\t', header=None, names=[\"Text\", \"Label\"])\n\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Valdrin_Smakaj.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "import pandas as pd\n\n# function to convert labels to numerical values\ndef label_to_numeric(label):\n    return 1 if label == 'pos' else 0\n\n# Path\nfile_path = './polarity.txt'\n\n# Read the file into a DataFrame\ndf = pd.read_csv(file_path, sep='\\\\t', header=None, names=[\"Text\", \"Label\"])\n\ndf['Label_Numeric'] = df['Label'].apply(label_to_numeric)\n\n# Drop the original 'Label' column\ndf.drop('Label', axis=1, inplace=True)\n\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Valdrin_Smakaj.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\nimport csv\n\ndef create_count_and_probability(file_name):\n    \n    # read file content\n    with open(file_name, 'r') as file:\n        corpus = file.readlines()\n\n    # Tokenization / set of all unique words\n    vocabulary = set()\n    tokenized_corpus = []\n    for document in corpus:\n        tokens = re.findall(r'\\b\\w+\\b', document.lower())\n        vocabulary.update(tokens)\n        tokenized_corpus.append(tokens)\n\n    # Sort the vocabulary \n    vocabulary = sorted(vocabulary)\n\n    # Each word occurrences track in each document \n    matrix = []\n    probability_vectors = []\n    for tokens in tokenized_corpus:\n        count = Counter(tokens)\n        row = [count.get(word, 0) for word in vocabulary]\n        matrix.append(row)\n\n    # Process each document in the corpus\n    for document in corpus:\n        # Extract words with regular expression\n        words_list = re.findall(r\"[\\w']+\", document.lower())\n\n        # Count the words\n        word_count = Counter(words_list)\n        \n        # Calculate total number of words in the document\n        total_words = sum(word_count.values())\n\n        # Calculate the probability of each word\n        probability_vector = [f\"{word_count[word]}/{total_words}\" for word in words_list]\n        probability_vectors.append(probability_vector)\n    \n    # write the content in the result csv file\n    csv_file = 'results.csv'\n    with open(csv_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Text', 'Count_Vector', 'Probability'])\n\n        for i, document in enumerate(corpus):\n            writer.writerow([document.strip(), matrix[i], probability_vectors[i]])\n\n    return csv_file\n    \n    \n\n# define file path and call function\nfile_path = './corpus.txt'\ncreate_count_and_probability(file_path)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Valdrin_Smakaj.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\n\n# evaluation of classifiers\ndef evaluate_classifier(clf, X_test, y_test):\n    predictions = clf.predict(X_test.toarray() if clf == gnb else X_test)\n    return {\n        'accuracy': accuracy_score(y_test, predictions),\n        'recall': recall_score(y_test, predictions, average='macro'),\n        'precision': precision_score(y_test, predictions, average='macro'),\n        'f1_score': f1_score(y_test, predictions, average='macro'),\n        'confusion_matrix': confusion_matrix(y_test, predictions)\n    }\n\n\n# Read the files and make dataframe\nwith open(\"rural.txt\", \"r\") as file:\n    rural = file.readlines()\n\nwith open(\"science.txt\", \"r\") as file:\n    science = file.readlines()\n\ndata = {\n    \"Document\": rural + science,\n    \"Class\": ['rural'] * len(rural) + ['science'] * len(science)\n}\ndf = pd.DataFrame(data)\ndf['Document'] = df['Document'].str.strip()  \n\n# Split the data into train (70%) and test (30%) \nX_train, X_test, y_train, y_test = train_test_split(df['Document'], df['Class'], test_size=0.3, random_state=42)\n\n# Initialize the TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Train GaussianNB \ngnb = GaussianNB()\ngnb.fit(X_train_tfidf.toarray(), y_train)  # dense matrix format\n\n# Train LinearSVC \nsvc = LinearSVC(dual=False)\nsvc.fit(X_train_tfidf, y_train)\n\n# Evaluation\ngnb_evaluation_results = evaluate_classifier(gnb, X_test_tfidf, y_test)\nsvc_evaluation_results = evaluate_classifier(svc, X_test_tfidf, y_test)\n\nprint(\"Gaussian Naive Bayes Results:\", gnb_evaluation_results)\nprint(\"LinearSVC Results:\", svc_evaluation_results)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_BrunoScheider.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n#here comes your code\ndf = pd.read_csv('polarity.txt', on_bad_lines='skip', delimiter='\\t', names=[\"Text\", \"Label\"])\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_BrunoScheider.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n#df['Label'] = df['Label'].apply(lambda x: int(x=='pos'))\ndf['numLabel'] = df['Label'].apply(lambda x: int(x=='pos'))\ndf = df.drop('Label', axis=1)\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_BrunoScheider.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\n\ndef create_count_and_probability(file_name):\n    # here comes your code\n    df, vocabulary = file2df(file_name)\n    append_counts(df, vocabulary)\n    append_probablitiy(df)\n    csv_file = 'out.csv'\n    df.to_csv(csv_file)\n    return csv_file, df\n\ndef file2df(filename, word_vec = True):\n    \n    documents = []\n    all_words = []\n    with open(filename) as f:\n        for line in f:\n            doc = line.strip()\n            documents.append(doc)\n            if word_vec:\n                [all_words.append(w.lower()) for w in re.findall(r\"[\\w']+\", doc)]\n    if word_vec:\n        unique_words = set(all_words)\n        all_words = list(unique_words)\n        all_words.sort()\n    df = pd.DataFrame(data={'Text':documents})\n    return df, all_words\n\ndef append_counts(df, voc):\n    df['Count_Vector'] = df['Text'].apply(get_count_vector, args=(voc,))\n    \ndef append_probablitiy(df):\n    df['Probability'] = df['Text'].apply(get_probablity, args=(get_count_vector,))\n    \ndef get_count_vector(text, voc):\n    words_in_text = [w.lower() for w in re.findall(r\"[\\w']+\", text)]\n    count_set = dict.fromkeys(voc,0)\n    for w in words_in_text:\n        count_set[w] +=1\n    count_vec = count_set.values()\n    return count_vec\n\ndef get_probablity(text, get_count_vector):\n    words = [w.lower() for w in re.findall(r\"[\\w']+\", text)]\n    oc = get_count_vector(text, words)   \n    probs = [x/len(words) for x in oc]\n    \n    return probs\nfile_name = 'corpus.txt'\n\n_, df = create_count_and_probability(file_name)\n\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_BrunoScheider.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "def get_df_from_files(rural_file='rural.txt', science_file='science.txt'):\n    print(rural_file)\n    df_r, _ = file2df(rural_file)\n    df_r['Class'] = 'rural'\n    \n    df_s, _ = file2df(science_file)\n    df_s['Class'] = 'science'\n    \n    df = pd.concat([df_r,df_s])\n    df = df.sample(len(df))\n    #df.reset_index(drop=True)\n    return df\ndf = get_df_from_files()\ndf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    PrecisionRecallDisplay,\n    f1_score\n)\nimport numpy as np\n\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['Text'].to_numpy())\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, df['Class'].to_numpy(), test_size=0.3, random_state=125\n)\n\nX_train.toarray()\nX_test.toarray()\n#y_train.toarray()\n#y_test.toarray()\nmodel_NB = GaussianNB()\n\nmodel_NB.fit(X_train.toarray(), y_train)\n\ny_pred = model_NB.predict(X_test.toarray())\n\n###evaluation of naive bayes\naccuray = accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\n\n#display = PrecisionRecallDisplay.from_estimator(\n#    model_NB, X_test, y_test, name=\"Naive Bayes\", plot_chance_level=True\n#)\n\nprint(\"Accuracy:\", accuray)\nprint(\"F1 Score:\", f1)\nfrom sklearn.svm import LinearSVC\n\n\nsvm_classifier = LinearSVC()\nsvm_classifier.fit(X_train, y_train)\n\ny_pred = svm_classifier.predict(X_test.toarray())\n\n\n#evaluation of svm\naccuray = accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy:\", accuray)\nprint(\"F1 Score:\", f1)\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Shashank_Dammalapati.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\nimport pandas as pd\n\ndf = pd.read_csv(\"polarity.txt\", sep='\\t', header=None)\ndf.columns = [\"text\", \"polarity\"]\ndf.sample(5)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Shashank_Dammalapati.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n# Custom funtion that is applied to each element\ndef numberize(polarity):\n    if polarity == \"pos\":\n        return 1\n    else: \n        return 0\n\ndf[\"labels\"] = df[\"polarity\"].apply(numberize)\ndf.sample(5)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Shashank_Dammalapati.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def vocab(file):\n    '''\n    Input: txt file url\n    Output: list of unique words in the file\n    '''\n\n    if type(file) is pd.core.frame.DataFrame:\n        df = file\n        # Vocab from df unique words\n        vocab = set()\n        for text in df[\"Document\"]:\n            # Taking care of symbols - only words are considered\n            vocab.update(text.split(\" \"))\n\n        # Return as a list\n        vocab = set(vocab)\n        return(list(vocab))\n    else: \n        with open(file, 'r') as f:\n            vocab = set(f.read().split())\n            # Return as a list\n        return(list(vocab))\n\n\n\n\ndef count_vector_n_probabilities(text, vocab):\n    '''\n    Input: text, vocab\n    Output: list of counts of words in vocab\n    '''\n    count_vector = [0]*len(vocab)\n    # Split the text into words\n    words = text.split()\n    # For each word - add 1 to the corresponding index\n    for word in words:\n        if word in vocab:\n            count_vector[vocab.index(word)] += 1\n            \n    # Calculate the probability of each word\n    total_words = len(words)\n    probabilities = [count/total_words for count in count_vector]\n    return(count_vector, probabilities)\n\n\n\n\ndef create_count_and_probability(file_name):\n    '''\n    Input: txt file url\n    Output: CSV file with \"Text\", \"Count_Vector\", \"Probability\" columns\n    '''\n    df = pd.read_csv(file_name, sep='\\t', header=None)\n    # Add a name to the column\n    df.columns = [\"text\"]\n    # Create a column with the count vector\n    df[\"count_vector\"] = df[\"text\"].apply(lambda x: count_vector_n_probabilities(x, vocab(\"corpus.txt\"))[0])\n    # Create a column with the probability\n    df[\"probability\"] = df[\"text\"].apply(lambda x: count_vector_n_probabilities(x, vocab(\"corpus.txt\"))[1])\n    # Save as a CSV file\n    df.to_csv(\"count_vector_and_probability.csv\", index=False)\n    \n    return(df)\n\ndf = create_count_and_probability(\"corpus.txt\")\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Shashank_Dammalapati.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "rural_df = pd.read_csv(\"rural.txt\", header=None, sep=\"\\t\")\nrural_df.columns = ['Document']\nrural_df[\"Class\"] = \"rural\"\n\nscience_df = pd.read_csv(\"science.txt\", header=None, sep=\"\\t\")\nscience_df.columns = ['Document']\nscience_df[\"Class\"] = \"science\"\n\ndf = pd.concat([science_df, rural_df])\ndf\n# Splitting into testing and training\n# Using scikit learn\nfrom sklearn.model_selection import train_test_split\n\n\nX = df[\"Document\"]\ny = df[\"Class\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0) # Use same random state for reproducibility\n# TF-IDF : Term Frequency - Inverse Document Frequency\n\n# TF(term, document) = (#term apperences/ # total terms in document) \n# ITF(term, Corpus) = log(#Number of Docs in Corpus / # documents containing term + 1)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndocuments = df[\"Document\"]\n\n# Creating a TF-IDF vectorizer\nvectorizer = TfidfVectorizer(max_features=4000, ngram_range=(1,2))\n\n# Making tf-idf matrix\ntfidf_train = vectorizer.fit_transform(X_train)\ntfidf_test = vectorizer.transform(X_test)\n\n# Print the shape of the matrices\nprint(tfidf_train.shape)\nprint(tfidf_test.shape)\n# Classification using GaussianNB\n\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(tfidf_train.toarray(), y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(tfidf_test.toarray())\n\n# Accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy_score = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy_score)\n# Recall and Precision\nfrom sklearn.metrics import recall_score, precision_score\nrecall_score = recall_score(y_test, y_pred, average='weighted')\nprecision_score = precision_score(y_test, y_pred, average='weighted')\nprint(\"Recall: \", recall_score)\nprint(\"Precision: \", precision_score)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \\n\", cm)\n# Classification using svm.LinearSVC()\n\nfrom sklearn.svm import LinearSVC\n\nclassifier = LinearSVC()\nclassifier.fit(tfidf_train.toarray(), y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(tfidf_test.toarray())\n\n# Accuracy. Recall and Precision\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\naccuracy_score = accuracy_score(y_test, y_pred)\nrecall_score = recall_score(y_test, y_pred, average='weighted')\nprecision_score = precision_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: \", accuracy_score)\nprint(\"Recall: \", recall_score)\nprint(\"Precision: \", precision_score)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \\n\", cm)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Timon_Oerder.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('polarity.txt', sep='\\t', header=None)\ndf.columns = ['text', 'label']\n#here comes your code\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Timon_Oerder.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\ndf[['label']] = df[['label']].apply(lambda x: pd.factorize(x)[0]) # use factorize to convert labels to numbers\ndf['label'] = df['label'].astype(float)\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Timon_Oerder.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nimport collections\n\n\ndef create_count_and_probability(file_name):\n    ret=[] \n    counter = collections.Counter() # as requested in the assignment\n    with open(file_name, 'r') as file:\n        for line in file:\n            words_list = re.findall(r\"[\\w']+\", line.lower()) # lower case and split by words\n            counter.update(words_list) # update the counter\n\n            ret.append(words_list)\n    \n    ret = [(\n        \" \".join(ex), \n        [counter[word] for word in ex], # count of each word\n        [counter[word]/sum([counter[word] for word in ex]) for word in ex] # frequency of each word\n        ) for ex in ret] # casting\n\n    ret = [ex[0] + \";\" + str(ex[1])+ \";\" + str(ex[2]) for ex in ret] # casting\n    csvstr = \"\\n\".join(ret) # convert to string\n    return(csvstr)\n\ncsvstr = create_count_and_probability(\"corpus.txt\")\nprint(csvstr)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Timon_Oerder.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntry: # test if tqdm is aviable\n    from tqdm import tqdm\n    print(\"Module tqdm imported successfully.\")\nexcept ImportError as e:\n    print(f\"Error importing module: {e}\")\n    def tqdm(iterable):\n        return iterable\n\n\ndocuments = []\ndocstfidf = []\n\ndef loadfile(filename, classname):\n    global documents\n    global docstfidf\n    with open(filename, 'r') as file:\n        for line in file:\n            documents.append((line, classname))\n            docstfidf.append(line)\n    # tfidf matrix should be based on the words of both classes\n\nloadfile(\"rural.txt\", \"rural\")\nloadfile(\"science.txt\", \"science\")\n# variables for the vectorizer\nngram_range = (1, 4)\nmin_df = 2\nanalyzer = 'word' #char\n\ntfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range, min_df=min_df, analyzer=analyzer)\ntfidf_matrix = tfidf_vectorizer.fit_transform(docstfidf) #fit the vectorizer to synopses\nfeature_names = tfidf_vectorizer.get_feature_names_out() # get the feature names\ncols =[*(feature_names.tolist()), \"Class\"] # add the class column\n\nnewdf = pd.DataFrame(columns={col: [] for col in cols}) #create a new data frame with the columns\nclassnamemaping = { \"rural\": -1.0, \"science\": 1.0} # map the class names to numbers could also use factorize\n\ndfs = [newdf]\n# for each sentence in the documents we add a data frame\nfor (line, classname) in tqdm(documents):\n    line_tfidf = tfidf_vectorizer.transform([line]).toarray().tolist()[0]\n    line_tfidf.append(classnamemaping[classname]) # add the class to the end of the list\n    tfidf_df = pd.DataFrame(data=[line_tfidf], columns=cols) # create a new data frame with the columns and the new data of that line\n    dfs.append(tfidf_df) # add the new data frame to the list of data frames\n\nnewdf = pd.concat(dfs, axis=0).reset_index(drop=True) # concat the data frames to one data frame\nnewdf['Class'] = newdf['Class'].astype(float)\n\nnewdf = newdf.sample(frac=1).reset_index(drop=True) #randomize the order of the rows\n\nprint(newdf.head()) # print to have a look\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\ndef testclassifier(dataframe, classifier):\n    # function to test the given classifier\n    X = dataframe[cols[:-1]] # get the features\n    y = dataframe['Class'] # get the class\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 0.3 test size, 0.7 train size\n\n\n    classifier.fit(X_train, y_train) # fit the classifier\n    y_pred = classifier.predict(X_test) # predict the test data\n\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    conf_matrix = confusion_matrix(y_test, y_pred)\n\n    print(\"Classifier: \", classifier)\n    print(\"Accuracy: \", accuracy)\n    print(\"Precision: \", precision)\n    print(\"Recall: \", recall)\n    print(\"F1: \", f1)\n    print(\"Confusion Matrix: \", conf_matrix)\n    return accuracy, precision, recall, f1, conf_matrix\n\n# create the classifiers\nnb_classifier = GaussianNB() \nsvc_classifier = LinearSVC()\n\n# test the classifiers\ntestclassifier(newdf, nb_classifier)\ntestclassifier(newdf, svc_classifier)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_RaoRohilPrakash.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\n\nimport pandas as pd\n\n# Assuming polarity.txt contains lines of text and labels separated by tabs (\"\\t\")\nfile_path = 'polarity.txt'\n\n# Read the file into a DataFrame with appropriate column names\ndf = pd.read_csv(file_path, sep='\\t', header=None, names=['Text', 'Label'])\n\nprint(df['Label'].value_counts())\n\n# Display the DataFrame\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_RaoRohilPrakash.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n\nlabel_mapping = {'pos': 1, 'neg': 0}\n\n# Create a new column 'NumericLabel' using the apply() function\ndf['NumericLabel'] = df['Label'].apply(lambda x: label_mapping[x])\n\nprint(df['NumericLabel'].value_counts())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_RaoRohilPrakash.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nimport csv\nfrom collections import Counter\n\ndef create_count_and_probability(file_name):\n    # Read the input file\n    with open(file_name, 'r') as file:\n        lines = file.readlines()\n\n    data = []\n\n    # Process each line\n    for line in lines:\n        # Use regular expression to extract words\n        words_list = re.findall(r\"[\\w']+\", line.lower())  # Convert to lowercase for consistency\n\n        # Create count vector using Counter\n        count_vector = [words_list.count(word) for word in set(words_list)]\n\n        # Calculate probabilities\n        total_words = len(words_list)\n        probabilities = [count / total_words for count in count_vector]\n\n        # Append data to the list\n        data.append({\n            'Text': line.strip(),\n            'Count_Vector': count_vector,\n            'Probability': probabilities\n        })\n\n    # Define the output CSV file name\n    csv_file = 'output.csv'\n\n    # Write the data to a CSV file\n    with open(csv_file, 'w', newline='') as csvfile:\n        fieldnames = ['Text', 'Count_Vector', 'Probability']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        # Write header\n        writer.writeheader()\n\n        # Write data\n        for row in data:\n            writer.writerow(row)\n\n    return csv_file\n\n# Example usage:\nfile_name = 'corpus.txt'\noutput_csv = create_count_and_probability(file_name)\nprint(f\"Output CSV file generated: {output_csv}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_RaoRohilPrakash.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n\n# a) Read the data into a DataFrame\ndef read_data(file_path, label):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n    data = {'Document': lines, 'Class': [label] * len(lines)}\n    return pd.DataFrame(data)\n\nrural_df = read_data('rural.txt', 'rural')\nscience_df = read_data('science.txt', 'science')\n\n# Concatenate the dataframes\ndf = pd.concat([rural_df, science_df], ignore_index=True)\n\n# Display the dataframe\nprint(df.head())\n\n# b) Split the data into train (70%) and test (30%) sets and use tf-idf vectorizer\nX_train, X_test, y_train, y_test = train_test_split(df['Document'], df['Class'], test_size=0.3, random_state=42)\n\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Train the classifiers\ngnb = GaussianNB()\ngnb.fit(X_train_tfidf.toarray(), y_train)\n\nsvc = LinearSVC()\nsvc.fit(X_train_tfidf, y_train)\n\n# c) Evaluate the classifiers\ndef evaluate_classifier(classifier, X_test, y_test):\n    y_pred = classifier.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    precision = metrics.precision_score(y_test, y_pred, average='weighted')\n    recall = metrics.recall_score(y_test, y_pred, average='weighted')\n    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n\n    return accuracy, precision, recall, f1_score, confusion_matrix\n\n# Evaluate Gaussian Naive Bayes\ngnb_accuracy, gnb_precision, gnb_recall, gnb_f1, gnb_confusion_matrix = evaluate_classifier(gnb, X_test_tfidf.toarray(), y_test)\n\n# Evaluate Linear SVC\nsvc_accuracy, svc_precision, svc_recall, svc_f1, svc_confusion_matrix = evaluate_classifier(svc, X_test_tfidf, y_test)\n\n# Display the results\nprint(\"\\nResults for Gaussian Naive Bayes:\")\nprint(f\"Accuracy: {gnb_accuracy}\")\nprint(f\"Precision: {gnb_precision}\")\nprint(f\"Recall: {gnb_recall}\")\nprint(f\"F1 Score: {gnb_f1}\")\nprint(f\"Confusion Matrix:\\n{gnb_confusion_matrix}\")\n\nprint(\"\\nResults for Linear SVC:\")\nprint(f\"Accuracy: {svc_accuracy}\")\nprint(f\"Precision: {svc_precision}\")\nprint(f\"Recall: {svc_recall}\")\nprint(f\"F1 Score: {svc_f1}\")\nprint(f\"Confusion Matrix:\\n{svc_confusion_matrix}\")\n"
    },
    {
        "file_name": "Assignment2_Hossam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\nimport pandas as pd\n\nfile_path = 'polarity.txt'\n\n# Define column names\ncolumn_names = ['Text', 'Label']\n\n# Read the data into a DataFrame\ndf = pd.read_csv(file_path, sep='\\t', header=None, names=column_names)\n\n# Display the DataFrame\nprint(df)"
    },
    {
        "file_name": "Assignment2_Hossam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n\nimport pandas as pd\n\n# Create a mapping dictionary for label conversion\nlabel_mapping = {'neg': 0, 'pos': 1}\n\n# Apply the mapping to create a new numerical label column\ndf['NumericalLabel'] = df['Label'].apply(lambda x: label_mapping.get(x, x))\n\n# Drop the original 'Label' column\ndf = df.drop(columns=['Label'])\n\n# Display the updated DataFrame\nprint(df)"
    },
    {
        "file_name": "Assignment2_Hossam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nimport pandas as pd\n\ndef create_count_and_probability(input_file):\n    # Read the text file\n    with open(input_file, 'r') as file:\n        lines = file.readlines()\n\n    # Create a list of words of all lines\n    words_list = set()\n    for line in lines:\n        # Tokenize the line using regular expression\n        line_words_list = re.findall(r\"[\\w']+\", line.lower())\n        # Update the set of words\n        words_list.update(line_words_list)\n    \n    \n    # Process each line and create a list of dictionaries\n    data = []\n    for line in lines:\n        # Tokenize the line using regular expression\n        line_words_list = re.findall(r\"[\\w']+\", line.lower())\n\n        # Count the occurrences of each word in word list\n        word_counts = {}\n        for word in words_list:\n            word_counts[word] = line_words_list.count(word)\n            \n        # Calculate total count for normalization\n        total_count = sum(word_counts.values())\n\n        # Calculate probability for each word\n        word_probabilities = {word: count / total_count for word, count in word_counts.items()}\n\n        # Append data to the list\n        data.append({\n            'Text': line.strip(),\n            'Count_Vector': str([word_counts[word] for word in words_list]),\n            'Probability': str([word_probabilities[word] for word in words_list])\n        })\n\n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv('output2.csv', index=False)\n\n# Example usage:\ncreate_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Assignment2_Hossam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n\n# Read the data from the files and create a DataFrame\ndef read_file(file_path, class_label):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n    data = {'Document': lines, 'Class': class_label}\n    df = pd.DataFrame(data)\n    return df\n\nrural_df = read_file('rural.txt', 'rural')\nscience_df = read_file('science.txt', 'science')\n\n# Concatenate the dataframes\ncombined_df = pd.concat([rural_df, science_df], ignore_index=True)\n\n# Split the data into train (70%) and test (30%) sets\nX_train, X_test, y_train, y_test = train_test_split(\n    combined_df['Document'], \n    combined_df['Class'], \n    test_size=0.3, \n    random_state=42\n)\n\n# TF-IDF Vectorizer\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Naive Bayes Classifier\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train_tfidf.toarray(), y_train)\nnb_predictions = nb_classifier.predict(X_test_tfidf.toarray())\n\n# SVM Classifier\nsvm_classifier = LinearSVC()\nsvm_classifier.fit(X_train_tfidf, y_train)\nsvm_predictions = svm_classifier.predict(X_test_tfidf)\n\n# Evaluation Metrics\ndef evaluate_classifier(predictions, classifier_name):\n    accuracy = metrics.accuracy_score(y_test, predictions)\n    recall = metrics.recall_score(y_test, predictions, pos_label='science')\n    precision = metrics.precision_score(y_test, predictions, pos_label='science')\n    f1 = metrics.f1_score(y_test, predictions, pos_label='science')\n    confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n\n    print(f\"Metrics for {classifier_name}:\")\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"F1 Score: {f1:.2f}\")\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix)\n    print(\"\\n\")\n\n# Evaluate Naive Bayes Classifier\nevaluate_classifier(nb_predictions, \"Naive Bayes\")\n\n# Evaluate SVM Classifier\nevaluate_classifier(svm_predictions, \"SVM\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muskaan_Chopra.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#importing libraries\nimport pandas as pd\nimport numpy as np\n\n#create dataframe\ndf=pd.read_csv(\"/content/drive/MyDrive/Bonn/NLP Assgn/2/polarity.txt\", sep=\"\\t\", header=None,\n            names=[\"Text\", \"Label\"])\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muskaan_Chopra.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n\n#converting labels to numerical values\ndf['Label_num'] = df['Label'].apply(lambda x: 1 if x == 'pos' else 0)\ndf = df.drop('Label', axis=1)\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muskaan_Chopra.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "# Import necessary libraries\nimport csv\nimport re\nfrom collections import Counter\nfrom fractions import Fraction\n\n# Define a function to create count and probability\ndef create_count_and_probability(input_file):\n    # Define the output file name\n    output_file = 'output.csv'\n    \n    # Open the input file in read mode and output file in write mode\n    with open(input_file, 'r') as in_file, open(output_file, 'w', newline='') as out_file:\n        # Create a csv writer object\n        writer = csv.writer(out_file)\n        \n        # Write the header row to the csv file\n        writer.writerow(['Text', 'Count_Vector', 'Probability'])\n\n        # Loop through each line in the input file\n        for line in in_file:\n            # Find all words in the line and create a list\n            words_list = re.findall(r\"[\\w']+\", line)\n            \n            # Count the occurrence of each word in the list\n            word_counts = Counter(words_list)\n            \n            # Get the total number of words in the list\n            total_words = len(words_list)\n            \n            # Create a count vector for the words in the list\n            count_vector = [word_counts[word] for word in words_list]\n            \n            # Initialize an empty list to store the probabilities\n            probability = []\n            \n            # Loop through each count in the count vector\n            for count in count_vector:\n              # Calculate the probability of the count\n              x=Fraction(count,total_words)\n              \n              # Append the probability to the list\n              probability.append(f\"{x.numerator}/{x.denominator}\")\n            \n            # Write the line, count vector, and probability to the csv file\n            writer.writerow([line.strip(), count_vector, probability])\n\n    # Return the output file name\n    return output_file\ncsv=create_count_and_probability('corpus.txt')\npd.read_csv('output.csv')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muskaan_Chopra.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your codeimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import naive_bayes, svm\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report\n\n# Load the data\nrural = pd.read_csv('/content/drive/MyDrive/Bonn/NLP Assgn/2/rural.txt', sep='\\t', header=None, names=['Document'])\nrural['Class'] = 'rural'\nscience = pd.read_csv('/content/drive/MyDrive/Bonn/NLP Assgn/2/science.txt', sep='\\t', header=None, names=['Document'])\nscience['Class'] = 'science'\n\n# Combine the data\ndf = pd.concat([rural, science])\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['Document'], df['Class'], test_size=0.3)\n\n# Vectorize the data\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Train the classifiers\nclf_nb = naive_bayes.GaussianNB()\nclf_nb.fit(X_train_tfidf.toarray(), y_train)\n\nclf_svm = svm.LinearSVC()\nclf_svm.fit(X_train_tfidf, y_train)\n\n# Evaluate the classifiers\nfor clf, name in [(clf_nb, 'Naive Bayes'), (clf_svm, 'Linear SVM')]:\n    y_pred = clf.predict(X_test_tfidf.toarray() if name == 'Naive Bayes' else X_test_tfidf)\n    print(f'{name} Results:')\n    print(classification_report(y_test, y_pred))\n    # print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n    # print(f'Recall: {recall_score(y_test, y_pred, pos_label=\"science\")}')\n    # print(f'Precision: {precision_score(y_test, y_pred, pos_label=\"science\")}')\n    # print(f'F1 Score: {f1_score(y_test, y_pred, pos_label=\"science\")}')\n    print(f'Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\\n')\n"
    },
    {
        "file_name": "Assignment_2_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "df = pd.read_csv('polarity.txt', sep='\\t', header=None, names=['Text', 'Label'])\n\ndf.head(5)"
    },
    {
        "file_name": "Assignment_2_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# create a new col (1 for pos, 0 for neg)\ndf['Label Values'] = df['Label'].apply(lambda x: 1 if x == 'pos' else 0)\ndf = df.drop(columns=['Label'])\n\ndf.head(5)"
    },
    {
        "file_name": "Assignment_2_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def create_count_and_probability(input_file):\n    \n    with open(input_file, 'r') as file:\n        lines = [line.strip() for line in file.readlines()]\n\n    # Word tokenization\n    tokenized_lines = [re.findall(r'\\b\\w+\\b', line.lower()) for line in lines]\n\n    # A set of unique words\n    unique_words = sorted(set(word for line in tokenized_lines for word in line))\n\n    count_vectors = []\n    probabilities = []\n\n    # Calculate count vectors and probabilities for each line\n    for line in tokenized_lines:\n        line_word_counts = [line.count(word) for word in unique_words]\n        count_vectors.append(line_word_counts)\n        # Based on the provided example, the probabilites should be like 1/6 and not 0.16, so we have to save them as strings not numbers.\n        probabilities.append([str(Fraction(count, len(line))) if count != 0 else '0' for count in line_word_counts])\n\n    df = pd.DataFrame({\n        'Text': lines,\n        'Count_Vector': count_vectors,\n        'Probability': probabilities\n    })\n\n    csv_file = df.to_csv('output.csv', index=False)\n    \n    return(csv_file)\ncreate_count_and_probability('corpus.txt')\npd.read_csv('output.csv')"
    },
    {
        "file_name": "Assignment_2_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Define a function for creating dataframe\ndef create_df(file_path, class_label):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n        \n    lines = [line.strip() for line in lines]\n\n    df = pd.DataFrame({\n        'Document': lines,\n        'Class': class_label\n    })\n\n    return df\n# Create DataFrames for rural and science documents\nrural_df = create_df('rural.txt', 'Rural')\nscience_df = create_df('science.txt', 'Science')\n\n# Concatenate the two DFs\nfinal_df = pd.concat([rural_df, science_df], ignore_index=True)\n\nfinal_df.head(5)\n# Split the data into train and test sets\ntrain_df, test_df = train_test_split(final_df, test_size=0.3, random_state=32)\n\n# TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features and other parameters\n\n# Transform the training and test data\nX_train = vectorizer.fit_transform(train_df['Document'])\nX_test = vectorizer.transform(test_df['Document'])\n\n# Convert sparse matrix to dense matrix for Gaussian Naive Bayes\nX_train_dense = X_train.toarray()\nX_test_dense = X_test.toarray()\n\n# Train Gaussian Naive Bayes and SVM Classifier\ngnb = GaussianNB()\ngnb.fit(X_train_dense, train_df['Class'])\n\nsvm = LinearSVC()\nsvm.fit(X_train, train_df['Class'])\n\n# Evaluate classifiers on the test set\ndef evaluate_classifier(classifier, X_test, y_test):\n    y_pred = classifier.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    recall = metrics.recall_score(y_test, y_pred, pos_label='Science', average='binary')\n    precision = metrics.precision_score(y_test, y_pred, pos_label='Science', average='binary')\n    f1 = metrics.f1_score(y_test, y_pred, pos_label='Science', average='binary')\n    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n    \n    return accuracy, recall, precision, f1, confusion_matrix\n# Evaluate Gaussian Naive Bayes\ngnb_accuracy, gnb_recall, gnb_precision, gnb_f1, gnb_confusion_matrix = evaluate_classifier(gnb, X_test_dense, test_df['Class'])\nprint(\"Gaussian Naive Bayes:\")\nprint(\"Accuracy:\", gnb_accuracy)\nprint(\"Recall:\", gnb_recall)\nprint(\"Precision:\", gnb_precision)\nprint(\"F1 Score:\", gnb_f1)\nprint(\"Confusion Matrix:\\n\", gnb_confusion_matrix)\n\n# Evaluate Linear SVM\nsvm_accuracy, svm_recall, svm_precision, svm_f1, svm_confusion_matrix = evaluate_classifier(svm, X_test, test_df['Class'])\nprint(\"\\nLinear SVM:\")\nprint(\"Accuracy:\", svm_accuracy)\nprint(\"Recall:\", svm_recall)\nprint(\"Precision:\", svm_precision)\nprint(\"F1 Score:\", svm_f1)\nprint(\"Confusion Matrix:\\n\", svm_confusion_matrix)\nresults_df = pd.DataFrame({\n    'Classifier': ['Gaussian Naive Bayes', 'Linear SVM'],\n    'Accuracy': [gnb_accuracy, svm_accuracy],\n    'Recall': [gnb_recall, svm_recall],\n    'Precision': [gnb_precision, svm_precision],\n    'F1 Score': [gnb_f1, svm_f1]\n})\n\nresults_df"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\ndata = pd.read_csv('polarity.txt', sep=\"\t\", header = None, names = [\"comment\", \"assessment\"])\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\ndata['label'] = data[\"assessment\"].apply(lambda x: 1 if x ==\"pos\" else 0)\nprint(data)\ndata = data.drop(columns=['assessment'])\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Elwakeel_Wedermann.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def create_count_and_probability(file_name):\n    # here comes your code\n    \n    #1.\n    df = pd.read_csv(file_name, sep=\"\\t\", header = None, names = [\"text\"])\n    text = str()\n    for x in df['text']:\n        text = text + x + \"\\n\"\n    \n    words_list = re.findall(r\"[\\w']+\", text.lower())\n    \n    count = Counter(words_list)\n    \n    count_vector = [[0]*len(count) for t in range(len(df[\"text\"]))]\n    \n    probabilities = [0]*len(df[\"text\"])\n                    \n    \n    i = 0\n    \n    #2. + 3.\n    for x in df[\"text\"]:\n        j = 0\n        current_list = re.findall(r\"[\\w']+\", x.lower())\n        probabilities[i] = [0] * len(current_list)\n        for y in count:\n            indices = [i for i, x in enumerate(current_list) if x == y]\n            if indices != []:\n                for z in indices:\n                    count_vector[i][j] += 1\n                    probabilities[i][z] = len(indices)/len(current_list)\n            j += 1\n        i +=1\n    df['count vector'] = count_vector\n    df['probabilities'] = probabilities\n    print(df)\n    \n    with open(\"task_2.csv\", 'w', newline='') as csv_file:\n        df.to_csv(csv_file, sep='\\t', encoding='utf-8')\n    \n    return(csv_file)\n\ntestfile= create_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Elwakeel_Wedermann.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\n#a)\ndef load_df(filename_1, filename_2):\n    f1 = pd.read_csv(filename_1, sep=\"\t\", header = None, names = [\"Document\", \"Class\"])\n    f1[\"Class\"], temp = os.path.splitext(filename_1)\n    f2 = pd.read_csv(filename_2, sep=\"\t\", header = None, names = [\"Document\", \"Class\"])\n    f2[\"Class\"], temp = os.path.splitext(filename_2)\n    frames = [f1, f2]\n    return pd.concat(frames, ignore_index=True)\n\n\n#b)\ndf = load_df(\"rural.txt\", \"science.txt\")\ntext_train, text_test, label_train, label_test = train_test_split(df[\"Document\"], df[\"Class\"], test_size=0.3, \n                                                                  random_state= 10, shuffle=True)\n\nidf_vectorizer = TfidfVectorizer()\ntrain_matrix = idf_vectorizer.fit_transform(text_train)\ntest_matrix = idf_vectorizer.transform(text_test)\n\nsvm_classifier = svm.LinearSVC()\nsvm_classifier.fit(train_matrix.toarray(), label_train)\npred_result_svm = svm_classifier.predict(test_matrix.toarray())\n\ngnb = GaussianNB()\ngnb.fit(train_matrix.toarray(), label_train)\npred_result_gnb = gnb.predict(test_matrix.toarray())\n\n\n#c)\ncorrect_answers = np.sum(np.equal(pred_result_svm, label_test))\naccuracy_svm = correct_answers / (len(pred_result_svm)*1.0)\nprint(\"Accuracy by svm: \", accuracy_svm)\n\nconfusion_matrix_svm = metrics.confusion_matrix(label_test, pred_result_svm)\ncm_display_svm = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_svm, display_labels = [\"rural\", \"science\"])\ncm_display_svm.plot()\nplt.title('Confusion Matrix for svm')\n\nTP_svm = np.sum(np.logical_and(pred_result_svm == \"rural\", label_test == \"rural\"))\nTN_svm = np.sum(np.logical_and(pred_result_svm == \"science\", label_test == \"science\"))\nFP_svm = np.sum(np.logical_and(pred_result_svm == \"rural\", label_test == \"science\"))\nFN_svm = np.sum(np.logical_and(pred_result_svm == \"science\", label_test == \"rural\"))\nprecision_svm = TP_svm/(TP_svm + FP_svm)\nrecall_svm = TP_svm/(TP_svm + FN_svm)\nf1_svm = 2 * precision_svm * recall_svm /(recall_svm + precision_svm)\nprint(\"Precision by svm: \", precision_svm)\nprint(\"Recall by svm: \", recall_svm)\nprint(\"F1 by svm: \", f1_svm)\n\nplt.show()\n\ncorrect_answers_gnb = np.sum(np.equal(pred_result_gnb, label_test))\naccuracy_gnb = correct_answers_gnb / (len(pred_result_gnb)*1.0)\n\nconfusion_matrix_gbm = metrics.confusion_matrix(label_test, pred_result_gnb)\ncm_display_gbm = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_gbm, display_labels = [\"rural\", \"science\"])\ncm_display_gbm.plot()\nplt.title('Confusion Matrix for gbm')\n\nprecision_gnb = metrics.precision_score(label_test, pred_result_gnb, pos_label = 'rural')\nrecall_gnb = metrics.recall_score(label_test, pred_result_gnb, pos_label = 'rural')\nprecision_f1_gnb = metrics.f1_score(label_test, pred_result_gnb, pos_label = 'rural')\n\nprint(\"Accuracy by gnb: \", accuracy_gnb)\nprint(\"Precision by gnb: \", precision_gnb)\nprint(\"Recalln by gnb: \", recall_gnb)\nprint(\"F1 by gnb: \", precision_f1_gnb)\n\nplt.show()\n\n#with open(\"task_3_shuffel.csv\", 'w', newline='') as csv_file:\n#        df.sample(frac=1).sample(frac=1).to_csv(csv_file, sep='\\t', encoding='utf-8')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Abhishek_Murtuza.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\nimport re\n\n\ndf = pd.read_csv(\"polarity.txt\",sep='\\t' , names=['Text', 'Label'])\n\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Abhishek_Murtuza.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "def changetoNum(x):\n    if x == 'pos':\n        return 1\n    elif x == 'neg':\n        return 0\n    else:\n        return None  \n\ndf['Label_num'] = df['Label'].apply(changetoNum)\nprint(df)\ndf = df.drop(['Label'], axis=1)\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Abhishek_Murtuza.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "df = pd.read_csv(\"corpus.txt\",sep='\\t', names=['Text'])\nprint(df)\nimport pandas as pd\nfrom collections import Counter\nimport re\n\ndef create_count_and_probability(input_df, output_file):\n    # Extract the 'Text' column from the DataFrame\n    corpus = input_df['Text'].tolist()\n\n    count_vectors = []\n    probabilities_list = []\n\n    for text in corpus:\n        # Count occurrences of each word in the current row\n        word_counts = Counter(re.findall(r\"[\\w']+\", text))\n\n        #total number of words in the current row\n        total_words = sum(word_counts.values())\n\n        # Calculate Count_Vector and Probability \n        count_vector = [word_counts[word] for word in re.findall(r\"[\\w']+\", text)]\n        probability = [count / total_words for count in count_vector]\n\n        # Appending\n        count_vectors.append(count_vector)\n        probabilities_list.append(probability)\n\n\n    df = pd.DataFrame({\n        'Text': corpus,\n        'Count_Vector': count_vectors,\n        'Probability': probabilities_list\n    })\n\n\n    df.to_csv(output_file, index=False)\n\ninput_file_path = \"corpus.txt\"\ninput_df = pd.read_csv(input_file_path, sep='\\t', names=['Text'])\noutput_file = 'output.csv'\ncreate_count_and_probability(input_df, output_file)\n\n\ndf1 = pd.read_csv(output_file)\nprint(df1)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Abhishek_Murtuza.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\n\n\n# Create DataFrame from the files\ndf1 = pd.read_csv(\"rural.txt\",sep='\\t' ,names=['Document'])\ndf1['Class'] = 'rural'\n\ndf2 = pd.read_csv(\"science.txt\",sep='\\t' , names=['Document'])\ndf2['Class'] = 'science'\n\n\nframes = [df1,df2]\n# Concatenate the dataframes\nresult = pd.concat(frames)\n\nX = result['Document']\ny = result['Class']\n\n# Split the data into train and test sets and use tf-idf vectorizer\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nvectorizer = TfidfVectorizer(stop_words='english')\n\n# Use the same set of features for training and testing and fit the model\nX_train = vectorizer.fit_transform(X_train)\nX_test = vectorizer.fit_transform(X_test)\n\nclf = GaussianNB()\nclf.fit(X_train.toarray()[:, :X_test.shape[1]], y_train)\n\nsvm_classifier = LinearSVC(dual=False)\nsvm_classifier.fit(X_train.toarray()[:, :X_test.shape[1]], y_train)\n# Evaluate classifiers\ndef evaluate_classifier(classifier, X_test, y_test, label):\n    y_pred = classifier.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    recall = metrics.recall_score(y_test, y_pred, pos_label=label)\n    precision = metrics.precision_score(y_test, y_pred, pos_label=label)\n    f1 = metrics.f1_score(y_test, y_pred, pos_label=label)\n    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n    \n    print(f\"\\nEvaluation for {label} Classifier:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Confusion Matrix:\\n{confusion_matrix}\")\n# Evaluate Gaussian NB Classifier\nevaluate_classifier(clf, X_test.toarray(), y_test, 'rural')\n# Evaluate SVM Classifier\nevaluate_classifier(svm_classifier, X_test.toarray(), y_test, 'science')\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mauel_Maximilian.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\nimport pandas as pd\n\ndf = pd.read_csv('polarity.txt', sep='\\t', header=None, names=[\"text\", \"label\"])\nprint(df.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mauel_Maximilian.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "def convert_label(label):\n  if label == 'pos':\n    return 1\n  elif label == 'neg':\n    return 0\n\ndf['label'] = df['label'].apply(convert_label)\n\n#df.drop(columns=['label'], inplace=True)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mauel_Maximilian.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\n\ndef create_count_and_probability(file_name):\n    lines = []\n    with open(file_name, \"r\") as file:\n        lines = file.readlines()\n\n    has_seen_word = set()\n    words = []\n    for line in lines:\n      for word in re.findall(r\"[\\w']+\", line):\n        if word in has_seen_word:\n          continue\n        has_seen_word.add(word)\n        words.append(word)\n\n    csv_file = 'output.txt'\n    with open(csv_file, 'w') as file:\n      file.write(\"Text,Count_Vector,Probability\\n\")\n      for line in lines:\n        ws = re.findall(r\"[\\w']+\", line)\n        to_write = line.strip() + \", \"\n        cs = Counter(ws)\n        cs_ary = []\n        for word in words:\n          cs_ary.append(f\"{cs.get(word, 0)}\")\n        to_write += \"[\" + ','.join(cs_ary) + \"], \"\n        to_write += \"[\" + ','.join([f\"{cs[w]}/{len(ws)}\" for w in ws]) + \"]\\n\"\n        file.write(to_write)\n\n\n    return(csv_file)\noutput_file_name = create_count_and_probability(\"corpus.txt\")\nwith open(output_file_name, 'r') as file:\n  for line in file:\n    print(line)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mauel_Maximilian.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# a)\ndef read_df(file_name, class_name):\n  with open(file_name, \"r\") as file:\n      lines = file.readlines()\n      data = {\"document\": lines, \"class\": [class_name] * len(lines)}\n      return pd.DataFrame(data)\n\ndf = pd.concat([read_df(\"rural.txt\", \"rural\"), read_df(\"science.txt\", \"science\")], ignore_index=True)\n# b)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(df['document'], df['class'], test_size=0.3, random_state=42)\n\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\nnb_classifier = GaussianNB() # naive bayes\nnb_classifier.fit(X_train_tfidf.toarray(), y_train)\n\nsvm_classifier = LinearSVC() # svm\nsvm_classifier.fit(X_train_tfidf, y_train)\n\ndef eval(classifier, X_test, y_test, classifier_name):\n  predictions = classifier.predict(X_test.toarray())\n  accuracy = accuracy_score(y_test, predictions)\n  precision = precision_score(y_test, predictions, pos_label='science')\n  recall = recall_score(y_test, predictions, pos_label='science')\n  f1 = f1_score(y_test, predictions, pos_label='science')\n  conf_matrix = confusion_matrix(y_test, predictions)\n\n  print(f\"{classifier_name}:\")\n  print(f\"Accuracy: {accuracy:.5f}\")\n  print(f\"Precision: {precision:.5f}\")\n  print(f\"Recall: {recall:.5f}\")\n  print(f\"F1 Score: {f1:.5f}\")\n  print(\"Confusion Matrix:\")\n  print(conf_matrix)\n\neval(nb_classifier, X_test_tfidf, y_test, \"Gaussian Naive Bayes\")\neval(svm_classifier, X_test_tfidf, y_test, \"SVM\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\nfrom IPython.display import display\n\n# Read the data.\ndf = pd.read_csv(\"polarity.txt\", sep=\"\\t\", header=None)\n# Print head of the data.\ndisplay(df.head())\n# Rename columns.\ndf.columns = [\"text\", \"label\"]\n# Verify that the classes are only 'pos' and 'neg'.\ndisplay(df[\"label\"].unique())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Create new column converting 'pos' to 1 and 'neg' to 0.\ndf[\"class\"] = df[\"label\"].apply(lambda x: 0 if x == \"neg\" else 1)\n# Check head\ndisplay(df.head())\n# Verify uniqueness of class\ndisplay(df[\"class\"].unique())\n# Drop original column\nnew = df.drop(\"label\", axis=1)\nnew"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ishfaq_Herbrik.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\n\n\ndef create_count_and_probability(file_name):\n    # Read file.\n    with open(\"corpus.txt\", \"r\") as f:\n        raw = f.readlines()\n    # Convert the whole text to lowercase.\n    data = list(map(lambda x: x.lower(), raw))\n    # Get all words by line.\n    words_by_line = list(map(lambda x: re.findall(r\"[\\w]+\", x), data))\n    print(words_by_line)\n    # Create a set of all words.\n    all_words = list(set([word for line in words_by_line for word in line]))\n    all_words = {element: index for index, element in enumerate(all_words)}\n    print(all_words)\n    # Create count vector per line\n    count_vectors = [\n        [0 for _ in range(len(all_words))] for _ in range(len(words_by_line))\n    ]\n    for i, line in enumerate(words_by_line):\n        for word in line:\n            index = all_words[word]\n            count_vectors[i][index] += 1\n    # Create probability vector per line\n    probability_vectors = [[all_words[word] for word in line] for line in words_by_line]\n    # Query count vectors for number of times a word appears in a line.\n    probability_vectors = [\n        [count_vectors[i][index] for index in line]\n        for i, line in enumerate(probability_vectors)\n    ]\n    # Divide each entry of each vectors by the length of the vector.\n    probability_vectors = [\n        [count / len(line) for count in line] for line in probability_vectors\n    ]\n    # Return csv containing three columns: the raw lines, the count vectors, and the probability vectors.\n    df = pd.DataFrame(\n        {\"raw\": raw, \"count\": count_vectors, \"probability\": probability_vectors}\n    )\n    df.to_csv(\"corpus.csv\")\n    return \"corpus.csv\"\n\n\ncreate_count_and_probability(\"corpus.txt\")\n# Verify that csv can be read.\ndf = pd.read_csv(\"corpus.csv\")\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ishfaq_Herbrik.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import (\n    accuracy_score,\n    recall_score,\n    precision_score,\n    f1_score,\n    confusion_matrix,\n)\n\n# Load the contents of the files\nfile_rural_path = \"rural.txt\"\nfile_science_path = \"science.txt\"\nclasses = [\"rural\", \"science\"]\n\nwith open(file_rural_path, \"r\") as file_rural, open(\n    file_science_path, \"r\"\n) as file_science:\n    rural_docs = file_rural.readlines()\n    science_docs = file_science.readlines()\n\n# Create the dataframe\n# Each document is labeled as either rural or science\nrural_data = [(doc.strip(), \"rural\") for doc in rural_docs if doc.strip()]\nscience_data = [(doc.strip(), \"science\") for doc in science_docs if doc.strip()]\ncombined_data = rural_data + science_data\ndf = pd.DataFrame(combined_data, columns=[\"Document\", \"Class\"])\n\n# Split the data into training and testing sets\n# The test size is 30% of the total data\n# The random state is set to 13 for reproducibility\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    df[\"Document\"], df[\"Class\"], test_size=0.3, random_state=13\n)\n\n# Vectorize the text data, this converts the text into numerical features\nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(train_data)\ntest_vectors = vectorizer.transform(test_data)\n\n# Train classifiers\ngnb = GaussianNB()\ngnb.fit(train_vectors.toarray(), train_labels)\n\nsvc = LinearSVC(random_state=13)\nsvc.fit(train_vectors, train_labels)\n\n# Predict with both classifiers, using the test vectors as input\ngnb_predictions = gnb.predict(test_vectors.toarray())\nsvc_predictions = svc.predict(test_vectors)\n\n\n# Evaluation function to calculate the performance metrics for a classifier\ndef evaluate_classifier(predictions, labels):\n    accuracy = accuracy_score(labels, predictions)\n    recall = recall_score(labels, predictions, pos_label=\"rural\")\n    precision = precision_score(labels, predictions, pos_label=\"rural\")\n    f1 = f1_score(labels, predictions, pos_label=\"rural\")\n    confusion = confusion_matrix(labels, predictions)\n    return accuracy, recall, precision, f1, confusion\n\n\n# Evaluate both classifiers\ngnb_scores = evaluate_classifier(gnb_predictions, test_labels)\nsvc_scores = evaluate_classifier(svc_predictions, test_labels)\n\n\ndef print_confusion_matrix(cm, classes):\n    # Formatting the output as a table using pandas\n    cm_table = pd.DataFrame(cm, index=classes, columns=classes)\n    print(cm_table.to_string())\n\n\n# Print the results\nprint(\"GaussianNB:\")\nprint(\"Accuracy: \", gnb_scores[0])\nprint(\"Recall: \", gnb_scores[1])\nprint(\"Precision: \", gnb_scores[2])\nprint(\"F1 Score: \", gnb_scores[3])\nprint(\"Confusion Matrix:\")\nprint_confusion_matrix(gnb_scores[4], classes)\n\n\nprint(\"\\nLinearSVC:\")\nprint(\"Accuracy: \", svc_scores[0])\nprint(\"Recall: \", svc_scores[1])\nprint(\"Precision: \", svc_scores[2])\nprint(\"F1 Score: \", svc_scores[3])\nprint(\"Confusion Matrix:\")\nprint_confusion_matrix(svc_scores[4], classes)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(cm, classes, classifier_name):\n    df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n\n    # Plotting\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(df_cm, annot=True, fmt=\"g\", cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix for {classifier_name}\")\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.show()\n\n\n# Plotting confusion matrix for both classifiers using a heatmap with seaborn\nplot_confusion_matrix(gnb_scores[4], classes=classes, classifier_name=\"GaussianNB\")\nplot_confusion_matrix(svc_scores[4], classes=classes, classifier_name=\"LinearSVC\")\n"
    },
    {
        "file_name": "Assignment_2_Naman_Jain.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# file\nfile_name = 'polarity.txt'\n\n# read the file into DataFrame\ndf = pd.read_csv(file_name, sep='\\t', names=['Text', 'Polarity (String)'])\n\ndf.head()"
    },
    {
        "file_name": "Assignment_2_Naman_Jain.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# function to map polarity strings to numerical values\ndef polarity_to_num(polarity):\n    if polarity == 'pos':\n        return 1\n    else:\n        return 0\n\n# apply\ndf['Polarity (Numeric)'] = df['Polarity (String)'].apply(polarity_to_num)\n\n#drop original column\ndf = df.drop('Polarity (String)', axis=1)\n\ndf.head()"
    },
    {
        "file_name": "Assignment_2_Naman_Jain.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter, defaultdict\n\ndef create_count_and_probability(file_name):\n    # read file\n    with open(file_name, 'r') as file:\n        corpus = file.readlines()\n        \n    # Tokenize\n    tokenized_docs = [re.findall(r\"[\\w']+\", doc) for doc in corpus]\n\n    # vocabulary\n    vocabulary = sorted(set(word for doc in tokenized_docs for word in doc))\n\n    data = defaultdict(list)\n    for i, doc in enumerate(tokenized_docs):\n        word_count = Counter(doc)\n        \n        # count vector\n        count_vector = [word_count[word] for word in vocabulary]\n        \n        # for probability\n        word_count_in_doc = [word_count[word] for word in doc]\n        total_words = len(doc)\n        probability_vector = [f'{count}/{total_words}' for count in word_count_in_doc]\n\n        data['Text'].append(corpus[i])\n        data['Count_Vector'].append(count_vector)\n        data['Probability'].append(probability_vector)\n\n    # dataframe\n    df = pd.DataFrame(data)\n\n    return(df.to_csv('output.csv', index=False))\n\ncreate_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Assignment_2_Naman_Jain.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\ndef prepare_data_and_evaluate(file_1, file_2):\n    #read file\n    with open(file_1, 'r') as file:\n        data1 = [(line.strip(), 1) for line in file if line.strip()]\n\n    with open(file_2, 'r') as file:\n        data2 = [(line.strip(), 2) for line in file if line.strip()]\n\n    # dataframe\n    df = pd.DataFrame(data1 + data2, columns=['Document', 'Class'])\n\n    # split data\n    X_train, X_test, y_train, y_test = train_test_split(df['Document'], df['Class'], test_size=0.3, random_state=42)\n\n    # vectorize\n    vectorizer = TfidfVectorizer()\n    X_train_tfidf = vectorizer.fit_transform(X_train)\n    X_test_tfidf = vectorizer.transform(X_test)\n\n    # train GaussianNB and evaluate\n    gnb = GaussianNB()\n    gnb.fit(X_train_tfidf.toarray(), y_train)\n    y_pred_gnb = gnb.predict(X_test_tfidf.toarray())\n    print(\"GaussianNB Evaluation: \")\n    print(evaluate(y_test, y_pred_gnb))\n    \n\n    # train LinearSVC and evaluate\n    svc = LinearSVC()\n    svc.fit(X_train_tfidf, y_train)\n    y_pred_svc = svc.predict(X_test_tfidf)\n    print(\"LinearSVC Evaluation: \")\n    print(evaluate(y_test, y_pred_svc))\n\ndef evaluate(y_true, y_pred):\n    accuracy = accuracy_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n    \n    metrics = (\n        f\"Accuracy: {accuracy}\\n\"\n        f\"Recall: {recall}\\n\"\n        f\"Precision: {precision}\\n\"\n        f\"F1 Score: {f1}\\n\"\n        f\"Confusion Matrix:\\n{cm}\\n\"\n    )\n    \n    return metrics\n\nprepare_data_and_evaluate(\"rural.txt\", \"science.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Sinem_Donmez.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Creating DataFrame\ndf = pd.read_csv(\"polarity.txt\", sep=\"\\t\", header=None, names=[\"Text\", \"Label\"])"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Sinem_Donmez.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Function to convert labels to numerical values\ndef label_to_numeric(label):\n    return 1 if label == 'pos' else 0\n\n# Applying the function to create a new column with numerical values\ndf['NumericLabel'] = df['Label'].apply(label_to_numeric)\n\n# Dropping the original 'Label' column\ndf_dropped = df.drop(columns=['Label'])\ndf_dropped"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Sinem_Donmez.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\nimport numpy as np\n\n\ndef vocab_creator(corpus_array):\n  vocab = set()\n  word_counter = Counter()\n  for line in corpus_array:\n    words_list = re.findall(r\"[\\w']+\", line.lower())\n    vocab.update(words_list)\n    word_counter.update(words_list)\n  return sorted(vocab)\n\ndef count_vectorizer(corpus_array, vocab_dict):\n    count_vector = np.zeros(len(vocab_dict), dtype=int)\n    words_list = re.findall(r\"[\\w']+\", line.lower())\n    for word in words_list:\n        if word in vocab_dict:\n            count_vector[vocab_dict[word]] += 1\n    return count_vector\n\ndef probability(line):\n    words_list = re.findall(r\"[\\w']+\", line.lower())\n    total_words = len(words_list)\n    probabilities = [f\"{words_list.count(word)}/{total_words}\" for word in words_list]\n    return probabilities\n\n\ndef create_count_and_probability(file_name):\n\n    corpus_array = []\n    with open(file_name, 'r', encoding='utf-8') as my_file:\n          for line in my_file:\n            corpus_array.append(line)\n\n    vocab = vocab_creator(corpus_array)\n    vocab_dict = {word: i for i, word in enumerate(vocab)}\n\n    csv_file = [['Text', 'Count Vector', 'Probability']]\n    for line in corpus_array:\n        vector = count_vectorizer(line, vocab_dict)\n        probabilities = probability(line)\n        csv_file.append([line[:-1], vector, probabilities])\n\n\n\n    # Create a DataFrame\n    # Assuming the first row of the array is the header\n    df = pd.DataFrame(csv_file[1:], columns=csv_file[0])\n\n    # Write DataFrame to a CSV file\n    df.to_csv('my_data.csv', index=False)\n    return(df)\n\n\n\nfile_name = \"corpus.txt\"\ndf = create_count_and_probability(file_name)\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Sinem_Donmez.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Function to read a file and return documents with a label\ndef read_file(file_name, label):\n    with open(file_name, 'r', encoding='utf-8') as file:\n        return [(line.strip(), label) for line in file if line.strip()]\n\n# Read the documents from each file\nrural_documents = read_file('rural.txt', 'rural')\nscience_documents = read_file('science.txt', 'science')\n\n# Combine both sets of documents and labels\nall_documents = rural_documents + science_documents\n\n# Create DataFrame\ndata = pd.DataFrame(all_documents, columns=['Document', 'Class'])\n\n\n#split\ntext_train, text_test, label_train, label_test = train_test_split(data['Document'], data['Class'],\n                                                                  test_size=0.30,\n                                                                  random_state=1234, shuffle=True)\n\nvectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english', use_idf=True)\n\n# Fit and transform the training data\ntfidf_train = vectorizer.fit_transform(text_train)\n\n# Only transform the test data\ntfidf_test = vectorizer.transform(text_test)\n\n#GaussianNB\nnb_classifier = GaussianNB()\nnb_classifier.fit(tfidf_train.toarray(), label_train)\n\n#Linear SVC\nsvm_classifier = svm.LinearSVC()\nsvm_classifier.fit(tfidf_train, label_train)\n\n\n# Function to evaluate a classifier\ndef evaluate_classifier(classifier, tfidf_test, label_test):\n    # Predict on test data\n    predictions = classifier.predict(tfidf_test.toarray() if isinstance(classifier, GaussianNB) else tfidf_test)\n\n    # Calculate metrics\n    accuracy = accuracy_score(label_test, predictions)\n    recall = recall_score(label_test, predictions, average='weighted')\n    precision = precision_score(label_test, predictions, average='weighted')\n    f1 = f1_score(label_test, predictions, average='weighted')\n    conf_matrix = confusion_matrix(label_test, predictions)\n\n    return accuracy, recall, precision, f1, conf_matrix\n\n# Evaluate Naive Bayes Classifier\nnb_accuracy, nb_recall, nb_precision, nb_f1, nb_conf_matrix = evaluate_classifier(nb_classifier, tfidf_test, label_test)\nprint(nb_accuracy, nb_recall, nb_precision, nb_f1, nb_conf_matrix)\n\n# Evaluate Linear SVC Classifier\nsvm_accuracy, svm_recall, svm_precision, svm_f1, svm_conf_matrix = evaluate_classifier(svm_classifier, tfidf_test, label_test)\nprint(svm_accuracy, svm_recall, svm_precision, svm_f1, svm_conf_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\nimport numpy as np\nimport pandas as pd \nimport re\n# load text file \nwith open('./polarity.txt', 'r') as file:\n    # split line by line\n    data = file.read().split('\\n')\n    \n    \n    \n # re pattern for sentence ending with any punctuation mark followed by the label  \ntext_pattern = re.compile(r'(.+?)\\s+(pos|neg)$')\ntexts = []\nlabels = []\nfor line in data:\n    # match text and label\n    match = text_pattern.match(line)\n    if match:\n        # extract text and label\n        text, label = match.groups()\n        # append to list\n        texts.append(text)   \n        labels.append(label)\n    \n\n\n# # create dataframe\ndf_sentiment = pd.DataFrame({'text':texts, 'labels':labels})\ndf_sentiment.describe()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n# label encoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_sentiment['numerical_labels'] = le.fit_transform(df_sentiment['labels'])\ndf_sentiment[ df_sentiment['labels']=='neg' ].head(5)\ndf_sentiment[ df_sentiment['labels']=='pos' ].head(5)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def load_corpus(file_name):\n    \"\"\" \n    Load corpus from file\n    input: file_name\n    output: corpus\n    \"\"\"\n    with open(file_name, 'r') as file:\n        corpus = file.read().split('\\n')\n    return corpus\n\ndef get_vocab_freq(corpus):\n    \"\"\"\n    get the vocabulary frequency of the corpus\n    input: corpus\n    output: word_freq dictionary where key is the word and value is the frequency\n    \"\"\"\n    word_pattern = re.compile(r'\\w+(?=[\\s.,?!;:])')\n    word_freq = {}\n    token_set = {}\n    # first sweep over the corpus to get the vocabulary counts \n    for doc in corpus:\n        tokens = word_pattern.findall(doc)\n        for token in tokens:\n            if token in token_set:\n                token_set[token] = token_set[token] + 1\n            else:\n                token_set[token] = 1\n    # second sweep to get the frequency and probability of each word           \n    for doc in corpus:\n        tokens = word_pattern.findall(doc)\n        token_freq = np.zeros(len(tokens)).astype(int)\n        for i, token in enumerate(tokens):\n                token_freq[i] = token_set[token]\n\n        token_prob = token_freq / len(token_set.keys())\n        # round to 2 decimal places\n        token_prob = np.round(token_prob, 2)\n        word_freq[doc] = [token_freq.tolist(), token_prob.tolist()]\n        \n    return word_freq\n    \n \n    \ndef create_count_and_probability(file_name):\n    \"\"\"\n    Loads corpus from disk and calls get_vocab_freq to get the vocabulary frequency then saves and returns csv file \n    input: file_name\n    output: csv file as text\n    \"\"\"\n    # here comes your code\n    corpus = load_corpus(file_name)\n    word_freq = get_vocab_freq(corpus)\n    # create dataframe with 3 columns : text, count, probability\n    text = word_freq.keys()\n    count = [word_freq[key][0] for key in word_freq.keys()]\n    probability = [word_freq[key][1] for key in word_freq.keys()]\n    df = pd.DataFrame({'Text':text, 'Count Vector':count, 'Probability':probability})\n    csv_file = df.to_csv('corpus_count_and_probability.csv', index=False)\n    # load csv file and return it as text \n    with open('corpus_count_and_probability.csv', 'r') as file:\n        csv_file = file.read()\n    return csv_file\n    \nprint( create_count_and_probability('./corpus.txt') )"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\nscience = load_corpus('./science.txt')\nrural = load_corpus('./rural.txt')\ndf_class = { 'Document':[], 'Class':[] }\nfor doc in science:\n    df_class['Document'].append(doc)\n    df_class['Class'].append('science')\nfor doc in rural:\n    df_class['Document'].append(doc)\n    df_class['Class'].append('rural')\ndf_class = pd.DataFrame(df_class)\n# numerical labeling\ndf_class['Label'] = le.fit_transform(df_class['Class'])\n\n# shuffle dataframe\ndf_class = df_class.sample(frac=1, random_state=1)\ndf_class.head(5)\n# split data into train and test using sklearn\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_class['Document'], df_class['Label'], test_size=0.3, random_state=1)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# create tf-idf vectorizer\nvectorizer = TfidfVectorizer()\n# fit and transform training data\nX_train = vectorizer.fit_transform(X_train)\n# transform test data\nX_test = vectorizer.transform(X_test)\n\n# Train GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train.toarray(), y_train)\n# predict\ny_pred = gnb.predict(X_test.toarray())\n# accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: ', accuracy)\n# classification report\nprint(classification_report(y_test, y_pred))\n# confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n# plot confusion matrix\nsns.heatmap(cm, annot=True, fmt='d')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Gaussian NB Confusion Matrix')\nplt.show()\n# Train SVM\nclf = svm.SVC(kernel='linear')\nclf.fit(X_train, y_train)\n# predict\ny_pred = clf.predict(X_test)\n# accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: ', accuracy)\n# classification report\nprint(classification_report(y_test, y_pred))\n# confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n# plot confusion matrix\nsns.heatmap(cm, annot=True, fmt='d')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Linear SVM Confusion Matrix')\nplt.show()\n## Side by Side comparison between two models using a table for the four metrics\n# here comes your code\n# accuracy\naccuracy = [0.90, 0.92]\n# precision\nprecision = [0.915, 0.925]\n# recall\nrecall = [0.905, 0.925]\n# f1\nf1 = [0.905, 0.92]\n# create dataframe\ndf = pd.DataFrame({'Accuracy':accuracy, 'Precision':precision, 'Recall':recall, 'F1':f1}, index=['Gaussian NB', 'Linear SVM'])\ndf\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jan_Rogge.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\ndef data_frame(fpath):\n    data = pd.read_csv(fpath, sep=\"\\t\", header=None)#read in the data from a file and output a dataframe\n    data = data.rename(columns={0: \"test\", 1: \"label\"})#set the labels of the dataframe\n    return data\n\npol_df = data_frame(\"polarity.txt\")\npol_df"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jan_Rogge.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n#label_to_num turns the labels to 1 or 0\ndef label_to_num(label):\n    nums = []\n    if label[\"label\"] == \"pos\":\n        return 1\n    else:\n        return 0\n\npol_df = data_frame(\"polarity.txt\")\npol_df[\"nums\"] = pol_df.apply(label_to_num, axis= 1)# create a new column \"nums\", that takes the result of apply\ndisplay(pol_df)\n\n#drop the label column\npol_df = pol_df.drop(\"label\", axis=1)\ndisplay(pol_df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jan_Rogge.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def create_count_and_probability(file_name):\n    with open(\"corpus.txt\", \"r\") as file:\n        text = file.read()\n\n    word_list = re.findall(r\"[\\w]+\", text)#list of all words\n    counts = Counter(word_list)#dictionary with {word: count ...}\n    counts_list = list(counts.items())#list of key, value tuples, so [(word, count),...]\n    df = pd.DataFrame(counts_list, columns = [\"text\", \"count_vector\"])#word, count are the columns\n    df[\"probability\"] = df[\"count_vector\"].div(len(counts_list))#probability by dividing every value in count vector by the amount of unique words\n\n    df.to_csv(\"output.csv\")#save the file in an output file\n    return df\n\ncreate_count_and_probability(\"corpus.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jan_Rogge.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\ndata1 = pd.read_csv(\"rural.txt\", sep=\"\\t\", header=None, names=[\"Document\"])\ndata1[\"Class\"] = \"rural\"#add column called class with value rural\n\ndata2 = pd.read_csv(\"science.txt\", sep=\"\\t\", header=None, names=[\"Document\"])\ndata2[\"Class\"] = \"science\"\n\ndata = pd.concat([data1, data2], ignore_index=True)#ignore index regenerates the indeces for resulting df, which is important for df.drop"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jan_Rogge.ipynb",
        "question": "### Naive Bayes:",
        "answer": "X = data[\"Document\"]\ny = data[\"Class\"]\n\ntfidf_vectorizer = TfidfVectorizer()\ngnb = GaussianNB()\n\nX = tfidf_vectorizer.fit_transform(X)\nX = X.toarray()\n\n#split the data into test and training data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = gnb.fit(X,y)#fit the naive bayes to train data\n\ny_pred = model.predict(X_test)\n\n#calculate metrics with sklearn\naccuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\nprecision = sklearn.metrics.precision_score(y_test.to_numpy(), y_pred, pos_label=\"science\")\n#y_test.to_numpy()\nrecall = sklearn.metrics.recall_score(y_test, y_pred, pos_label=\"science\")\nf1 = sklearn.metrics.f1_score(y_test, y_pred, pos_label=\"science\")\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n\nprint(\"accuracy:\", accuracy)\nprint(\"precision:\", precision)\nprint(\"recall:\", recall)\nprint(\"f1:\", f1)\nprint(\"confusion:\", confusion_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jan_Rogge.ipynb",
        "question": "### SVC:",
        "answer": "X = data[\"Document\"]\ny = data[\"Class\"]\n\n#init classes\ntfidf_vectorizer = TfidfVectorizer()\nsvc = LinearSVC()\n\nX = tfidf_vectorizer.fit_transform(X)#vectorize data\nX = X.toarray()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)#split data into test and training data\n\nmodel = svc.fit(X,y)#fit the SVC to train data\n\ny_pred = model.predict(X_test)#\n\n\n#calculate metrics with sklearn\naccuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\nprecision = sklearn.metrics.precision_score(y_test.to_numpy(), y_pred, pos_label=\"science\")\n#y_test.to_numpy()\nrecall = sklearn.metrics.recall_score(y_test, y_pred, pos_label=\"science\")\nf1 = sklearn.metrics.f1_score(y_test, y_pred, pos_label=\"science\")\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n\nprint(\"accuracy:\", accuracy)\nprint(\"precision:\", precision)\nprint(\"recall:\", recall)\nprint(\"f1:\", f1)\nprint(\"confusion:\", confusion_matrix)"
    },
    {
        "file_name": "Assignment_2_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Filename\nfile = 'polarity.txt'\n\n# Column labels\nnames_column = ['Text', 'Label']\n\n# Read file in as Pandas DataFrame\ndf = pd.read_csv(file, sep='\\t', header=None, names=names_column)\n\n# Print the DataFrame\nprint(df)"
    },
    {
        "file_name": "Assignment_2_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Function which converts labels {pos = 1, neg = 0} in integers \ndef label_to_numeric(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n    else:\n        return None  # Handle other cases if necessary\n\n# Create a new column with the numerical labels, using the apply() function\ndf['NumericLabel'] = df['Label'].apply(label_to_numeric)\n\n# Delete the old label column\ndf = df.drop('Label', axis=1)\n\n# Print the updated Dataframe\nprint(df)"
    },
    {
        "file_name": "Assignment_2_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import pandas as pd\nfrom collections import Counter\nimport re\nimport csv\n\ndef create_count_and_probability(file_name):\n\n    # Name for output file\n\n    output_file = \"corpus.csv\"\n\n    # Read the file and extract the text\n    with open(file_name, 'r') as file:\n        text = file.read()\n\n    # Convert the text to lowercase\n    text = text.lower()\n\n    # Get list with all words in the text\n    all_words = get_list_of_all_words(text)\n\n    # Split text in lines\n    lines = text.split(\"\\n\")\n\n    count_vector = []\n    probability_vector = []\n\n    for line in lines:\n        count_vector.append(word_counts_per_line(line,all_words))\n        probability_vector.append(word_probability_per_line(line))\n\n\n    # Create a DataFrame with 'Text', 'Count_Vector', and 'Probability' columns\n    df = pd.DataFrame(columns=['Text', 'Count_Vector', 'Probability'])\n\n    # Populate the DataFrame\n    df['Text'] = lines\n    df['Count_Vector'] = count_vector\n    df['Probability'] = probability_vector\n\n    # Print the updated Dataframe\n    print(df)\n\n    # Write the DataFrame to a CSV file\n    df.to_csv(output_file, index=False, quotechar=\"'\", quoting=csv.QUOTE_ALL)\n\ndef word_probability_per_line(line):\n\n    # Tokenize the line => Delete all special characters\n    words = re.findall(r\"[\\w']+\", line)\n\n    # Count the occurrences of each word\n    word_counts = Counter(words)\n\n    # Save the word probability in a string to support / character\n    result = \"[\"\n\n    # Add for every word the word probability\n    for word in words:\n        result+=str(word_counts[word]) + \"/\" + str(len(words)) + \", \"\n    \n    # Delete the last two characters \n    result = result[:-2]\n\n    # Close the word probability string \n    result += \"]\"\n\n    return result\n\ndef word_counts_per_line(line, all_words):\n\n    # Tokenize the line => Delete all special characters\n    words = re.findall(r\"[\\w']+\", line)\n\n    # Initialize the counter with zero for all words\n    word_counts = Counter({word: 0 for word in all_words})\n\n    # Count the repetitions of the words in the line\n    word_counts.update(words)\n\n    # Return the list of word repetitions\n    return list(word_counts.values())\n\ndef get_list_of_all_words(text):\n\n    # Tokenize text => Delete all special characters\n    words_list = re.findall(r\"[\\w']+\", text)\n\n    # Count the occurrences of each word\n    word_counts = Counter(words_list)\n\n    # Return the list of all Words in the text\n    return(list(word_counts.keys()))\n\ncreate_count_and_probability(\"corpus.txt\")"
    },
    {
        "file_name": "Assignment_2_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Import libarys\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Task a):\n\n# Read the content of the files\nwith open('rural.txt', 'r') as file:\n    rural_content = file.read().split(\"\\n\")\n\nwith open('science.txt', 'r') as file:\n    science_content = file.read().split(\"\\n\")\n\n# Create the DataFrame\ndata = {\n    'Document': rural_content + science_content,\n    'Class': ['rural'] * len(rural_content) + ['science'] * len(science_content)\n}\n\ndf = pd.DataFrame(data)\n\n# Task b):\n\n# Split the data into train (70%) and test (30%) sets\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n\n# Initialize the TfidfVectorizer for tf-idf -> Alternativ use CountVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 1),) # Best result is using ngram = 1\n\n# Fit and transform the training data -> For learning use fit_transform\nX_train = vectorizer.fit_transform(train_df['Document'])\ny_train = train_df['Class']\n\n# Transform the test data -> For testing use transform\nX_test = vectorizer.transform(test_df['Document'])\ny_test = test_df['Class']\n\n# Train Naive Bayes classifier\nnaive_bayes_model = GaussianNB()\nnaive_bayes_model.fit(X_train.toarray(), y_train)\n\n# Train LinearSVC classifier\nsvm_model = LinearSVC()\nsvm_model.fit(X_train, y_train)\n\n# Task c):\n\n# Evaluate Naive Bayes classifier\nnb_predictions = naive_bayes_model.predict(X_test.toarray())\nnb_accuracy = accuracy_score(y_test, nb_predictions)\nnb_classification_report = classification_report(y_test, nb_predictions)\nnb_confusion_matrix = confusion_matrix(y_test, nb_predictions)\n\nprint(\"Naive Bayes Classifier Evaluation:\")\nprint(f\"Accuracy: {nb_accuracy}\")\nprint(\"Classification Report:\\n\", nb_classification_report)\nprint(\"Confusion Matrix:\\n\", nb_confusion_matrix)\n\n# Evaluate LinearSVC classifier\nsvm_predictions = svm_model.predict(X_test)\nsvm_accuracy = accuracy_score(y_test, svm_predictions)\nsvm_classification_report = classification_report(y_test, svm_predictions)\nsvm_confusion_matrix = confusion_matrix(y_test, svm_predictions)\n\nprint(\"\\nLinearSVC Classifier Evaluation:\")\nprint(f\"Accuracy: {svm_accuracy}\")\nprint(\"Classification Report:\\n\", svm_classification_report)\nprint(\"Confusion Matrix:\\n\", svm_confusion_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Faris_Hajdarpasic.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "file_path = \"./polarity.txt\"\npolarity_df = pd.read_csv(file_path, sep='\\t', names=['Text', 'Label'])\npolarity_df.head(4)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Faris_Hajdarpasic.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "polarity_df['numLabel'] = polarity_df['Label'].apply(lambda x: 1 if x == 'pos' else 0)\npolarity_df.drop('Label', axis=1, inplace=True)\npolarity_df.head(4)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Faris_Hajdarpasic.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "\n\n# Generate bag-of-words i.e. features (same as \"count_vectorizer.vocabulary_.keys()\" from tutorial)\n# From corpus => list of documents/sentences\ndef unique_words(corpus):\n    all_words = [re.findall(r\"[\\w']+\", sentence) for sentence in corpus]\n    unique_words_set = set(word.lower() for words in all_words for word in words)\n    unique_words_list = list(unique_words_set)\n    return unique_words_set\n# Generate bag-of-words i.e. features (same as \"count_vectorizer.vocabulary_.keys()\" from tutorial)\n# From plain text\ndef unique_words2(text):\n    words_list = re.findall(r\"[\\w']+\", text)\n    unique_words_set = set(word.lower() for word in words_list)\n    unique_words_list = list(unique_words_set)\n    return unique_words_list\ndef words_probability(sentence):\n    word_list = re.findall(r\"[\\w']+\", sentence)\n    word_list = list(word.lower() for word in word_list)\n    \n    # Calculate word frequencies (returns dictionary where key is word, value is frequency)\n    word_counts = Counter(word_list)\n    \n    total_words = len(word_list)\n    \n    # Calculate probabilities for each word\n    word_probabilities = [word_counts[word] / total_words for word in word_list]\n    \n    return word_probabilities\ndef count_vectorizer(sentence, unique_words):\n    word_list = re.findall(r\"[\\w']+\", sentence)\n    word_list = list(word.lower() for word in word_list)\n    \n    # Count occurrences of UNIQUE words (i.e of all vocabulary words / features / bag-of-words)\n    feature_vector = [word_list.count(word.lower()) for word in unique_words]\n    return feature_vector"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Faris_Hajdarpasic.ipynb",
        "question": "#### Note: DataFrame is returned as the output from the function, and .csv file is generated during execution of this function",
        "answer": "def create_count_and_probability(file_name):\n    # Read file into variable\n    with open(file_name, 'r') as file:\n        text = file.read()\n        \n    # Use sentence tokenizer from nltk to get sentences from string text. Output is list of sentences (i.e. corpus)\n    corpus = sent_tokenize(text)\n    \n    \n    # Get list of unique words from whole text => This words represent features (Bag of Words)\n    # I will assume that capitalized and non-capitalized words are same (so I will lower all words)\n    bag_of_words = unique_words(corpus)\n    \n    # Create an empty DataFrame\n    df = pd.DataFrame(columns=['Sentence', 'Count_Vector', 'Probability'])\n    \n    \n    for sentence in corpus:\n        # For each sentece (i.e. sample) calculate how many times unique words appears in that sentence (i.e. calculate values for all features)\n        feature_vector = count_vectorizer(sentence, bag_of_words)\n        word_counts = words_probability(sentence)\n        df = df.append({'Sentence': sentence, 'Count_Vector': feature_vector, 'Probability': word_counts}, ignore_index=True)\n\n    \n    \n    df.to_csv(\"corpus_parsed_csv.csv\", index=False)\n    \n    return df\ndf = create_count_and_probability(\"corpus.txt\")\n\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Faris_Hajdarpasic.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "science_path = \"./science.txt\"\nrural_path = \"./rural.txt\"\nwith open(science_path, 'r') as file:\n        science = file.read()\nwith open(rural_path, 'r') as file:\n        rural = file.read()\n#science_corpus = re.split(r'[\\n]', science)\n##rural_corpus = re.split(r'[\\n]', rural)\nscience_corpus = science.split('\\n')[:-1]\nrural_corpus = rural.split('\\n')[:-1]\nlen(science_corpus) + len(rural_corpus)\ndf_dataset = pd.DataFrame(columns=['Document', 'Class'])\nfor i in range(len(science_corpus)):\n    df_dataset.loc[i,'Document'] = science_corpus[i]\n    df_dataset.loc[i,'Class'] = \"science\"\nprev_length = len(science_corpus)\nfor j in range(len(rural_corpus)):\n    df_dataset.loc[prev_length + j,'Document'] = rural_corpus[j]\n    df_dataset.loc[prev_length + j,'Class'] = \"rural\"\ndf_dataset.head(5)\ntext_train, text_test, label_train, label_test = train_test_split(df_dataset['Document'], df_dataset['Class'], \n                                                                  test_size=0.30, \n                                                                  random_state=1234, shuffle=True)\n# Create Instance of TfidVectorizer\ntf_idf_vectorizer = TfidfVectorizer()\n\n# Fit that instance to train data\ntf_idf_vectorizer.fit(text_train)\n\n# Transform texual data to numeric data (BoW Features)\ntrain_bow_features = tf_idf_vectorizer.transform(text_train)\ntest_bow_features = tf_idf_vectorizer.transform(text_test)\n# Convert from sparse matrix to dense numpy matrix\ntrain_bow_features = train_bow_features.toarray()\ntest_bow_features = test_bow_features.toarray()\ndef categorical_to_numerical(categorical):\n    numerical = [1 if label == 'science' else 0 for label in categorical]\n    return np.array(numerical)\n\ndef calculate_accuracy(predicted_labels_categorical, gt_labels_categorical):\n    predicted_labels = categorical_to_numerical(predicted_labels_categorical)\n    gt_labels = categorical_to_numerical(gt_labels_categorical)\n    \n    correct_predictions = np.sum(predicted_labels == gt_labels)\n    total_samples = len(gt_labels)\n    accuracy = correct_predictions / total_samples\n    return accuracy\n\ndef calculate_precision(predicted_labels_categorical, gt_labels_categorical):\n    predicted_labels = categorical_to_numerical(predicted_labels_categorical)\n    gt_labels = categorical_to_numerical(gt_labels_categorical)\n    \n    true_positives = np.sum((predicted_labels == 1) & (gt_labels == 1))\n    false_positives = np.sum((predicted_labels == 1) & (gt_labels == 0))\n    precision = true_positives / (true_positives + false_positives)\n    return precision\n\ndef calculate_recall(predicted_labels_categorical, gt_labels_categorical):\n    predicted_labels = categorical_to_numerical(predicted_labels_categorical)\n    gt_labels = categorical_to_numerical(gt_labels_categorical)\n    \n    true_positives = np.sum((predicted_labels == 1) & (gt_labels == 1))\n    false_negatives = np.sum((predicted_labels == 0) & (gt_labels == 1))\n    recall = true_positives / (true_positives + false_negatives)\n    return recall\n\ndef calculate_f1(predicted_labels_categorical, gt_labels_categorical):    \n    precision = calculate_precision(predicted_labels_categorical, gt_labels_categorical)\n    recall = calculate_recall(predicted_labels_categorical, gt_labels_categorical)\n    f1 = 2 * (precision * recall) / (precision + recall)\n    return f1\n\ndef calculate_confusion_matrix(predicted_labels_categorical, gt_labels_categorical):\n    predicted_labels = categorical_to_numerical(predicted_labels_categorical)\n    gt_labels = categorical_to_numerical(gt_labels_categorical)\n    \n    true_positives = np.sum((predicted_labels == 1) & (gt_labels == 1))\n    false_positives = np.sum((predicted_labels == 1) & (gt_labels == 0))\n    true_negatives = np.sum((predicted_labels == 0) & (gt_labels == 0))\n    false_negatives = np.sum((predicted_labels == 0) & (gt_labels == 1))\n\n    confusion_matrix = np.array([[true_negatives, false_positives], [false_negatives, true_positives]])\n\n    return confusion_matrix\nsvm_classifier = svm.LinearSVC()\nsvm_classifier.fit(train_bow_features, label_train);\npredicted_label_test = svm_classifier.predict(test_bow_features)\naccuracy_svc = calculate_accuracy(predicted_label_test, label_test)\nprecision_svc = calculate_precision(predicted_label_test, label_test)\nrecall_svc = calculate_recall(predicted_label_test, label_test)\nf1_svc = calculate_f1(predicted_label_test, label_test)\nconfusion_matrix_svc = calculate_confusion_matrix(predicted_label_test, label_test)\nprint(\"Accuracy of LinearSVC: \", accuracy_svc)\nprint(\"Precision of LinearSVC: \", precision_svc)\nprint(\"Recall of LinearSVC: \", recall_svc)\nprint(\"F1 Score of LinearSVC: \", f1_svc)\nprint(\"Confusion Matrix of LinearSVC: \", confusion_matrix_svc)\nnaive_bayes_classifier = naive_bayes.GaussianNB()\nnaive_bayes_classifier.fit(train_bow_features, label_train);\npredicted_label_test = naive_bayes_classifier.predict(test_bow_features)\naccuracy_gnb = calculate_accuracy(predicted_label_test, label_test)\nprecision_gnb = calculate_precision(predicted_label_test, label_test)\nrecall_gnb = calculate_recall(predicted_label_test, label_test)\nf1_gnb = calculate_f1(predicted_label_test, label_test)\nconfusion_matrix_gnb = calculate_confusion_matrix(predicted_label_test, label_test)\nprint(\"Accuracy of GNB: \", accuracy_gnb)\nprint(\"Precision of GNB: \", precision_gnb)\nprint(\"Recall of GNB: \", recall_gnb)\nprint(\"F1 Score of GNB: \", f1_gnb)\nprint(\"Confusion Matrix of GNB: \", confusion_matrix_gnb)\n\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Gjergj_Plepi.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\nimport pandas as pd\ndata = [] #will store the data: text + label\nwith open(\"polarity.txt\", 'r', encoding='utf-8') as my_file: #read through the file line by line\n    for line in my_file:\n        text, label = line.split(\"\\t\")\n        data.append([text, label[:-1]])\n\ndf = pd.DataFrame(data=data, columns=['Text', 'Label'])\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Gjergj_Plepi.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\ndef apply(df):\n    label_ids = []\n    for index, row in df.iterrows():\n        if row['Label'] == 'pos':\n            label_ids.append(1)\n        else:\n            label_ids.append(0)\n    \n    df['Label_id'] = label_ids\n\n    return df\ndf = apply(df) # create a new column 'Label_id'\ndf = df.drop('Label', axis=1) #drop the 'Label column'\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Gjergj_Plepi.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\nimport numpy as np\nimport csv\ndef create_corpus(file_name):\n    corpus = []\n    with open(file_name, 'r', encoding='utf-8') as my_file: #read through the file line by line\n        for line in my_file:\n            words = re.findall(r\"[\\w']+\", line) #get the words from every line/sentence/document\n            for word in words:\n                word_lower = word.lower() # put every word on lowercase, e.g. we want 'Repeated' and 'repeated' to be the same word\n                if word_lower not in corpus: #if it is a new word, add it to the corpus\n                    corpus.append(word_lower)\n    \n    return corpus\ncorpus = create_corpus(\"corpus.txt\")\nprint(corpus)\nprint(len(corpus))\ndef create_count_and_probability(file_name):\n    # here comes your code\n\n    #Initialize the csv file\n    columns_headers = ['Text', 'Count_Vector', 'Probability']\n    csv_file_name = 'output_csv_file.csv'\n\n    with open(csv_file_name, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(columns_headers)\n\n    #create the corpus\n    corpus = create_corpus(file_name)\n\n    with open(file_name, 'r', encoding='utf-8') as my_file: #read through the file line by line\n        for line in my_file:\n            text = line\n\n            words = re.findall(r\"[\\w']+\", line) #get the words from every line/sentence/document\n            count_word_occurrences = Counter(words) #count the occurrences of each word in this line/sentence/document\n            count_vector = np.zeros(len(corpus)) #the count vector representing the text/senctence. Will be the same size as the length of the corpus\n\n            probability_vector = [] #the probability vector\n            \n            for key in count_word_occurrences.keys():\n                word_lowercase = key.lower()\n                count_vector[corpus.index(word_lowercase)] = count_word_occurrences[key]\n            \n            for word in words:\n                probability_vector.append(count_word_occurrences[word] / len(words))\n            \n            #add row to the csv file\n            data = [text, count_vector, probability_vector]\n            with open(csv_file_name, 'a', newline='') as csv_file:\n                writer = csv.writer(csv_file)\n                writer.writerow(data)\n\n\n    return(csv_file)\ncsv_file = create_count_and_probability(\"corpus.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Gjergj_Plepi.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\ndef create_df(file_name_1, file_name_2):\n    data = [] #will store the data: document + class\n    \n    with open(file_name_1, 'r', encoding='utf-8') as file: #read through the file line by line\n        for line in file:\n            data.append([line, file_name_1[:-4]])\n    \n    with open(file_name_2, 'r', encoding='utf-8') as file: #read through the file line by line\n        for line in file:\n            data.append([line, file_name_2[:-4]])\n    \n    # create the dataframe\n    df = pd.DataFrame(data=data, columns=['Document', 'Class'])\n    # shuffle the dataframe and reset the indices\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    return df\ndf = create_df('rural.txt', 'science.txt')\ndf\n\"\"\"The 'rural' class will be considered as 1, the 'science' class will be considered as 0 \"\"\"\ndef apply(df):\n    label_ids = []\n\n    for index, row in df.iterrows():\n        if row['Class'] == 'rural':\n            label_ids.append(1)\n        else:\n            label_ids.append(0)\n    \n    df['Label'] = label_ids\n    df = df.drop('Class', axis=1) #drop the 'Class' column\n    \n    return df\ndf = apply(df)\ndf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import svm, naive_bayes\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\"\"\"Dataset splitting\"\"\"\ntext_train, text_test, label_train, label_test = train_test_split(df['Document'], df['Label'], \n                                                                  test_size=0.3, \n                                                                  random_state=1234, shuffle=True)\n\"\"\"Encode the text\"\"\"\ntf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),) # 1-gram with words (only unigrams).\nencoded_input_matrix = tf_idf_vectorizer.fit_transform(text_train) \nencoded_test_matrix = tf_idf_vectorizer.transform(text_test)\nencoded_input_matrix.shape # means we have 4117 distinct tokens/words in the vocubulary/corpus\n\"\"\"Initialize the classifier\"\"\"\nnaive_bayes_classifier = naive_bayes.GaussianNB()\n\"\"\"Train the classifier\"\"\"\nnaive_bayes_classifier.fit(encoded_input_matrix.toarray(), label_train)\n\"\"\"Test the classifier\"\"\"\npredictions = naive_bayes_classifier.predict(encoded_test_matrix.toarray())\n\"\"\"Evaluate the model\"\"\"\n#accuracy\ncorrect_answers = np.sum(np.equal(predictions, label_test))\naccuracy = correct_answers / (len(predictions)) * 100\n\ntp = 0 # true positives: predicted as positives (1) and the true label is also positive (1)\nfn = 0 # false negatives: predicted as negatives (0) but the true label is positive (1)\nfp = 0 # false positive: predicted as positives (1) but the true label is negative (0)\n\nfor i in range(len(predictions)):\n    if predictions[i] == 1 and label_test.values[i] == 1:\n        tp += 1\n    elif predictions[i] == 0 and label_test.values[i] == 1:\n        fn += 1\n    elif predictions[i] == 1 and label_test.values[i] == 0:\n        fp += 1\n\n#recall\nrecall = tp / (tp + fn)\n\n#precision\nprecision = tp / (tp + fp)\n\n#f1-score\nf1_score = 2 * precision * recall / (precision + recall)\n\n#confussion matrix\nconf_matrix = confusion_matrix(label_test.values, predictions)\n\nprint(\"Accuracy: \", accuracy)\nprint(\"Precision: \", precision)\nprint(\"Recall: \", recall)\nprint(\"F1-score: \", f1_score)\nconf_matrix #Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.\ndisp = ConfusionMatrixDisplay(conf_matrix, display_labels=['science', 'rural'])\ndisp.plot()\n\"\"\"Initialize the classifier\"\"\"\nsvm_classifier = svm.LinearSVC()\n\"\"\"Train the classifier\"\"\"\nsvm_classifier.fit(encoded_input_matrix.toarray(), label_train)\n\"\"\"Test the classifier\"\"\"\npredictions = naive_bayes_classifier.predict(encoded_test_matrix.toarray())\n\"\"\"Evaluate the model\"\"\"\n#accuracy\ncorrect_answers = np.sum(np.equal(predictions, label_test))\naccuracy = correct_answers / (len(predictions)) * 100\n\ntp = 0 # true positives: predicted as positives (1) and the true label is also positive (1)\nfn = 0 # false negatives: predicted as negatives (0) but the true label is positive (1)\nfp = 0 # false positive: predicted as positives (1) but the true label is negative (0)\n\nfor i in range(len(predictions)):\n    if predictions[i] == 1 and label_test.values[i] == 1:\n        tp += 1\n    elif predictions[i] == 0 and label_test.values[i] == 1:\n        fn += 1\n    elif predictions[i] == 1 and label_test.values[i] == 0:\n        fp += 1\n\n#recall\nrecall = tp / (tp + fn)\n\n#precision\nprecision = tp / (tp + fp)\n\n#f1-score\nf1_score = 2 * precision * recall / (precision + recall)\n\n#confussion matrix\nconf_matrix = confusion_matrix(label_test.values, predictions)\n\nprint(\"Accuracy: \", accuracy)\nprint(\"Precision: \", precision)\nprint(\"Recall: \", recall)\nprint(\"F1-score: \", f1_score)\nconf_matrix #Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.\ndisp = ConfusionMatrixDisplay(conf_matrix, display_labels=['science', 'rural'])\ndisp.plot()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Vella_Rosario.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\nfile_path = './polarity.txt'\ndelimiter = '\\t'\ncolumns = ['Text', 'Label']\n\n# Read the file into a DataFrame\ndf = pd.read_csv(file_path, delimiter=delimiter, header=None, names=columns)\n\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Vella_Rosario.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "def label_to_numeric(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n    else:\n        return None \n    \n#Create new column with numerical labels using apply() \ndf['NumericLabel'] = df['Label'].apply(label_to_numeric)\n\n# Drop the original column\ndf = df.drop('Label', axis=1)\n\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Vella_Rosario.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import pandas as pd\nfrom collections import Counter\nimport re\n\ndef create_count_and_probability(file_name):\n    # Read the input file into a DataFrame with a specified delimiter\n    df = pd.read_csv(file_name, delimiter='\\t', header=None, names=['Text'])\n\n    #count and calculate probability for each line\n    df['WordsList'] = df['Text'].apply(lambda x: re.findall(r\"[\\w']+\", x))\n    df['WordCounts'] = df['WordsList'].apply(lambda x: Counter(x))\n    df['TotalWords'] = df['WordsList'].apply(len)\n    df['Count_Vector'] = df.apply(lambda row: [row['WordCounts'][word] for word in row['WordsList']], axis=1)\n    df['Probability'] = df.apply(lambda row: [count / row['TotalWords'] for count in row['Count_Vector']], axis=1)\n\n    # Drop columns\n    df = df.drop(['WordsList', 'WordCounts', 'TotalWords'], axis=1)\n\n    return df\n\nfile_name = 'corpus.txt'\nresult_df = create_count_and_probability(file_name)\n\nresult_df.to_csv('output.csv', index=False)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Vella_Rosario.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# a\nimport pandas as pd\n\nrural_df = pd.read_csv('rural.txt', sep='\\t', header=None, names=['Document'])\nrural_df['Class'] = 'rural'\n\nscience_df = pd.read_csv('science.txt', sep='\\t', header=None, names=['Document'])\nscience_df['Class'] = 'science'\n\n# Concatenate the two DataFrames into one\ncombined_df = pd.concat([rural_df, science_df], ignore_index=True)\n\n# Display the combined DataFrame\nprint(combined_df.head())\n# b)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    combined_df['Document'], combined_df['Class'], test_size=0.3, random_state=42\n)\n\n# Create the tf-idf vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the vectorizer on the training data\nX_train_tfidf = vectorizer.fit_transform(X_train)\n\n# Transform the test data using the same vectorizer\nX_test_tfidf = vectorizer.transform(X_test)\n# c)\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\n# Gaussian Naive Bayes \ngnb = GaussianNB()\ngnb.fit(X_train_tfidf.toarray(), y_train)\n\n#Predictions on the test set\ny_pred_gnb = gnb.predict(X_test_tfidf.toarray())\n\n#LinearSVC\nsvc = LinearSVC()\nsvc.fit(X_train_tfidf, y_train)\n\n#Prediction on the test set\ny_pred_svc = svc.predict(X_test_tfidf)\n\n# Evaluate Gaussian Naive Bayes \nprint(\"Gaussian Naive Bayes Classifier:\")\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_gnb))\nprint(\"Precision:\", metrics.precision_score(y_test, y_pred_gnb, pos_label='science'))\nprint(\"Recall:\", metrics.recall_score(y_test, y_pred_gnb, pos_label='science'))\nprint(\"F1 Score:\", metrics.f1_score(y_test, y_pred_gnb, pos_label='science'))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_gnb))\n\n#Evaluate LinearSVC\nprint(\"\\nLinear Support Vector Classification (LinearSVC):\")\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_svc))\nprint(\"Precision:\", metrics.precision_score(y_test, y_pred_svc, pos_label='science'))\nprint(\"Recall:\", metrics.recall_score(y_test, y_pred_svc, pos_label='science'))\nprint(\"F1 Score:\", metrics.f1_score(y_test, y_pred_svc, pos_label='science'))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_svc))"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ali_Sehran.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Reading the data\ndf = pd.read_csv('./polarity.txt', sep='\\t', header=None)\n\n# Naming the columns\ndf.columns = ['Text', 'Label']"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ali_Sehran.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Function to convert the label to numeric values\ndef label_to_numeric_value(label):\n    # pos >> 1 and neg >> 0\n    return 1 if label == 'pos' else 0\n\n# Applying the function to the 'Label' column\ndf['N_Label'] = df['Label'].apply(label_to_numeric_value)\n\n# Dropping  the original 'Label' column\ndf = df.drop('Label', axis=1)\n\n# Display the DataFrame\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ali_Sehran.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nimport csv\nfrom collections import Counter\n\n\ndef create_count_and_probability(file_name):\n    with open(file_name, 'r') as f:\n        lines = f.readlines()\n\n    # Create a set of all unique words in the file\n    all_words = []\n    for line in lines:\n        words_list = re.findall(r\"[\\w']+\", line)\n\n        # Convert all words to lowercase\n        words_list = [word.lower() for word in words_list]\n        # add to end of set\n        all_words.extend(words_list)\n\n    # Remove duplicates in list\n    all_words = list(set(all_words))\n    \n    print(len(all_words))\n\n    with open('./output.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Text', 'Count_Vector', 'Probability']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for line in lines:\n            words_list = re.findall(r\"[\\w']+\", line)\n            word_counts = Counter(words_list)\n            total_words = len(words_list)\n            # Create a count vector and probability that includes all words\n            count_vector = [word_counts[word] if word in word_counts else 0 for word in all_words]\n            probability = [count/total_words for count in count_vector]\n            writer.writerow({'Text': line.strip(), 'Count_Vector': count_vector, 'Probability': probability})\n\n    print('Done')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ali_Sehran.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import naive_bayes, svm\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n# Load the data\nrural_df = pd.read_csv('./rural.txt', sep='\\t', header=None,  names=['Document'])\nrural_df['Class'] = 'rural'\nscience_df = pd.read_csv('./science.txt', sep='\\t', header=None,  names=['Document'])\nscience_df['Class'] = 'science'\n# Combine the dataframes\ncombined_df = pd.concat([rural_df, science_df])\n\n# Split the data into training and test sets\n# 70% of the data will be used for training and 30% for testing : test_size=0.3\ntrain_x, test_x, train_y, test_y = train_test_split(combined_df['Document'], combined_df['Class'], test_size=0.3)\n\n# Vectorize the data\nvectorizer = TfidfVectorizer()\ntrain_x_vectors = vectorizer.fit_transform(train_x)\ntest_x_vectors = vectorizer.transform(test_x)\n\n# Gussian Naive Bayes Classifier\nnaive_bayes_classifier = naive_bayes.GaussianNB()\nnaive_bayes_classifier.fit(train_x_vectors.toarray(), train_y)\nnaive_bayes_predictions = naive_bayes_classifier.predict(test_x_vectors.toarray())\n\n# SVM Classifier\nsvm_classifier = svm.SVC(kernel='linear')\nsvm_classifier.fit(train_x_vectors.toarray(), train_y)\nsvm_predictions = svm_classifier.predict(test_x_vectors.toarray())\n\n# Evaluation\nprint('-'*50)\nprint('Naive Bayes Classifier')\nprint('Accuracy: ', accuracy_score(test_y, naive_bayes_predictions))\nprint('Precision: ', precision_score(test_y, naive_bayes_predictions, average='weighted'))\nprint('Recall: ', recall_score(test_y, naive_bayes_predictions, average='weighted'))\nprint('F1: ', f1_score(test_y, naive_bayes_predictions, average='weighted'))\nprint('Confusion Matrix: ', confusion_matrix(test_y, naive_bayes_predictions))\n\nprint('-'*50)\nprint('SVM Classifier')\nprint('Accuracy: ', accuracy_score(test_y, svm_predictions))\nprint('Precision: ', precision_score(test_y, svm_predictions, average='weighted'))\nprint('Recall: ', recall_score(test_y, svm_predictions, average='weighted'))\nprint('F1: ', f1_score(test_y, svm_predictions, average='weighted'))\nprint('Confusion Matrix: ', confusion_matrix(test_y, svm_predictions))\n# Predicting the class of a new document - Test\nnew_doc = ['The sky is blue']\nnew_doc_vector = vectorizer.transform(new_doc)\nprint('-'*50)\nprint('New Document')\nprint('Naive Bayes Classifier: ', naive_bayes_classifier.predict(new_doc_vector.toarray()))\nprint('SVM Classifier: ', svm_classifier.predict(new_doc_vector.toarray()))"
    },
    {
        "file_name": "Assignment_2_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Load the data from the file\nfile_path = 'polarity.txt'\ndf = pd.read_csv(file_path, header=None, delimiter='\\t')\n\n# Naming the columns appropriately\ndf.columns = ['Text', 'Label']\n\ndf.head()"
    },
    {
        "file_name": "Assignment_2_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Function to convert labels to numerical values\ndef convert_label_to_numerical(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n\n# Create a new column with numerical values\ndf['Label_Numerical'] = df['Label'].apply(convert_label_to_numerical)\n\n# Drop the original Label column\ndf.drop('Label', axis=1, inplace=True)\n\ndf.head()"
    },
    {
        "file_name": "Assignment_2_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\nimport pandas as pd\n\ndef create_count_and_probability(file_name):\n    # Read the file\n    with open(file_name, 'r') as file:\n        lines = file.readlines()\n\n    # Process each line\n    data = []\n    for line in lines:\n        # Clean and split the line into words\n        words = re.findall(r\"[\\w']+\", line.lower())\n\n        # Count the words\n        word_count = Counter(words)\n        total_words = sum(word_count.values())\n\n        # Create count vector and probability\n        count_vector = [word_count[word] for word in words]\n        probability = [f\"{count}/{total_words}\" for count in count_vector]\n\n        # Append to data\n        data.append([line.strip(), count_vector, probability])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Text\", \"Count_Vector\", \"Probability\"])\n\n    # Save as CSV\n    csv_file = file_name.replace('.txt', '_output.csv')\n    df.to_csv(csv_file, index=False)\n\n    return csv_file\n\n# Path to the file (replace this with your file path)\nfile_path = 'corpus.txt'\n\n# Call the function and create the CSV file\ncsv_output_path = create_count_and_probability(file_path)"
    },
    {
        "file_name": "Assignment_2_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\nimport numpy as np\n\n# Load the datasets\nwith open('rural.txt', 'r') as file:\n    rural_docs = file.readlines()\n\nwith open('science.txt', 'r') as file:\n    science_docs = file.readlines()\n\n# Create a DataFrame\ndf_rural = pd.DataFrame(rural_docs, columns=['Document'])\ndf_rural['Class'] = 'rural'\n\ndf_science = pd.DataFrame(science_docs, columns=['Document'])\ndf_science['Class'] = 'science'\n\ndf = pd.concat([df_rural, df_science], ignore_index=True)\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(df['Document'], df['Class'], test_size=0.3, random_state=42)\n\n# Vectorizing the data\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Convert to dense matrix for GaussianNB\nX_train_dense = X_train_tfidf.toarray()\nX_test_dense = X_test_tfidf.toarray()\n\n# Initialize classifiers\ngnb = GaussianNB()\nsvc = LinearSVC()\n\n# Train the classifiers\ngnb.fit(X_train_dense, y_train)\nsvc.fit(X_train_tfidf, y_train)\n\n# Predictions\ny_pred_gnb = gnb.predict(X_test_dense)\ny_pred_svc = svc.predict(X_test_tfidf)\n\n# Evaluation\ndef evaluate_model(y_true, y_pred):\n    return {\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Recall\": recall_score(y_true, y_pred, average='weighted'),\n        \"Precision\": precision_score(y_true, y_pred, average='weighted'),\n        \"F1 Score\": f1_score(y_true, y_pred, average='weighted'),\n        \"Confusion Matrix\": confusion_matrix(y_true, y_pred)\n    }\n\nresults = {\n    \"GaussianNB\": evaluate_model(y_test, y_pred_gnb),\n    \"LinearSVC\": evaluate_model(y_test, y_pred_svc)\n}\n\nprint(results)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Weiberg.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\ndata = pd.read_csv('polarity.txt', header = None, sep = \" \t\", names = [\"Text\", \"Label\"])\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Weiberg.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "data[\"NumLabel\"] = data.apply(lambda x: 1 if x[\"Label\"] == \"pos\" else 0, axis=1)\ndata = data.drop(columns=[\"Label\"])\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Weiberg.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\n\ncsv_file_name = \"csv_file.csv\"\n\ndef create_count_and_probability(file_name):\n    with open(file_name, \"r\") as f:\n        lines = f.readlines()\n    words_lists = [re.findall(r\"[\\w']+\", l) for l in lines]     # This is a list of list of words\n    counter = Counter()                                         # Create a \"global\" counter\n    for list in words_lists:                                    # Count global occurences of words\n        counter.update(list)\n\n    # Note: I am using \" to escape commas in sentences and lists\n\n    with open(csv_file_name, \"w\") as csv_f:\n        for i in range(len(lines)):                             # We iterate over input lines\n            line = lines[i]\n            words = words_lists[i]\n            csv_f.write(\"\\\"{}\\\",\".format(line[:-1]))            # We write the original line without \\n\n            counts = [counter[w] for w in words]                # We get the number of global occurences for each word in the line\n            csv_f.write(\"\\\"[{}]\\\",\".format(\",\".join([str(c) for c in counts]))) #And print them\n\n            local_counter = Counter()                           # Then we count the local occurences of words\n            local_counter.update(words)\n            local_counts = [local_counter[w] for w in words]\n            csv_f.write(\"\\\"[{}]\\\"\\n\".format(\",\".join([str(c) + \"/\" + str(len(words)) for c in local_counts])))  # And write them as quotient over the total number of words\n\n    # The text is expected to be output, so I read the file in again to output it\n    with open(csv_file_name, \"r\") as csv_f:\n        csv_file = csv_f.read()\n    return(csv_file)\n\nprint(create_count_and_probability(\"corpus.txt\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Weiberg.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n#a\n# Load rural data\nrural_data = pd.read_table(\"rural.txt\", header = None, names = [\"Document\"])\nrural_data[\"Class\"] = \"rural\"\n# Load science data\nscience_data = pd.read_table(\"rural.txt\", header = None, names = [\"Document\"])\nscience_data[\"Class\"] = \"science\"\n\n# Concatenate into one table\ndata = pd.concat([rural_data, science_data])\n\n#b\n# Split\ntrain, test = train_test_split(data, test_size=0.3)\nvectorizer = TfidfVectorizer()                          #Fit TF IDF Vectorizer to training data\nvectorizer.fit(train)\n# vectorizer.fit(data[\"Document\"])\ntfidf_wm_train = vectorizer.transform(train[\"Document\"])\n\n# Fit GaussianNB\ngauss = GaussianNB()\ngauss.fit(tfidf_wm_train.toarray(), train[\"Class\"])\n\n# Fit LinearSVC\nlsvc = LinearSVC()\nlsvc.fit(tfidf_wm_train.toarray(), train[\"Class\"])\n\n#c\ntfidf_wm_test = vectorizer.transform(test[\"Document\"])\n\ntrain_guess = gauss.predict(tfidf_wm_train.toarray())\nguess = gauss.predict(tfidf_wm_test.toarray())\nprint(\"GaussianNB:\")\nprint(\"Train:\\n\", classification_report(train[\"Class\"], train_guess, zero_division = 0.0))\nprint(\"Test:\\n\", classification_report(test[\"Class\"], guess, zero_division = 0.0))\nprint(\"Test confusion:\\n\", confusion_matrix(test[\"Class\"], guess))\n\ntrain_guess = lsvc.predict(tfidf_wm_train.toarray())\nguess = lsvc.predict(tfidf_wm_test.toarray())\nprint(\"LinearSVC:\")\nprint(\"Train:\\n\", classification_report(train[\"Class\"], train_guess, zero_division = 0.0))\nprint(\"Test:\\n\", classification_report(test[\"Class\"], guess, zero_division = 0.0))\nprint(\"Test confusion:\\n\", confusion_matrix(test[\"Class\"], guess))"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Assuming polarity.txt has two columns, separated by tabs\nfile_path = './polarity.txt'\n\n# Define column names\ncolumn_names = [\"Text\", \"Label\"]\n\n# Read the data into a DataFrame\ndf = pd.read_csv(file_path, sep='\\t', header=None, names=column_names)\n\n# Display the first few rows of the DataFrame\nprint(df.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Define a function to map labels to numerical values\ndef label_to_numeric(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n    else:\n        return label  # Handle other cases if needed\n\n# Apply the function to create a new numerical label column\ndf['Numeric_Label'] = df['Label'].apply(label_to_numeric)\n\n# Drop the original 'Label' column\ndf = df.drop('Label', axis=1)\n\n# Display the updated DataFrame\nprint(df.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import csv\nfrom collections import Counter\nimport re\n\ndef create_count_and_probability(file_name):\n    #read the input file\n    with open(file_name, 'r') as file:\n        lines = file.readlines()\n\n    # initialize lists to store data\n    texts = []\n    count_vectors = []\n    probabilities = []\n\n    #Pprocess each line in the input file\n    for line in lines:\n        #extract words using regular expression\n        words_list = re.findall(r\"[\\w']+\", line)\n\n        # calculate count vector using Counter\n        count_vector = list(Counter(words_list).values())\n\n        #calculate probabilities\n        total_words = len(words_list)\n        probability = [f'{count}/{total_words}' for count in count_vector]\n\n        #append data to lists\n        texts.append(line.strip())\n        count_vectors.append(count_vector)\n        probabilities.append(probability)\n\n    #Create DataFrame\n    result_df = pd.DataFrame({\n        'Text': texts,\n        'Count_Vector': count_vectors,\n        'Probability': probabilities\n    })\n\n    #Write DataFrame to CSV\n    csv_file=result_df.to_csv(index=False, quoting=csv.QUOTE_NONNUMERIC)\n\n    return(csv_file)\nprint(create_count_and_probability('corpus.txt'))"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\nimport pandas as pd\n# Function to read and preprocess data from a file\ndef read_and_preprocess(file_path, class_label):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Document': lines,\n        'Class': class_label\n    })\n    \n    return df\n\n# Read and preprocess rural data\nrural_df = read_and_preprocess('rural.txt', 'rural')\n\n# Read and preprocess science data\nscience_df = read_and_preprocess('science.txt', 'science')\n\n# Concatenate the two DataFrames\ndf = pd.concat([rural_df, science_df], ignore_index=True)\n\n# Split the data into train and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n\n# Initialize the TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(lowercase=True,analyzer=\"word\",sublinear_tf=True,ngram_range=(1,2),norm=\"l2\",use_idf=True,max_features=None,min_df=1)\n\n# Transform the training data\nX_train = vectorizer.fit_transform(train_df['Document'])\n\n# Transform the test data\nX_test = vectorizer.transform(test_df['Document'])\n\n# Convert to dense matrix for Gaussian NB\nX_train_dense = X_train.toarray()\nX_test_dense = X_test.toarray()\n\n# Initialize classifiers\ngnb = GaussianNB()\nsvm = LinearSVC(dual=False)\n\n# Train classifiers\ngnb.fit(X_train_dense, train_df['Class'])\nsvm.fit(X_train, train_df['Class'])\n\n# Predictions\ny_pred_gnb = gnb.predict(X_test_dense)\ny_pred_svm = svm.predict(X_test)\n\n# Evaluation metrics\naccuracy_gnb = metrics.accuracy_score(test_df['Class'], y_pred_gnb)\naccuracy_svm = metrics.accuracy_score(test_df['Class'], y_pred_svm)\n\nrecall_gnb = metrics.recall_score(test_df['Class'], y_pred_gnb,  average='weighted')\nrecall_svm = metrics.recall_score(test_df['Class'], y_pred_svm, average='weighted')\n\nprecision_gnb = metrics.precision_score(test_df['Class'], y_pred_gnb,  average='weighted')\nprecision_svm = metrics.precision_score(test_df['Class'], y_pred_svm,  average='weighted')\n\nf1_gnb = metrics.f1_score(test_df['Class'], y_pred_gnb, average='weighted')\nf1_svm = metrics.f1_score(test_df['Class'], y_pred_svm, average='weighted')\n\nconf_matrix_gnb = metrics.confusion_matrix(test_df['Class'], y_pred_gnb)\nconf_matrix_svm = metrics.confusion_matrix(test_df['Class'], y_pred_svm)\n\n# Print results\nprint(\"Gaussian NB Classifier:\")\nprint(f\"Accuracy: {accuracy_gnb}\")\nprint(f\"Recall: {recall_gnb}\")\nprint(f\"Precision: {precision_gnb}\")\nprint(f\"F1 Score: {f1_gnb}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix_gnb)\n\nprint(\"\\nLinear SVM Classifier:\")\nprint(f\"Accuracy: {accuracy_svm}\")\nprint(f\"Recall: {recall_svm}\")\nprint(f\"Precision: {precision_svm}\")\nprint(f\"F1 Score: {f1_svm}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix_svm)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ilaha_Manafova.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter\nfrom fractions import Fraction\nimport math\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn import naive_bayes\nimport csv\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\ndf = pd.read_csv(\"polarity.txt\", sep='\\t', names=[\"Text\", \"Label\"])\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ilaha_Manafova.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "def convert_to_num(label):\n    label_mapping = {'pos': 1, 'neg': 0}\n    return label_mapping[label]\ndf['Numerical_Label'] = df['Label'].apply(convert_to_num)\ndf\ndf.drop('Label', axis=1, inplace=True)\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ilaha_Manafova.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def create_count_and_probability(file_name):\n    \n    data = pd.read_csv(file_name, sep='\\t', index_col=False, header=None, names=[\"Docs\"])\n    \n    # list of all unique words sorted alphabetically\n    words_list = sorted(data['Docs'].str.lower().str.findall(r\"[\\w']+\").explode().unique().tolist())\n    \n    \n    count_final = list()\n    probability_final = list()\n\n\n    for doc in data['Docs']:\n\n        # these 2 lists are for keeping results for each document\n        res_per_line = [0] * len(words_list)\n        prob_list_doc = list()\n        \n        words = re.findall(r\"[\\w']+\", doc.lower())\n        c = Counter(words) \n        \n        \n        # counting the words\n        for ind in range(0, len(words_list)):\n            res_per_line[ind] = c[words_list[ind]]\n        count_final.append(res_per_line)    \n\n        # calculating probabilities --> count of each word divided by the total number of words in the doc\n        for word in words:\n            cnt = c[word]\n            ln = len(words)\n            prob_list_doc.append((str(cnt) + '/' + str(ln))) #kept the fraction as str not to have the reduced form eg.1/3\n        probability_final.append(prob_list_doc)\n            \n\n    output_df = {'Text': data['Docs'], 'Count_Vector': count_final, 'Probability': probability_final}\n \n    df = pd.DataFrame(output_df)\n    \n    csv_file = 'output.csv'\n    \n    df.to_csv(csv_file, index=False)\n\n   \n    return(csv_file)\ncreate_count_and_probability(\"corpus.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ilaha_Manafova.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# a\nrural = pd.read_csv(\"rural.txt\", sep='\\t', header=None, names=['Document'])\nscience = pd.read_csv(\"science.txt\", sep='\\t', header=None, names=['Document'])\n\n# adding new column \nrural['Class'] = 'rural'\nscience['Class'] = 'science'\n\n# concating 2 dataframes\ndata = pd.concat([rural, science], ignore_index=True)\ndata\n# b\ntext_train, text_test, label_train, label_test = train_test_split(data['Document'], data['Class'],\n                                                                  test_size=0.30, random_state=42, shuffle=True)\ntf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),)\ntext_train_transformed = tf_idf_vectorizer.fit_transform(text_train) # transformed features of train data\n# print(\"FEATURES:\")\n# print(tf_idf_vectorizer.vocabulary_.keys())\nprint(text_train_transformed.toarray())\nsvm_classifier = svm.LinearSVC()\nsvm_classifier.fit(text_train_transformed, label_train)\ntext_test_transformed = tf_idf_vectorizer.transform(text_test)\ntest_svm = svm_classifier.predict(text_test_transformed)\ntest_svm\nnaive_bayes_classifier = naive_bayes.GaussianNB()\nnaive_bayes_classifier.fit(text_train_transformed.toarray(), label_train)\ntest_naive_bayes = naive_bayes_classifier.predict(text_test_transformed.toarray())\ntest_naive_bayes\n# c\nprint(\"SVM results\")\nprint(accuracy_score(label_test, test_svm))\nprint(precision_score(label_test, test_svm, average='weighted'))\nprint(recall_score(label_test, test_svm, average='weighted'))\nprint(f1_score(label_test, test_svm, average='weighted'))\nprint(confusion_matrix(label_test, test_svm))\nprint(\"Naive Bayes results\")\nprint(accuracy_score(label_test, test_naive_bayes))\nprint(precision_score(label_test, test_naive_bayes, average='weighted'))\nprint(recall_score(label_test, test_naive_bayes, average='weighted'))\nprint(f1_score(label_test, test_naive_bayes, average='weighted'))\nprint(confusion_matrix(label_test, test_naive_bayes))"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_AlekseiZhuravlev_AffanZafar_1.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\ndata = pd.read_csv(\"polarity.txt\", sep=\"\\t\", names=['Text', 'Label'])\n\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_AlekseiZhuravlev_AffanZafar_1.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "def convert_labels(label):\n  if(label=='pos'):\n    label=1\n  elif(label=='neg'):\n    label=-1\n\n  return label\n\ndata['numeric_labels'] = data['Label'].apply(lambda x: convert_labels(x))\ndata.drop(columns=['Label'])\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_AlekseiZhuravlev_AffanZafar_1.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\n\ndef create_count_and_probability(file_name):\n    text = open(file_name,'r').read()\n    words_list = re.findall(r\"[\\w']+\", text)\n\n    word_counts = Counter(words_list)\n\n    # Total number of words\n    total_words = sum(word_counts.values())\n\n    # Calculate probability of each word\n    word_probabilities = {word: count / total_words for word, count in word_counts.items()}\n\n    data = {\n        'Text': list(word_counts.keys()),\n        'Count_Vector': list(word_counts.values()),\n        'Probability': [word_probabilities[word] for word in word_counts.keys()]\n    }\n\n    result = pd.DataFrame(data)\n\n    # Write DataFrame to CSV\n    result.to_csv(\"output_file.csv\", index=False)\n    return\n\ncreate_count_and_probability(\"corpus.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_AlekseiZhuravlev_AffanZafar_1.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# read rural.txt\nwith open(\"rural.txt\", \"r\") as f:\n    rural = f.readlines()\n\nprint(rural[:5])\n# Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n\nimport pandas as pd\n\nrural = [x.strip() for x in rural]\nrural = pd.DataFrame(rural, columns = [\"Document\"])\nrural[\"Class\"] = \"rural\"\nrural.head()\n# read science.txt\nwith open(\"science.txt\", \"r\") as f:\n    science = f.readlines()\n\nprint(science[:5])\nscience = [x.strip() for x in science]\nscience = pd.DataFrame(science, columns = [\"Document\"])\n\nscience[\"Class\"] = \"science\"\nscience.head()\n# concatenate the two dataframes\ndf = pd.concat([rural, science], ignore_index=True)\ndf.head()\n# b) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n#- naive_bayes.GaussianNB()\n#- svm.LinearSVC().\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\nX_train, X_test, y_train, y_test = train_test_split(df[\"Document\"], df[\"Class\"], test_size=0.3, random_state=42)\n\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train)\nX_test = vectorizer.transform(X_test)\n\ngnb = GaussianNB()\ngnb.fit(X_train.toarray(), y_train)\nprint('gnb.score', gnb.score(X_test.toarray(), y_test))\n\nlsvc = svm.LinearSVC()\nlsvc.fit(X_train.toarray(), y_train)\nprint('lsvc.score', lsvc.score(X_test.toarray(), y_test))\n# c) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ny_pred_gnb = gnb.predict(X_test.toarray())\nprint('####### GaussianNB #######')\nprint('score', gnb.score(X_test.toarray(), y_test))\nprint(classification_report(y_test, y_pred_gnb))\nprint('confusion matrix\\n', confusion_matrix(y_test, y_pred_gnb))\n\nprint('\\n')\n\ny_pred_lsvc = lsvc.predict(X_test.toarray())\nprint('####### LinearSVC #######')\nprint('score', lsvc.score(X_test.toarray(), y_test))\nprint(classification_report(y_test, y_pred_lsvc))\nprint('confusion matrix\\n', confusion_matrix(y_test, y_pred_lsvc))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Load the data from the file\nfile_path = 'polarity.txt'\ndata = pd.read_csv(file_path, header=None, delimiter=\"\\t\")\n\n# Renaming the columns\ndata.columns = [\"Text\", \"Label\"]\n\ndata.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Function to convert labels to numerical values\ndef convert_label_to_numeric(label):\n    return 1 if label == \"pos\" else 0\n\n# Applying the function to create a new column with numerical labels\ndata['NumericLabel'] = data['Label'].apply(convert_label_to_numeric)\n\n# Dropping the original 'Label' column\ndata.drop('Label', axis=1, inplace=True)\n\ndata.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\nimport pandas as pd\n\ndef create_count_and_probability(file_path):\n    # Reading the file\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n\n    # Preparing data for the DataFrame\n    data = []\n\n    for line in lines:\n        # Extract words using regular expression\n        words_list = re.findall(r\"[\\w']+\", line.lower())\n\n        # Counting words using Counter\n        counted_words = Counter(words_list)\n\n        # Creating count vector\n        count_vector = [counted_words[word] for word in words_list]\n\n        # Creating probability vector\n        total_words = len(words_list)\n        probability_vector = [counted_words[word]/total_words for word in words_list]\n\n        # Appending to data list\n        data.append([line.strip(), count_vector, probability_vector])\n\n    # Creating DataFrame\n    df = pd.DataFrame(data, columns=[\"Text\", \"Count_Vector\", \"Probability\"])\n\n    # Save DataFrame to CSV\n    file_csv = 'count_vector_and_probability.csv'\n    df.to_csv(file_csv, index=False)\n\n    return file_csv\n\n\ncorpus_file_path = 'corpus.txt'\n\n# Calling the function to create and save the CSV file\ncsv_output_path = create_count_and_probability(corpus_file_path)\n# csv_output_path"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\ndef read_file(file_path):\n    # Reading the file\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n    \n    return lines\n\ndef create_df(rural, science):\n    # Create a DataFrame for rural\n    rural_df = pd.DataFrame(rural, columns=['Document'])\n    rural_df['Class'] = 'rural'\n\n    # Create a DataFrame for science\n    science_df = pd.DataFrame(science, columns=['Document'])\n    science_df['Class'] = 'science'\n\n    return rural_df, science_df\n\ndef test_train_split(rural_df, science_df):\n    # Split into 70% train and 30% test dataset separately\n    # in order to split exact amount, it needs to be splitted separately and then datasets can be combined\n    rural_train, rural_test = train_test_split(rural_df, test_size=0.3, random_state=1000, shuffle=True)\n    science_train, science_test = train_test_split(science_df, test_size=0.3, random_state=1000, shuffle=True)\n\n    # Combine datasets\n    train_data = pd.concat([rural_train, science_train], ignore_index=0)\n    test_data = pd.concat([rural_test, science_test], ignore_index=0)\n\n    # Return with shuffling using sample()\n    return train_data.sample(frac=1), test_data.sample(frac=1)\n\ndef tf_idf_vectorizer(train_data, test_data):\n    vectorizer = TfidfVectorizer()\n    document_train_tfidf = vectorizer.fit_transform(train_data['Document'])\n    document_test_tfidf = vectorizer.transform(test_data['Document'])\n\n    return document_train_tfidf, document_test_tfidf\n\ndef train_model(document_train_tfidf, train_data):\n    # GaussianNB\n    gnb = GaussianNB()\n    gnb.fit(document_train_tfidf.toarray(), train_data['Class'])  # Converting to dense array\n\n    # LinearSVC\n    svc = LinearSVC()\n    svc.fit(document_train_tfidf, train_data['Class'])\n\n    return gnb, svc\n\ndef evaluate_model(model, document_test, class_test, title):\n    # Making predictions\n    y_pred = model.predict(document_test)\n\n    # Calculating metrics\n    accuracy = accuracy_score(class_test, y_pred)\n    recall = recall_score(class_test, y_pred, average='weighted')\n    precision = precision_score(class_test, y_pred, average='weighted')\n    f1 = f1_score(class_test, y_pred, average='weighted')\n\n    # Printing metrics\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n\n    # Confusion Matrix\n    cm = confusion_matrix(class_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title(title)\n    plt.show()\n\n\n\nrural_data = read_file(\"rural.txt\")\nscience_data = read_file(\"science.txt\")\nrural_df, science_df = create_df(rural_data, science_data)\ntrain_data, test_data = test_train_split(rural_df, science_df)\ndocument_train_tfidf, document_test_tfidf = tf_idf_vectorizer(train_data, test_data)\ngnb, svc = train_model(document_train_tfidf, train_data)\n\nevaluate_model(gnb, document_test_tfidf.toarray(), test_data['Class'], 'Gaussian NB Classifier')\nevaluate_model(svc, document_test_tfidf.toarray(), test_data['Class'], 'Linear SVC Classifier')"
    },
    {
        "file_name": "Assignment_2_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\ncolumns = ['Text', 'Label']  \ndf = pd.read_csv('polarity.txt', sep='\\t', header=None, names=columns)\nprint(df)"
    },
    {
        "file_name": "Assignment_2_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "def convert_label_to_numercialValue(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n    else:\n        return None  \n        \ndf['NumericalValue'] = df['Label'].apply(convert_label_to_numercialValue)\n\ndf.drop('Label', axis=1, inplace=True)\n\nprint(df)"
    },
    {
        "file_name": "Assignment_2_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nimport csv\nfrom collections import Counter\n\ndef create_count_and_probability(input_file):\n    with open(input_file, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n \n    data = []\n    for line in lines:\n        text = line.strip().lower()  # Convert to lowercase\n        words = re.findall(r'\\b\\w+\\b', text)  \n        word_count = Counter(words)\n        total_words = len(words)\n        count_vector = [word_count[word] for word in words]\n        probability = [count / total_words for count in count_vector]\n\n        data.append([text, count_vector, probability])\n\n\n    output_file = 'output.csv'\n    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        csv_writer = csv.writer(csvfile)\n        csv_writer.writerow(['Text', 'Count_Vector', 'Probability'])\n        csv_writer.writerows(data)\n    \n\ninput_file_path = 'corpus.txt'\ncreate_count_and_probability(input_file_path)"
    },
    {
        "file_name": "Assignment_2_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\nimport pandas as pd\nfrom tabulate import tabulate\n\ndef create_dataframe(file_path, class_label):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        documents = file.readlines()\n\n    df = pd.DataFrame({\n        'Document': documents,\n        'Class': class_label\n    })\n\n    return df\n\ndef evaluate_classifier(predictions, true_labels):\n    accuracy = metrics.accuracy_score(true_labels, predictions)\n    precision = metrics.precision_score(true_labels, predictions, average='weighted')\n    recall = metrics.recall_score(true_labels, predictions, average='weighted')\n    f1 = metrics.f1_score(true_labels, predictions, average='weighted')\n    confusion_matrix = metrics.confusion_matrix(true_labels, predictions)\n\n    return accuracy, precision, recall, f1, confusion_matrix\n\n\n# DataFrame\nrural_df = create_dataframe('rural.txt', 'rural')\nscience_df = create_dataframe('science.txt', 'science')\ncombined_df = pd.concat([rural_df, science_df], ignore_index=True)\n# print(combined_df)\ntrain_df, test_df = train_test_split(combined_df, test_size=0.3, random_state=42)\nvectorizer = TfidfVectorizer()\n\nX_train = vectorizer.fit_transform(train_df['Document'])\nX_train_dense = X_train.toarray()\n\nX_test = vectorizer.transform(test_df['Document'])\nX_test_dense = X_test.toarray()\n\n#Gaussian\ngnb = GaussianNB()\ngnb.fit(X_train_dense, train_df['Class'])\ngnb_predictions = gnb.predict(X_test_dense)\n\n#linearSVC\nsvc = LinearSVC()\nsvc.fit(X_train, train_df['Class'])\nsvc_predictions = svc.predict(X_test)\n\n\n#Evaluation\ngnb_accuracy, gnb_precision, gnb_recall, gnb_f1, gnb_confusion_matrix = evaluate_classifier(gnb_predictions, test_df['Class'])\nsvc_accuracy, svc_precision, svc_recall, svc_f1, svc_confusion_matrix = evaluate_classifier(svc_predictions, test_df['Class'])\n\nresults_table = [\n    [\"Gaussian Naive Bayes\", gnb_accuracy, gnb_precision, gnb_recall, gnb_f1],\n    [\"Linear Support Vector\", svc_accuracy, svc_precision, svc_recall, svc_f1]\n]\n\nheaders = [\"Classifier\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\nprint(\"\\n\")\nprint(tabulate(results_table, headers, tablefmt=\"grid\"))\nprint(\"\\nConfusion Matrix for Gaussian Naive Bayes:\")\nprint(gnb_confusion_matrix)\nprint(\"\\nConfusion Matrix for Linear Support Vector:\")\nprint(svc_confusion_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jing_Wu.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Read the file into a DataFrame\nfile_path = 'polarity.txt'  # Replace with the actual file path\ncolumn_names = [\"Text\", \"Label\"]  # Assuming two columns - Text and Label\n\n# Assuming the file is space-separated, adjust the separator accordingly\ndata = pd.read_csv(file_path, sep='\\t', header=None, names=column_names)\n\n# Display the first few rows of the DataFrame\nprint(data.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jing_Wu.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Create a mapping dictionary for string labels to numerical values\nlabel_mapping = {'pos': 1, 'neg': 0}  # Mapping 'pos' to 1 and 'neg' to 0\n\n# Apply the mapping function to create a new numerical column\ndata['Numerical_Label'] = data['Label'].apply(lambda x: label_mapping.get(x))\n\n# Drop the original 'Label' column\ndata.drop(columns=['Label'], inplace=True)\n\n# Display the updated DataFrame\nprint(data.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jing_Wu.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import csv\nfrom collections import Counter\nimport re\n\ndef create_count_and_probability(file_name):\n    # Read the content of the file\n    with open(file_name, 'r') as file:\n        text = file.read()\n    \n    # Extract words using regex and convert to lowercase\n    words_list = re.findall(r\"[\\w']+\", text.lower())\n    \n    # Count words using Counter\n    word_counts = Counter(words_list)\n    \n    # Calculate total word count\n    total_words = sum(word_counts.values())\n    \n    # Generate count vector and probability list for each word\n    count_vector = [word_counts[word] for word in words_list]\n    probabilities = [f\"{word_counts[word]}/{total_words}\" for word in words_list]\n    \n    # Create list of lists containing Text, Count_Vector, and Probability\n    data = []\n    for i, word in enumerate(words_list):\n        data.append([word, count_vector[i], probabilities[i]])\n    \n    # Write to CSV file\n    csv_file = 'output.csv'\n    with open(csv_file, 'w', newline='') as c_file:\n        writer = csv.writer(c_file)\n        writer.writerow(['Text', 'Count_Vector', 'Probability'])\n        writer.writerows(data)\n    \n    return csv_file\n\n# Example usage:\ninput_file_name = 'corpus.txt'  # Replace with the name of your input file\nresulting_csv = create_count_and_probability(input_file_name)\nprint(f\"CSV file '{resulting_csv}' has been generated.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Jing_Wu.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# Read data and create DataFrame\nrural_data = open('rural.txt', 'r').read().splitlines()\nscience_data = open('science.txt', 'r').read().splitlines()\n\nrural_df = pd.DataFrame({'Document': rural_data, 'Class': 'rural'})\nscience_df = pd.DataFrame({'Document': science_data, 'Class': 'science'})\n\ncombined_df = pd.concat([rural_df, science_df], ignore_index=True)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(combined_df['Document'], combined_df['Class'], test_size=0.3, random_state=42)\n\n# TF-IDF Vectorization\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Train classifiers\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train_tfidf.toarray(), y_train)\n\n# Train classifiers with explicit dual parameter setting to suppress the warning\nsvm_classifier = LinearSVC(dual=True)\nsvm_classifier.fit(X_train_tfidf, y_train)\n\n# Evaluate classifiers\ndef evaluate_classifier(classifier, X_test, y_test):\n    y_pred = classifier.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, pos_label='science')\n    recall = recall_score(y_test, y_pred, pos_label='science')\n    f1 = f1_score(y_test, y_pred, pos_label='science')\n    confusion = confusion_matrix(y_test, y_pred)\n    return accuracy, precision, recall, f1, confusion\n\n# Evaluate Naive Bayes Classifier\nnb_accuracy, nb_precision, nb_recall, nb_f1, nb_confusion = evaluate_classifier(nb_classifier, X_test_tfidf.toarray(), y_test)\n\n# Evaluate Linear SVC Classifier\nsvm_accuracy, svm_precision, svm_recall, svm_f1, svm_confusion = evaluate_classifier(svm_classifier, X_test_tfidf, y_test)\n\n# Print evaluation metrics\nprint(\"Naive Bayes Classifier:\")\nprint(f\"Accuracy: {nb_accuracy}\")\nprint(f\"Precision: {nb_precision}\")\nprint(f\"Recall: {nb_recall}\")\nprint(f\"F1 Score: {nb_f1}\")\nprint(f\"Confusion Matrix:\\n{nb_confusion}\")\n\nprint(\"\\nLinear SVC Classifier:\")\nprint(f\"Accuracy: {svm_accuracy}\")\nprint(f\"Precision: {svm_precision}\")\nprint(f\"Recall: {svm_recall}\")\nprint(f\"F1 Score: {svm_f1}\")\nprint(f\"Confusion Matrix:\\n{svm_confusion}\")\n"
    },
    {
        "file_name": "Assignment_2_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Load the data from the file\nfile_path = 'polarity.txt'\ndf = pd.read_csv(file_path, header=None, delimiter='\\t')\n\n# Naming the columns appropriately\ndf.columns = ['Text', 'Label']\n\ndf.head()"
    },
    {
        "file_name": "Assignment_2_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Function to convert labels to numerical values\ndef convert_label_to_numerical(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n\n# Create a new column with numerical values\ndf['Label_Numerical'] = df['Label'].apply(convert_label_to_numerical)\n\n# Drop the original Label column\ndf.drop('Label', axis=1, inplace=True)\n\ndf.head()"
    },
    {
        "file_name": "Assignment_2_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\nimport pandas as pd\n\ndef create_count_and_probability(file_name):\n    # Read the file\n    with open(file_name, 'r') as file:\n        lines = file.readlines()\n\n    # Process each line\n    data = []\n    for line in lines:\n        # Clean and split the line into words\n        words = re.findall(r\"[\\w']+\", line.lower())\n\n        # Count the words\n        word_count = Counter(words)\n        total_words = sum(word_count.values())\n\n        # Create count vector and probability\n        count_vector = [word_count[word] for word in words]\n        probability = [f\"{count}/{total_words}\" for count in count_vector]\n\n        # Append to data\n        data.append([line.strip(), count_vector, probability])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Text\", \"Count_Vector\", \"Probability\"])\n\n    # Save as CSV\n    csv_file = file_name.replace('.txt', '_output.csv')\n    df.to_csv(csv_file, index=False)\n\n    return csv_file\n\n# Path to the file (replace this with your file path)\nfile_path = 'corpus.txt'\n\n# Call the function and create the CSV file\ncsv_output_path = create_count_and_probability(file_path)"
    },
    {
        "file_name": "Assignment_2_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\nimport numpy as np\n\n# Load the datasets\nwith open('rural.txt', 'r') as file:\n    rural_docs = file.readlines()\n\nwith open('science.txt', 'r') as file:\n    science_docs = file.readlines()\n\n# Create a DataFrame\ndf_rural = pd.DataFrame(rural_docs, columns=['Document'])\ndf_rural['Class'] = 'rural'\n\ndf_science = pd.DataFrame(science_docs, columns=['Document'])\ndf_science['Class'] = 'science'\n\ndf = pd.concat([df_rural, df_science], ignore_index=True)\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(df['Document'], df['Class'], test_size=0.3, random_state=42)\n\n# Vectorizing the data\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Convert to dense matrix for GaussianNB\nX_train_dense = X_train_tfidf.toarray()\nX_test_dense = X_test_tfidf.toarray()\n\n# Initialize classifiers\ngnb = GaussianNB()\nsvc = LinearSVC()\n\n# Train the classifiers\ngnb.fit(X_train_dense, y_train)\nsvc.fit(X_train_tfidf, y_train)\n\n# Predictions\ny_pred_gnb = gnb.predict(X_test_dense)\ny_pred_svc = svc.predict(X_test_tfidf)\n\n# Evaluation\ndef evaluate_model(y_true, y_pred):\n    return {\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Recall\": recall_score(y_true, y_pred, average='weighted'),\n        \"Precision\": precision_score(y_true, y_pred, average='weighted'),\n        \"F1 Score\": f1_score(y_true, y_pred, average='weighted'),\n        \"Confusion Matrix\": confusion_matrix(y_true, y_pred)\n    }\n\nresults = {\n    \"GaussianNB\": evaluate_model(y_test, y_pred_gnb),\n    \"LinearSVC\": evaluate_model(y_test, y_pred_svc)\n}\n\nprint(results)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Suyash_Thapa.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd \nimport re \ndf = pd.read_csv('polarity.txt',sep=\"\\t\",header=None) \ndf = df.rename(columns={0:\"Text\", 1:\"Label\"}) \ndf = df.iloc[1:,:] \ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Suyash_Thapa.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code \ndf[\"Numerical_values\"] = df[\"Label\"].apply(lambda x: 1 if x == \"pos\" else 0) \ndf.drop([\"Label\"],axis=1,inplace=True) \ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Suyash_Thapa.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import csv\nfrom collections import Counter\nimport re\n\ndef create_count_and_probability(input_file, output_file):\n    # Read the input file\n    with open(input_file, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    # Initialize the CSV writer\n    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        fieldnames = ['Text', 'Count_Vector', 'Probability']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        # Process each line in the input file\n        for line in lines:\n            # Tokenize the words using regular expression\n            words_list = re.findall(r\"[\\w']+\", line.lower())\n\n            # Calculate word counts using Counter\n            word_counts = Counter(words_list)\n\n            # Create Count Vector and Probability lists\n            count_vector = [word_counts[word] for word in words_list]\n            total_words = len(words_list)\n            probability = [count / total_words for count in count_vector]\n\n            # Write the row to the CSV file\n            writer.writerow({\n                'Text': line.strip(),\n                'Count_Vector': str(count_vector),\n                'Probability': str(probability)\n            })\n\n# Example usage\ncreate_count_and_probability('corpus.txt', 'output.csv')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Suyash_Thapa.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code  \ndf_rural = pd.read_csv('rural.txt',sep=\"\\t\",header=None) \ndf_rural = df_rural.rename(columns={0:\"Document\"}) \ndf_rural = df_rural.iloc[1:,:] \ndf_rural[\"Class\"] = \"rural\" \ndf_rural\ndf_science = pd.read_csv('science.txt',sep=\"\\t\",header=None) \ndf_science = df_science.rename(columns={0:\"Document\"}) \ndf_science = df_science.iloc[1:,:] \ndf_science[\"Class\"] = \"science\" \ndf_science\nmerged_df = pd.concat([df_rural, df_science], ignore_index=True) \nmerged_df\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\n\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    merged_df['Document'], merged_df['Class'], test_size=0.3, random_state=42\n)\n\ntfidf_vectorizer = TfidfVectorizer()\ntrain_vectors = tfidf_vectorizer.fit_transform(train_data)\ntest_vectors = tfidf_vectorizer.transform(test_data)\nnb_classifier = GaussianNB()\nnb_classifier.fit(train_vectors.toarray(), train_labels)\nsvm_classifier = LinearSVC()\nsvm_classifier.fit(train_vectors, train_labels)\nnb_accuracy = nb_classifier.score(test_vectors.toarray(), test_labels)\nsvm_accuracy = svm_classifier.score(test_vectors, test_labels)\n\nprint(f\"Naive Bayes Classifier Accuracy: {nb_accuracy}\")\nprint(f\"Linear SVM Classifier Accuracy: {svm_accuracy}\")"
    },
    {
        "file_name": "Assignment2_Hossam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\nimport pandas as pd\n\nfile_path = 'polarity.txt'\n\n# Define column names\ncolumn_names = ['Text', 'Label']\n\n# Read the data into a DataFrame\ndf = pd.read_csv(file_path, sep='\\t', header=None, names=column_names)\n\n# Display the DataFrame\nprint(df)"
    },
    {
        "file_name": "Assignment2_Hossam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\n\nimport pandas as pd\n\n# Create a mapping dictionary for label conversion\nlabel_mapping = {'neg': 0, 'pos': 1}\n\n# Apply the mapping to create a new numerical label column\ndf['NumericalLabel'] = df['Label'].apply(lambda x: label_mapping.get(x, x))\n\n# Drop the original 'Label' column\ndf = df.drop(columns=['Label'])\n\n# Display the updated DataFrame\nprint(df)"
    },
    {
        "file_name": "Assignment2_Hossam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nimport pandas as pd\n\ndef create_count_and_probability(input_file):\n    # Read the text file\n    with open(input_file, 'r') as file:\n        lines = file.readlines()\n\n    # Create a list of words of all lines\n    words_list = set()\n    for line in lines:\n        # Tokenize the line using regular expression\n        line_words_list = re.findall(r\"[\\w']+\", line.lower())\n        # Update the set of words\n        words_list.update(line_words_list)\n    \n    \n    # Process each line and create a list of dictionaries\n    data = []\n    for line in lines:\n        # Tokenize the line using regular expression\n        line_words_list = re.findall(r\"[\\w']+\", line.lower())\n\n        # Count the occurrences of each word in word list\n        word_counts = {}\n        for word in words_list:\n            word_counts[word] = line_words_list.count(word)\n            \n        # Calculate total count for normalization\n        total_count = sum(word_counts.values())\n\n        # Calculate probability for each word\n        word_probabilities = {word: count / total_count for word, count in word_counts.items()}\n\n        # Append data to the list\n        data.append({\n            'Text': line.strip(),\n            'Count_Vector': str([word_counts[word] for word in words_list]),\n            'Probability': str([word_probabilities[word] for word in words_list])\n        })\n\n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv('output2.csv', index=False)\n\n# Example usage:\ncreate_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Assignment2_Hossam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n\n# Read the data from the files and create a DataFrame\ndef read_file(file_path, class_label):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n    data = {'Document': lines, 'Class': class_label}\n    df = pd.DataFrame(data)\n    return df\n\nrural_df = read_file('rural.txt', 'rural')\nscience_df = read_file('science.txt', 'science')\n\n# Concatenate the dataframes\ncombined_df = pd.concat([rural_df, science_df], ignore_index=True)\n\n# Split the data into train (70%) and test (30%) sets\nX_train, X_test, y_train, y_test = train_test_split(\n    combined_df['Document'], \n    combined_df['Class'], \n    test_size=0.3, \n    random_state=42\n)\n\n# TF-IDF Vectorizer\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Naive Bayes Classifier\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train_tfidf.toarray(), y_train)\nnb_predictions = nb_classifier.predict(X_test_tfidf.toarray())\n\n# SVM Classifier\nsvm_classifier = LinearSVC()\nsvm_classifier.fit(X_train_tfidf, y_train)\nsvm_predictions = svm_classifier.predict(X_test_tfidf)\n\n# Evaluation Metrics\ndef evaluate_classifier(predictions, classifier_name):\n    accuracy = metrics.accuracy_score(y_test, predictions)\n    recall = metrics.recall_score(y_test, predictions, pos_label='science')\n    precision = metrics.precision_score(y_test, predictions, pos_label='science')\n    f1 = metrics.f1_score(y_test, predictions, pos_label='science')\n    confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n\n    print(f\"Metrics for {classifier_name}:\")\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"F1 Score: {f1:.2f}\")\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix)\n    print(\"\\n\")\n\n# Evaluate Naive Bayes Classifier\nevaluate_classifier(nb_predictions, \"Naive Bayes\")\n\n# Evaluate SVM Classifier\nevaluate_classifier(svm_predictions, \"SVM\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Read the `polarity.txt` file into a dataframe\ndf = pd.read_csv('polarity.txt', delimiter='\\t', names=['Text', 'Label'])\n\n# Print the dataframe\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# create new column to keep numerical encodings of label\ndf['numerical_label'] = df['Label'].apply(lambda x: 1 if x=='pos' else 0)\n\n# drop previous label column\ndf.drop('Label', axis=1, inplace=True)\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nimport collections\nimport pandas as pd\n\ndef create_count_and_probability(file_name):\n    # Read the corpus file.\n    corpus = pd.read_csv('corpus.txt', sep='\\t', lineterminator='\\n', names=['documents'])\n    \n    # extract all words and convert them to lowercase in each document\n    corpus['documents_lower'] = corpus['documents'].apply(lambda x: x.lower())\n    corpus['words'] = corpus['documents_lower'].apply(lambda x: re.findall(r\"[\\w']+\", x))\n    corpus['word_counts'] = corpus['words'].apply(lambda x: collections.Counter(x))\n    \n    # find all possible words in the corpus\n    all_words = list(set(corpus['words'].sum()))\n    \n    # inner function to find probability of each word in a document given its count map\n    def find_probabilities(word_count_map):\n        word_count = sum(word_count_map.values())\n        word_probabilities = [ x/word_count for x in list(word_count_map.values())]\n        return word_probabilities\n    \n    # inner function that mimics count vectorizer\n    def count_vectorizer(document, all_words):\n        count_vector = []\n        for word in all_words:\n            count_vector.append(document.count(word))\n        return count_vector\n    \n    # create a column to keep count vectors for each document\n    corpus['count_vector'] = corpus['words'].apply(lambda x: count_vectorizer(x, all_words))\n    \n    # create a column to keep probabilities of each word in the document\n    corpus['probabilities'] = corpus['word_counts'].apply(lambda x: find_probabilities(x))\n    \n    # drop words column as it is not required in the csv file\n    corpus.drop(['words', 'word_counts', 'documents_lower'], axis=1, inplace=True)\n    \n    # save csv file \n    corpus.to_csv('corpus_count_and_probability.csv', index=False)\n\n# Call the function to create the CSV file\ncreate_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import naive_bayes, svm\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n\n\n\n# Read the `rural.txt` and 'science.txt' datasets and combine them into one dataset\nrural_df = pd.read_csv('rural.txt', delimiter='\\t', names=['Document'])\nrural_df['Class'] = 'rural'\nscience_df = pd.read_csv('science.txt', delimiter='\\t', names=['Document'])\nscience_df['Class'] = 'science'\ndataset = pd.concat([rural_df, science_df])\n\n# split data into 70/30 train-test datasets\ntext_train, text_test, class_train, class_test = train_test_split(dataset['Document'], dataset['Class'],\n                                                                  test_size=0.30,\n                                                                  random_state=1234, shuffle=True)\n\n# Vectorize both train and test datasets\ntf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),)\nX = tf_idf_vectorizer.fit_transform(text_train)\nX_test = tf_idf_vectorizer.transform(text_test)\n\n# train and predict using SVM\nsvm_classifier = svm.LinearSVC(dual=True)\nsvm_classifier.fit(X, class_train)\nsvm_predictions = svm_classifier.predict(X_test)\n\n# train and predict using Naive Bayes\nnb_classifier = naive_bayes.GaussianNB()\nnb_classifier.fit(X.toarray(), class_train)\nnb_predictions = nb_classifier.predict(X_test.toarray())\n\ndef evaluate_models(predictions, true_values, model_name):\n    cm = confusion_matrix(predictions, true_values)\n    print(f\"\"\"Model: {model_name}\n        Accuracy: {accuracy_score(true_values, predictions)}\n        Precision: {precision_score(true_values, predictions, pos_label='science')}\n        Recall: {recall_score(true_values, predictions, pos_label='science')}\n        F1: {f1_score(true_values, predictions, pos_label='science')}\n        Confusion Matrix:\n        'True Negatives': {cm[0, 0]}\n        'False Positives': {cm[0, 1]}\n        'False Negatives': {cm[1, 0]}\n        'True Positives': {cm[1, 1]}\"\"\")\n\nevaluate_models(svm_predictions, class_test, 'SVM')\nevaluate_models(nb_predictions, class_test, 'Naive Bayes')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# We just need to read the csv file and specify a custom delimeter for pandas to parse the file correctly\npolarity = pd.read_csv(\"polarity.txt\", delimiter='\t', names=[\"Text\", \"Label\"])\npolarity.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "polarity[\"Label_int\"] = polarity[\"Label\"].apply(lambda x: 0 if x==\"neg\" else 1)\npolarity.drop(\"Label\", axis=1, inplace=True)\npolarity.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "from collections import Counter\nimport re\n\n\ndef word_seperator(string):\n    # This function returns a list of all words in a string as a list\n    words_list = re.findall(r\"[\\w']+\", string)\n    \n    return words_list\n\ndef create_count_and_probability(file_name):\n    # We need all the unique words in the document to construct the word vectors because we are not aware of the missing words\n    # just by getting a single documents\n    with open(file_name, 'r') as f:\n        word_list = [word.lower() for word in word_seperator(f.read())]\n        unique_words = sorted(list(set(word_list)))\n\n    # The results are stored in this dictionary as we can easily convert it into a dataframe object\n    Result = {\n        \"Document\": [],\n        \"Word_Vector\": [],\n        \"Probability\": []\n    }\n\n    # We will need this dataframe as a template to sort word vectors and fill in the missing words as 0\n    count_vectors = pd.DataFrame(columns=unique_words)\n\n    with open(\"corpus.txt\", 'r') as f:\n        for line in f.readlines():\n            word_list = [word.lower() for word in word_seperator(line)]\n            word_freq = Counter(word_list)\n\n            # Calculating the probability\n            word_num = len(word_list)\n            word_prob = [f\"{word_freq[word]}/{word_num}\" for word in word_list]\n\n            # Constructing the word vector\n            word_freq = pd.Series(word_freq, dtype=\"int64\")\n            \n            # We merge the word frequencies to the template to put them in the same order and make room for missing words in document\n            merged_vector = pd.concat([count_vectors, word_freq.to_frame().T])\n            \n            # The missing values get a value of 0\n            merged_vector = merged_vector.fillna(0)\n            word_vector = merged_vector.iloc[0, :].to_list()\n\n            Result[\"Document\"].append(line.strip('\\n'))\n            Result[\"Word_Vector\"].append(str(word_vector))\n            Result[\"Probability\"].append(str(word_prob))\n\n    Result = pd.DataFrame(Result)\n    \n    return(Result.to_csv())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "random_seed = 11\n\n# we construct the dataset with documents and the class labels\nwith open(\"rural.txt\", 'r') as f:\n    stripped_lines = [line.strip('\\n') for line in f.readlines()]\n    rural = pd.DataFrame({\"Document\": stripped_lines, \"Class\": \"rural\"})\n    \nwith open(\"science.txt\", 'r') as f:\n    stripped_lines = [line.strip('\\n') for line in f.readlines()]\n    science = pd.DataFrame({\"Document\": stripped_lines, \"Class\": \"science\"})\ndocs = pd.concat([rural, science]).reset_index(drop=True)\ndocs\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nvectorizer = TfidfVectorizer()\n# We use the dense reprentation of the vectorized documents as out ttraining data\nX = vectorizer.fit_transform(docs[\"Document\"]).toarray()\ny = docs[\"Class\"].apply(lambda x: 1 if x==\"science\" else 0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\n# We fit a naive bayes classifier with default hyper parameters and evaluate its performance\nnb_clf = GaussianNB()\nnb_clf.fit(X_train, y_train)\nnb_pred = nb_clf.predict(X_test)\nprint(\"Metrics for Naive Bayes Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, nb_pred))\nprint(\"Recall:\", recall_score(y_test, nb_pred))\nprint(\"Precision:\", precision_score(y_test, nb_pred))\nprint(\"F1_score:\", f1_score(y_test, nb_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_pred))\n# We fit a support vector machine classifier with default hyper parameters and evaluate its performance\nsvm_clf = LinearSVC()\nsvm_clf.fit(X_train, y_train)\nsvm_pred = svm_clf.predict(X_test)\nprint(\"Metrics for Support Vector Machine Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, svm_pred))\nprint(\"Recall:\", recall_score(y_test, svm_pred))\nprint(\"Precision:\", precision_score(y_test, svm_pred))\nprint(\"F1_score:\", f1_score(y_test, svm_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, svm_pred))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "data = pd.read_csv(\"polarity.txt\", sep=\"\\t\", header=None, names=[\"Text\", \"Label\"])"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "def text_to_int(string):\n    if(string == \"neg\"):\n        return 1\n    return 0\ndata[\"Polarity\"] = data[\"Label\"].apply(text_to_int)\ndata.drop(\"Label\", inplace = True, axis = 1)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def get_lines_and_counter(file_name):\n    counter = Counter()\n    lines = []\n    with open(file_name, 'r', encoding='utf-8') as file:\n        all_words = set()\n        for line in file:\n            line = line.splitlines()[0]\n            lines.append(line)\n            words = re.findall(r\"[\\w']+\", line)\n            words = list(map(lambda x: x.lower(), words))\n            counter.update(Counter(words))\n    return lines, counter\n\ndef create_count_and_probability(file_name):\n    lines, counter = get_lines_and_counter(file_name)\n    data = []\n    frequency_list = [value[0] for value in counter.most_common()]\n    for i, line in enumerate(lines):\n        \n        words = re.findall(r\"[\\w']+\", line)\n        words = list(map(lambda x: x.lower(), words))\n        \n        c = Counter(words)\n        count = []\n        for word in frequency_list:\n            count.append(c[word])\n            \n        total = len(words)\n        probab = []\n        for word in words:\n            probab.append(str(c[word.lower()]) + \"/\" + str(total))\n        \n        data.append([line, count, probab])\n    \n    csv_file = \"csv_file.csv\"\n    \n    df = pd.DataFrame(data)\n    ## Note: this file is readable back to a dataframe via read_csv but the separator has to be tab.\n    ## For a space separated file, we get quotation marks around strings as the spaces in the string\n    ##    will be counted as delimiters if the quotations are removed.\n    ## If the file was not intended to be readable back to a dataframe, then a simple string concatination\n    ##    for the entire row would have worked.\n    ## Since this info is not provided, I'm assuming the file must be readable back to a 3 column dataframe\n    df.to_csv(csv_file, header=None, index=False, sep=\"\\t\", quoting=csv.QUOTE_NONE, escapechar=\" \")\n    \n    return csv_file\nfile_name = create_count_and_probability(\"corpus.txt\")\ndf = pd.read_csv(file_name, header=None, sep=\"\\t\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "rural = pd.read_csv(\"rural.txt\", sep=\"\\t\", header=None, names=[\"Text\"])\nscience = pd.read_csv(\"science.txt\", sep=\"\\t\", header=None, names=[\"Text\"])\nrural[\"class\"] = 1\nscience[\"class\"] = 0\ndata = pd.concat([rural, science], ignore_index=True, sort=False)\ntext_train, text_test, label_train, label_test = train_test_split(data[\"Text\"], data[\"class\"], test_size=0.30, random_state=1234, shuffle=True)\n\n# print(Counter(label_train))\n# print(Counter(label_test))\ncount_vectoriser = CountVectorizer()\ncount_vector = count_vectoriser.fit_transform(text_train)\ntest_count_vector = count_vectoriser.transform(text_test)\n\ntf_idf_vectorizer = TfidfVectorizer()\ntf_idf_vector = tf_idf_vectorizer.fit_transform(text_train)\ntest_tf_idf_vector = tf_idf_vectorizer.transform(text_test)\n# classification using CountVectoriser\nsvm_classifier = svm.LinearSVC()\n\nsvm_classifier.fit(count_vector, label_train)\nsvm_count_label = svm_classifier.predict(test_count_vector)\n\nnbg = naive_bayes.GaussianNB()\n\nnbg.fit(count_vector.toarray(), label_train)\nnbg_count_label = nbg.predict(test_count_vector.toarray())\nprint(\"Evaluation Metrics for CountVectoriser + SVM\")\nprint(\"Accuracy:  \", accuracy(label_test, svm_count_label))\nprint(\"F1 Score: \", f1(label_test, svm_count_label))\nprint(\"Precision: \", precision(label_test, svm_count_label))\nprint(\"Recall: \", recall(label_test, svm_count_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, svm_count_label))\nprint(\"Evaluation Metrics for CountVectoriser + Gaussian Naive Bayes\")\nprint(\"Accuracy:  \", accuracy(label_test, nbg_count_label))\nprint(\"F1 Score: \", f1(label_test, nbg_count_label))\nprint(\"Precision: \", precision(label_test, nbg_count_label))\nprint(\"Recall: \", recall(label_test, nbg_count_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, nbg_count_label))\n# classification using tf-Idf Vectoriser\nsvm_classifier = svm.LinearSVC()\n\nsvm_classifier.fit(tf_idf_vector, label_train)\nsvm_tfidf_label = svm_classifier.predict(test_tf_idf_vector)\n\nnbg = naive_bayes.GaussianNB()\n\nnbg.fit(tf_idf_vector.toarray(), label_train)\nnbg_tfidf_label = nbg.predict(test_tf_idf_vector.toarray())\nprint(\"Evaluation Metrics for tf-Idf Vectoriser + SVM\")\nprint(\"Accuracy:  \", accuracy(label_test, svm_tfidf_label))\nprint(\"F1 Score: \", f1(label_test, svm_tfidf_label))\nprint(\"Precision: \", precision(label_test, svm_tfidf_label))\nprint(\"Recall: \", recall(label_test, svm_tfidf_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, svm_tfidf_label))\nprint(\"Evaluation Metrics for tf-Idf Vectoriser + Gaussian Naive Bayes\")\nprint(\"Accuracy:  \", accuracy(label_test, nbg_tfidf_label))\nprint(\"F1 Score: \", f1(label_test, nbg_tfidf_label))\nprint(\"Precision: \", precision(label_test, nbg_tfidf_label))\nprint(\"Recall: \", recall(label_test, nbg_tfidf_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, nbg_tfidf_label))\n# CountVectoriser with uni-grams and bi-grams\n\ncount_vectoriser = CountVectorizer(ngram_range=(1,2))\ncount_vector = count_vectoriser.fit_transform(text_train)\ntest_count_vector = count_vectoriser.transform(text_test)\n\n# classification using CountVectoriser\nsvm_classifier = svm.LinearSVC()\n\nsvm_classifier.fit(count_vector, label_train)\nsvm_count_label = svm_classifier.predict(test_count_vector)\n\nnbg = naive_bayes.GaussianNB()\n\nnbg.fit(count_vector.toarray(), label_train)\nnbg_count_label = nbg.predict(test_count_vector.toarray())\nprint(\"Evaluation Metrics for uni and bi gram CountVectoriser + SVM\")\nprint(\"Accuracy:  \", accuracy(label_test, svm_count_label))\nprint(\"F1 Score: \", f1(label_test, svm_count_label))\nprint(\"Precision: \", precision(label_test, svm_count_label))\nprint(\"Recall: \", recall(label_test, svm_count_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, svm_count_label))\nprint(\"Evaluation Metrics for uni and bi gram CountVectoriser + Gaussian Naive Bayes\")\nprint(\"Accuracy:  \", accuracy(label_test, nbg_count_label))\nprint(\"F1 Score: \", f1(label_test, nbg_count_label))\nprint(\"Precision: \", precision(label_test, nbg_count_label))\nprint(\"Recall: \", recall(label_test, nbg_count_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, nbg_count_label))\n# CountVectoriser with no stopwords\n\ncount_vectoriser = CountVectorizer(stop_words='english')\ncount_vector = count_vectoriser.fit_transform(text_train)\ntest_count_vector = count_vectoriser.transform(text_test)\n\n# classification using CountVectoriser\nsvm_classifier = svm.LinearSVC()\n\nsvm_classifier.fit(count_vector, label_train)\nsvm_count_label = svm_classifier.predict(test_count_vector)\n\nnbg = naive_bayes.GaussianNB()\n\nnbg.fit(count_vector.toarray(), label_train)\nnbg_count_label = nbg.predict(test_count_vector.toarray())\nprint(\"Evaluation Metrics for stopwords CountVectoriser + SVM\")\nprint(\"Accuracy:  \", accuracy(label_test, svm_count_label))\nprint(\"F1 Score: \", f1(label_test, svm_count_label))\nprint(\"Precision: \", precision(label_test, svm_count_label))\nprint(\"Recall: \", recall(label_test, svm_count_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, svm_count_label))\nprint(\"Evaluation Metrics for stopwords CountVectoriser + Gaussian Naive Bayes\")\nprint(\"Accuracy:  \", accuracy(label_test, nbg_count_label))\nprint(\"F1 Score: \", f1(label_test, nbg_count_label))\nprint(\"Precision: \", precision(label_test, nbg_count_label))\nprint(\"Recall: \", recall(label_test, nbg_count_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, nbg_count_label))\n# tf-idf with no stopwords\n\ntf_idf_vectorizer = TfidfVectorizer(stop_words='english')\ntf_idf_vector = tf_idf_vectorizer.fit_transform(text_train)\ntest_tf_idf_vector = tf_idf_vectorizer.transform(text_test)\nsvm_classifier = svm.LinearSVC()\n\nsvm_classifier.fit(tf_idf_vector, label_train)\nsvm_tfidf_label = svm_classifier.predict(test_tf_idf_vector)\n\nnbg = naive_bayes.GaussianNB()\n\nnbg.fit(tf_idf_vector.toarray(), label_train)\nnbg_tfidf_label = nbg.predict(test_tf_idf_vector.toarray())\nprint(\"Evaluation Metrics for stopwrods removed tf-Idf Vectoriser + SVM\")\nprint(\"Accuracy:  \", accuracy(label_test, svm_tfidf_label))\nprint(\"F1 Score: \", f1(label_test, svm_tfidf_label))\nprint(\"Precision: \", precision(label_test, svm_tfidf_label))\nprint(\"Recall: \", recall(label_test, svm_tfidf_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, svm_tfidf_label))\nprint(\"Evaluation Metrics for stopwrods removed tf-Idf Vectoriser + Gaussian Naive Bayes\")\nprint(\"Accuracy:  \", accuracy(label_test, nbg_tfidf_label))\nprint(\"F1 Score: \", f1(label_test, nbg_tfidf_label))\nprint(\"Precision: \", precision(label_test, nbg_tfidf_label))\nprint(\"Recall: \", recall(label_test, nbg_tfidf_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, nbg_tfidf_label))\n# tf-idf with uni and bi grams\n\ntf_idf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\ntf_idf_vector = tf_idf_vectorizer.fit_transform(text_train)\ntest_tf_idf_vector = tf_idf_vectorizer.transform(text_test)\nsvm_classifier = svm.LinearSVC()\n\nsvm_classifier.fit(tf_idf_vector, label_train)\nsvm_tfidf_label = svm_classifier.predict(test_tf_idf_vector)\n\nnbg = naive_bayes.GaussianNB()\n\nnbg.fit(tf_idf_vector.toarray(), label_train)\nnbg_tfidf_label = nbg.predict(test_tf_idf_vector.toarray())\nprint(\"Evaluation Metrics for stopwrods removed tf-Idf Vectoriser + SVM\")\nprint(\"Accuracy:  \", accuracy(label_test, svm_tfidf_label))\nprint(\"F1 Score: \", f1(label_test, svm_tfidf_label))\nprint(\"Precision: \", precision(label_test, svm_tfidf_label))\nprint(\"Recall: \", recall(label_test, svm_tfidf_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, svm_tfidf_label))\nprint(\"Evaluation Metrics for stopwrods removed tf-Idf Vectoriser + Gaussian Naive Bayes\")\nprint(\"Accuracy:  \", accuracy(label_test, nbg_tfidf_label))\nprint(\"F1 Score: \", f1(label_test, nbg_tfidf_label))\nprint(\"Precision: \", precision(label_test, nbg_tfidf_label))\nprint(\"Recall: \", recall(label_test, nbg_tfidf_label))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(label_test, nbg_tfidf_label))"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\nfrom IPython.display import display\n\n# Read the data.\ndf = pd.read_csv(\"polarity.txt\", sep=\"\\t\", header=None)\n# Print head of the data.\ndisplay(df.head())\n# Rename columns.\ndf.columns = [\"text\", \"label\"]\n# Verify that the classes are only 'pos' and 'neg'.\ndisplay(df[\"label\"].unique())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Create new column converting 'pos' to 1 and 'neg' to 0.\ndf[\"class\"] = df[\"label\"].apply(lambda x: 0 if x == \"neg\" else 1)\n# Check head\ndisplay(df.head())\n# Verify uniqueness of class\ndisplay(df[\"class\"].unique())\n# Drop original column\nnew = df.drop(\"label\", axis=1)\nnew"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ishfaq_Herbrik.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\n\n\ndef create_count_and_probability(file_name):\n    # Read file.\n    with open(\"corpus.txt\", \"r\") as f:\n        raw = f.readlines()\n    # Convert the whole text to lowercase.\n    data = list(map(lambda x: x.lower(), raw))\n    # Get all words by line.\n    words_by_line = list(map(lambda x: re.findall(r\"[\\w]+\", x), data))\n    print(words_by_line)\n    # Create a set of all words.\n    all_words = list(set([word for line in words_by_line for word in line]))\n    all_words = {element: index for index, element in enumerate(all_words)}\n    print(all_words)\n    # Create count vector per line\n    count_vectors = [\n        [0 for _ in range(len(all_words))] for _ in range(len(words_by_line))\n    ]\n    for i, line in enumerate(words_by_line):\n        for word in line:\n            index = all_words[word]\n            count_vectors[i][index] += 1\n    # Create probability vector per line\n    probability_vectors = [[all_words[word] for word in line] for line in words_by_line]\n    # Query count vectors for number of times a word appears in a line.\n    probability_vectors = [\n        [count_vectors[i][index] for index in line]\n        for i, line in enumerate(probability_vectors)\n    ]\n    # Divide each entry of each vectors by the length of the vector.\n    probability_vectors = [\n        [count / len(line) for count in line] for line in probability_vectors\n    ]\n    # Return csv containing three columns: the raw lines, the count vectors, and the probability vectors.\n    df = pd.DataFrame(\n        {\"raw\": raw, \"count\": count_vectors, \"probability\": probability_vectors}\n    )\n    df.to_csv(\"corpus.csv\")\n    return \"corpus.csv\"\n\n\ncreate_count_and_probability(\"corpus.txt\")\n# Verify that csv can be read.\ndf = pd.read_csv(\"corpus.csv\")\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Ishfaq_Herbrik.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import (\n    accuracy_score,\n    recall_score,\n    precision_score,\n    f1_score,\n    confusion_matrix,\n)\n\n# Load the contents of the files\nfile_rural_path = \"rural.txt\"\nfile_science_path = \"science.txt\"\nclasses = [\"rural\", \"science\"]\n\nwith open(file_rural_path, \"r\") as file_rural, open(\n    file_science_path, \"r\"\n) as file_science:\n    rural_docs = file_rural.readlines()\n    science_docs = file_science.readlines()\n\n# Create the dataframe\n# Each document is labeled as either rural or science\nrural_data = [(doc.strip(), \"rural\") for doc in rural_docs if doc.strip()]\nscience_data = [(doc.strip(), \"science\") for doc in science_docs if doc.strip()]\ncombined_data = rural_data + science_data\ndf = pd.DataFrame(combined_data, columns=[\"Document\", \"Class\"])\n\n# Split the data into training and testing sets\n# The test size is 30% of the total data\n# The random state is set to 13 for reproducibility\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    df[\"Document\"], df[\"Class\"], test_size=0.3, random_state=13\n)\n\n# Vectorize the text data, this converts the text into numerical features\nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(train_data)\ntest_vectors = vectorizer.transform(test_data)\n\n# Train classifiers\ngnb = GaussianNB()\ngnb.fit(train_vectors.toarray(), train_labels)\n\nsvc = LinearSVC(random_state=13)\nsvc.fit(train_vectors, train_labels)\n\n# Predict with both classifiers, using the test vectors as input\ngnb_predictions = gnb.predict(test_vectors.toarray())\nsvc_predictions = svc.predict(test_vectors)\n\n\n# Evaluation function to calculate the performance metrics for a classifier\ndef evaluate_classifier(predictions, labels):\n    accuracy = accuracy_score(labels, predictions)\n    recall = recall_score(labels, predictions, pos_label=\"rural\")\n    precision = precision_score(labels, predictions, pos_label=\"rural\")\n    f1 = f1_score(labels, predictions, pos_label=\"rural\")\n    confusion = confusion_matrix(labels, predictions)\n    return accuracy, recall, precision, f1, confusion\n\n\n# Evaluate both classifiers\ngnb_scores = evaluate_classifier(gnb_predictions, test_labels)\nsvc_scores = evaluate_classifier(svc_predictions, test_labels)\n\n\ndef print_confusion_matrix(cm, classes):\n    # Formatting the output as a table using pandas\n    cm_table = pd.DataFrame(cm, index=classes, columns=classes)\n    print(cm_table.to_string())\n\n\n# Print the results\nprint(\"GaussianNB:\")\nprint(\"Accuracy: \", gnb_scores[0])\nprint(\"Recall: \", gnb_scores[1])\nprint(\"Precision: \", gnb_scores[2])\nprint(\"F1 Score: \", gnb_scores[3])\nprint(\"Confusion Matrix:\")\nprint_confusion_matrix(gnb_scores[4], classes)\n\n\nprint(\"\\nLinearSVC:\")\nprint(\"Accuracy: \", svc_scores[0])\nprint(\"Recall: \", svc_scores[1])\nprint(\"Precision: \", svc_scores[2])\nprint(\"F1 Score: \", svc_scores[3])\nprint(\"Confusion Matrix:\")\nprint_confusion_matrix(svc_scores[4], classes)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(cm, classes, classifier_name):\n    df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n\n    # Plotting\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(df_cm, annot=True, fmt=\"g\", cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix for {classifier_name}\")\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.show()\n\n\n# Plotting confusion matrix for both classifiers using a heatmap with seaborn\nplot_confusion_matrix(gnb_scores[4], classes=classes, classifier_name=\"GaussianNB\")\nplot_confusion_matrix(svc_scores[4], classes=classes, classifier_name=\"LinearSVC\")\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Suraj_Giri_Vu_Duc_Manh.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "# create list of data\ndata = list()\n\nwith open(\"./polarity.txt\", \"r\") as f:\n  lines = f.readlines()\n  for line in lines:\n    line = line[:-1]\n    v = line.split(sep=\" \\t\")\n    data.append(v)\n\ndf = pd.DataFrame(data,\n               columns =['Text', 'Label'])\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Suraj_Giri_Vu_Duc_Manh.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# creating mapping from Label to binary values\nmap_f = {\"pos\":1, \"neg\":0}\ndf[\"Class\"] = df[\"Label\"].map(map_f)\nprint(df)\n# dropping Label column\ndf = df.drop(columns=\"Label\")\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Suraj_Giri_Vu_Duc_Manh.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def create_count_and_probability(file_name):\n  # creating dataset\n  data = list()\n\n  # opening file for the first time to make total token list\n  with open(file_name, \"r\") as f:\n    words = re.findall(r'\\w+', f.read().lower())\n    word_counter = collections.Counter(words)\n    token_list = list(word_counter.keys())\n\n  # calculating freq and count vector for every line of data\n  with open(file_name, \"r\") as f:\n    lines = f.readlines()\n    for line in lines:\n      line = line.rstrip('\\n')\n      word_list = re.findall(r\"[\\w']+\", line.lower())\n\n      n = len(word_list)\n\n      # creating counter and count to create count vector\n      counter = {k:0 for k in token_list}\n      for t in word_list:\n        counter[t] += 1\n\n      # mapping token to frequency\n      prob = list(map(lambda x: counter[x] / n, word_list))\n\n      # adding to the dataset\n      data.append([line, list(counter.values()), prob])\n\n  df = pd.DataFrame(data,\n               columns =['Text', 'Count_Vector', 'Probability'])\n\n  df.to_csv(\"./result_2.csv\", index=False, quoting=False)\n  return df\n# creating the dataframe\ndf = create_count_and_probability(\"./corpus.txt\")\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Suraj_Giri_Vu_Duc_Manh.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Creating one dataframe from files rural.txt and science.txt as given in the template above\nwith open (\"./rural.txt\", \"r\") as f:\n  rural = f.readlines()\n\nwith open (\"./science.txt\", \"r\") as f:\n  science = f.readlines()\n\n# Creating the dataframe\nrural_df = pd.DataFrame({\"Document\": rural, \"Class\": \"rural\"})\nscience_df = pd.DataFrame({\"Document\": science, \"Class\": \"science\"})\n\n# Concatenating the dataframes\ndataframe = pd.concat([rural_df, science_df], ignore_index=True)\n\nprint(dataframe)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\n# Splitting the dataset into train and test (70:30)\nx_train, x_test, y_train, y_test = train_test_split(dataframe[\"Document\"], dataframe[\"Class\"], test_size=0.3, random_state=42)\n# print(x_train, x_test, y_train, y_test)\n\n# initializing the TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n\n#changing the training and testing data into TF-IDF vectors\nx_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\nx_test_tfidf = tfidf_vectorizer.transform(x_test)\n\n# initializing the naive_bayes.GaussianNB()\ngnb = GaussianNB()\n# initializing the svm.LinearSVC()\nsvm = LinearSVC(C=1.0, max_iter=1000, tol=0.0001)\n\n# training the classifier\ngnb.fit(x_train_tfidf.toarray(), y_train)\nsvm.fit(x_train_tfidf.toarray(), y_train)\n\n# predicting the labels on the test set\ngnb_predict = gnb.predict(x_test_tfidf.toarray())\nsvm_predict = svm.predict(x_test_tfidf.toarray())\n\n# calculating the accuracy score, recall score, precision score, f1 score and confusion matrix\ngnb_accuracy = accuracy_score(y_test, gnb_predict)\ngnb_recall = recall_score(y_test, gnb_predict, pos_label=\"science\")\ngnb_precision = precision_score(y_test, gnb_predict, pos_label=\"science\")\ngnb_f1 = f1_score(y_test, gnb_predict, pos_label=\"science\")\ngnb_confusion = confusion_matrix(y_test, gnb_predict)\n\nsvm_accuracy = accuracy_score(y_test, svm_predict)\nsvm_recall = recall_score(y_test, svm_predict, pos_label=\"science\")\nsvm_precision = precision_score(y_test, svm_predict, pos_label=\"science\")\nsvm_f1 = f1_score(y_test, svm_predict, pos_label=\"science\")\nsvm_confusion = confusion_matrix(y_test, svm_predict)\n\nprint(\"Accuracy metrics for Gaussian Naive Bayes Classifier\")\nprint(\"Accuracy Score: \", gnb_accuracy)\nprint(\"Recall Score: \", gnb_recall)\nprint(\"Precision Score: \", gnb_precision)\nprint(\"F1 Score: \", gnb_f1)\nprint(\"Confusion Matrix: \\n\", gnb_confusion)\nprint(\"\\n\")\nprint(\"Accuracy metrics for SVM Classifier\")\nprint(\"Accuracy Score: \", svm_accuracy)\nprint(\"Recall Score: \", svm_recall)\nprint(\"Precision Score: \", svm_precision)\nprint(\"F1 Score: \", svm_f1)\nprint(\"Confusion Matrix: \\n\", svm_confusion)\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# Read the content of the polarity.txt file\nwith open('polarity.txt', 'r') as file:\n    data = file.readlines()\n\n# Separate the data into text and label\ntext = [line.strip().split('\\t')[0] for line in data]\nlabel = [line.strip().split('\\t')[1] for line in data]\n\n# Create a DataFrame\ndf = pd.DataFrame({'Text': text, 'Label': label})\n\n# Display the DataFrame\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "df['BinaryLabel'] = df['Label'].apply(lambda label: 1 if label == 'pos' else 0)\ndf = df.drop('Label', axis=1)\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\nfrom fractions import Fraction\nimport pandas as pd\n\ndef create_count_and_probability(input_file, output_file):\n    # Read the content of the input file\n    with open(input_file, 'r') as file:\n        lines = file.readlines()\n\n    # Create lists to store data\n    text_list = []\n    count_vector_list = []\n    probability_list = []\n\n    for line in lines:\n        # Use regular expression to extract words\n        words_list = re.findall(r\"[\\w']+\", line)\n\n        # Calculate word frequencies using Counter\n        word_counts = Counter(words_list)\n\n        # Calculate count vector and probability for the line\n        count_vector = [word_counts[word] for word in (words_list)]\n        count_unique_vector = [word_counts[word] for word in set(words_list)]\n        total_words = sum(count_unique_vector)\n        probability = [Fraction(count, total_words) for count in count_vector]\n\n        # Append data to lists\n        text_list.append(line.strip())\n        count_vector_list.append(count_vector)\n        probability_list.append(probability)\n\n    # Create a DataFrame with the required columns\n    df = pd.DataFrame({'Text': text_list, 'Count_Vector': count_vector_list, 'Probability': probability_list})\n\n    # Write the DataFrame to a CSV file\n    df.to_csv(output_file)\n\n# Example usage:\ncreate_count_and_probability('corpus.txt', 'output.csv')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nimport string\nfrom sklearn.utils import shuffle\n\ndef preprocess_document(document):\n    # Remove punctuations and special characters and convert to lower case\n    translator = str.maketrans('', '', string.punctuation)\n    return document.rstrip('\\n').lower().translate(translator)\n\n# Read sentences from 'rural.txt' and create a DataFrame\nwith open('rural.txt', 'r', encoding='utf-8') as file:\n    rural_sentences = file.readlines()\n\n# Preprocess rural sentences\nrural_sentences = [preprocess_document(sentence) for sentence in rural_sentences]\n\nrural_df = pd.DataFrame({'Document': rural_sentences, 'Class': 'rural'})\n\n# Read sentences from 'science.txt' and create a DataFrame\nwith open('science.txt', 'r', encoding='utf-8') as file:\n    science_sentences = file.readlines()\n\n# Preprocess science sentences\nscience_sentences = [preprocess_document(sentence) for sentence in science_sentences]\n\nscience_df = pd.DataFrame({'Document': science_sentences, 'Class': 'science'})\n\n# Concatenate the two DataFrames\ncombined_df = pd.concat([rural_df, science_df], ignore_index=True)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot the distribution of labels\nplt.figure(figsize=(8, 5))\nsns.countplot(x='Class', data=combined_df)\nplt.title('Distribution of Labels')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.show()\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n\n# Split the data into train (70%) and test (30%) sets and shuffle data evenly\ntrain_df, test_df = train_test_split(combined_df, test_size=0.3, random_state=42,stratify=combined_df['Class'])\n\n# Create the TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer()\n\n# Transform the training data\nX_train = tfidf_vectorizer.fit_transform(train_df['Document'])\ny_train = train_df['Class']\n\n# Transform the test data\nX_test = tfidf_vectorizer.transform(test_df['Document'])\ny_test = test_df['Class']\n\n# Train the Gaussian Naive Bayes classifier\n\nnb_classifier = GaussianNB(var_smoothing=1e-1)\nnb_classifier.fit(X_train.toarray(), y_train)\n\n# Predict on the test set\nnb_predictions = nb_classifier.predict(X_test.toarray())\n\n# Evaluate Gaussian Naive Bayes classifier\nnb_accuracy = accuracy_score(y_test, nb_predictions)\nnb_classification_report = classification_report(y_test, nb_predictions)\nnb_conf_matrix = confusion_matrix(y_test, nb_predictions)\n\nprint(\"Gaussian Naive Bayes Evaluation:\")\nprint(f'Accuracy: {nb_accuracy:.4f}')\nprint(\"Classification Report:\\n\", nb_classification_report)\nprint(\"Confusion Matrix:\\n\", nb_conf_matrix)\n\n# Train the Linear SVM classifier\nsvm_classifier = LinearSVC(loss='hinge',C=2)\nsvm_classifier.fit(X_train, y_train)\n\n# Predict on the test set\nsvm_predictions = svm_classifier.predict(X_test)\n\n# Evaluate Linear SVM classifier\nsvm_accuracy = accuracy_score(y_test, svm_predictions)\nsvm_classification_report = classification_report(y_test, svm_predictions)\nsvm_conf_matrix = confusion_matrix(y_test, svm_predictions)\n\nprint(\"\\nLinear SVM Evaluation:\")\nprint(f'Accuracy: {svm_accuracy:.4f}')\nprint(\"Classification Report:\\n\", svm_classification_report)\nprint(\"Confusion Matrix:\\n\", svm_conf_matrix)\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n#here comes your code\n\nfile_path = 'polarity.txt'\n\n#Column names\ncolumns = ['Text', 'Label']\n\ndf = pd.read_csv(file_path, sep='\\t', header=None, names=columns)\n\n# Display the DataFrame\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\ndef convert_label_to_numeric(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n    else:\n        return None  # You might want to handle other cases accordingly\n\n# Create a new column 'NumericLabel' using the apply() function\ndf['NumericLabel'] = df['Label'].apply(convert_label_to_numeric)\n\n# Drop the original 'Label' column\ndf = df.drop(columns=['Label'])\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def create_count_and_probability(file_name):\n    # here comes your code\n    return(csv_file)\nimport re\nfrom collections import Counter\nimport pandas as pd\n\ndef create_count_and_probability(input_file):\n    # Read the input file\n    with open(input_file, 'r') as file:\n        lines = file.readlines()\n\n    # Initialize lists to store data\n    text_list = []\n    count_vector_list = []\n    probability_list = []\n\n    # Process each line in the file\n    for line in lines:\n        # Extract words using regular expression\n        words_list = re.findall(r\"[\\w']+\", line)\n\n\n        # Calculate word count\n        word_count = {}\n        for word in words_list:\n            if word in word_count:\n                word_count[word] += 1\n            else:\n                word_count[word] = 1\n\n        # Create count vector and calculate probability\n        count_vector = [word_count[word] for word in words_list]\n        total_words = len(words_list)\n        probability = [count / total_words for count in count_vector]\n\n        # Append data to lists\n        text_list.append(line.strip())\n        count_vector_list.append(count_vector)\n        probability_list.append(probability)\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Text': text_list,\n        'Count_Vector': count_vector_list,\n        'Probability': probability_list\n    })\n\n    # Write DataFrame to a CSV file\n    df.to_csv('output.csv', index=False)\n\ncreate_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n\n# Load the rural.txt and science.txt files into DataFrames\ndef load_data(file_path, label):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n    data = {'Document': lines, 'Class': [label] * len(lines)}\n    return pd.DataFrame(data)\n\nrural_df = load_data('rural.txt', 'rural')\nscience_df = load_data('science.txt', 'science')\n\n# Concatenate the DataFrames\ndf = pd.concat([rural_df, science_df], ignore_index=True)\n\n# Split the data into train (70%) and test (30%) sets\nX_train, X_test, y_train, y_test = train_test_split(df['Document'], df['Class'], test_size=0.3, random_state=42)\n\n# Create a TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the training data\nX_train_tfidf = vectorizer.fit_transform(X_train)\n\n# Transform the test data\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Train the Gaussian Naive Bayes classifier\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train_tfidf.toarray(), y_train)\n\n# Train the Linear SVM classifier\nsvm_classifier = LinearSVC()\nsvm_classifier.fit(X_train_tfidf, y_train)\n\n# Evaluate the classifiers using the test set\ndef evaluate_classifier(classifier, X_test_tfidf, y_test):\n    y_pred = classifier.predict(X_test_tfidf)\n\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    precision = metrics.precision_score(y_test, y_pred, pos_label='science')\n    recall = metrics.recall_score(y_test, y_pred, pos_label='science')\n    f1 = metrics.f1_score(y_test, y_pred, pos_label='science')\n    confusion_mat = metrics.confusion_matrix(y_test, y_pred)\n\n    return accuracy, precision, recall, f1, confusion_mat\n\n# Evaluate Gaussian Naive Bayes classifier\nnb_accuracy, nb_precision, nb_recall, nb_f1, nb_confusion_mat = evaluate_classifier(nb_classifier, X_test_tfidf.toarray(), y_test)\n\n# Evaluate Linear SVM classifier\nsvm_accuracy, svm_precision, svm_recall, svm_f1, svm_confusion_mat = evaluate_classifier(svm_classifier, X_test_tfidf, y_test)\n\n# Display the results\nprint(\"Gaussian Naive Bayes Classifier:\")\nprint(f\"Accuracy: {nb_accuracy}\")\nprint(f\"Precision: {nb_precision}\")\nprint(f\"Recall: {nb_recall}\")\nprint(f\"F1 Score: {nb_f1}\")\nprint(\"Confusion Matrix:\")\nprint(nb_confusion_mat)\n\nprint(\"\\nLinear SVM Classifier:\")\nprint(f\"Accuracy: {svm_accuracy}\")\nprint(f\"Precision: {svm_precision}\")\nprint(f\"Recall: {svm_recall}\")\nprint(f\"F1 Score: {svm_f1}\")\nprint(\"Confusion Matrix:\")\nprint(svm_confusion_mat)\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_SimonWelz_JonBreid.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\nfile_name = \"polarity.txt\"\ncolumns = [\"Text\", \"Label\"]\n\ndf = pd.read_csv(file_name, sep=\"\\t\", header=None, names= columns)\n\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_SimonWelz_JonBreid.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# function to translate given string to numierical value \ndef label_to_numeric(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n    else:\n        return None\n\n# create new column with numerical values \ndf['NumericLabel'] = df['Label'].apply(label_to_numeric)\n\n# drop the original column\ndf.drop('Label', axis='columns')\n\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_SimonWelz_JonBreid.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\nimport numpy as np\n\ndef create_count_and_probability(file_name):\n    with open(file_name, 'r') as file:\n        lines = file.readlines()\n    \n    # create mapping of all destinct words in the file to a specific index (sorted alphabetically)\n    full_text = \"\"\n    for line in lines:\n        full_text = full_text + line.lower()\n        \n    full_word_list =  re.findall(r\"[\\w']+\", full_text)\n    full_word_list = list(set(full_word_list))\n    full_word_list.sort() # sort alphabetically\n    words = {}\n    for i in range(len(full_word_list)):\n        words[full_word_list[i]] = i\n    \n    # create dict that saves the text, count vector and probablity\n    data = {'Text': [], 'Count_Vector': [], 'Probability': []}\n    for line in lines:\n        words_list = re.findall(r\"[\\w']+\", line)\n        counter = Counter(words_list)\n        \n        loc_count_vector = list(counter.values())\n    \n        # calculate probability\n        total_words = len(words_list)\n        probability = [f'{count}/{total_words}' for count in loc_count_vector]\n        \n        # calculate count vector over full text\n        full_count_vector = np.zeros(len(full_word_list), dtype=np.int16)\n        for elem in counter.keys():\n            full_count_vector[words[elem.lower()]] = counter[elem]\n            \n        data['Text'].append(line.strip())\n        data['Count_Vector'].append(list(full_count_vector))\n        data['Probability'].append(probability)\n        \n    # create a dataframe from the dict and save it in a csv file\n    df = pd.DataFrame(data)\n    df.to_csv('out.csv')\n    \n    return(df)\n\n\nfile_name = \"corpus.txt\"\ndf = create_count_and_probability(file_name)\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_SimonWelz_JonBreid.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "def create_dataframe(file_name, label):\n    with open(file_name, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n    data = {'Document': lines, 'Class': [label] * len(lines)}\n    df = pd.DataFrame(data)\n    return df\n\nrural_df = create_dataframe('rural.txt', 'rural')\n\nscience_df = create_dataframe('science.txt', 'science')\n\ncombined_df = pd.concat([rural_df, science_df], ignore_index=True)\n\nprint(combined_df)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# split dataset\ntrain_data, test_data, train_labels, test_labels = train_test_split(combined_df['Document'], combined_df['Class'], test_size = 0.3, random_state=42)\n\nvectorizer = TfidfVectorizer(stop_words='english')\n\nX_train = vectorizer.fit_transform(train_data)\n\nX_test = vectorizer.transform(test_data)\n\n# fit the data to the classifier\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train.toarray(), train_labels)\n# Make some predictions\nnb_predictions = nb_classifier.predict(X_test.toarray())\n\nprint(\"Naive Bayes Classifier:\")\nprint(\"Accuracy:\", accuracy_score(test_labels, nb_predictions))\nprint(\"Classification Report:\")\nprint(classification_report(test_labels, nb_predictions))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(test_labels, nb_predictions))\n\n# Train Linear SVM classifier\nsvm_classifier = LinearSVC(dual=False)\nsvm_classifier.fit(X_train, train_labels)\n\n# Make predictions on the test set\nsvm_predictions = svm_classifier.predict(X_test)\n\nprint(\"\\nLinear SVM Classifier:\")\nprint(\"Accuracy:\", accuracy_score(test_labels, svm_predictions))\nprint(\"Classification Report:\")\nprint(classification_report(test_labels, svm_predictions))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(test_labels, svm_predictions))"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\n# We use the pandas read_csv file with tab separator to read the data\ndf_polarity = pd.read_csv('polarity.txt', sep='\\t', names=['text', 'label'])\n\nprint(df_polarity.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "import pandas as pd\n\n# The apply function gets a function that we used a lambda function to convert the labels to numerical values\ndf_polarity['numerical_label'] =  df_polarity['label'].apply(lambda x: 1 if x == 'pos' else 0)\nprint(df_polarity.head())\n\n# Here we drop the label column and rename the numerical_label column to label\ndf_polarity.drop('label', axis=1, inplace=True)\ndf_polarity.rename({'numerical_label': 'label'}, axis=1, inplace=True)\n\nprint(df_polarity.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "from collections import Counter\nimport re\n\ndef create_count_and_probability(file_name):\n    \"\"\"\n    This function takes a file name and return the count vector and probability of each word in the file.\n    \"\"\"\n    # we create an empty dataframe with the desired columns\n    result = pd.DataFrame(columns=['Text', 'Count_Vector', 'Probability'])\n\n    # Read the file using with open in order to close the file automatically\n    with open(file_name, 'r') as f:\n        # We read the file and convert all the words to lower case\n        corpus = f.read().lower()\n        # We use the regular expression to find all the words in the corpus\n        all_words = re.findall(r\"[\\w']+\", corpus)\n        # We use the Counter function to count the frequency of each word in order to find the features (unique words)\n        global_frequency = Counter(all_words)\n        features = list(global_frequency.keys())\n        # We sort the features in order to have a consistent order\n        features.sort()\n        # In each iteration we generate the count vector and probability of each word in the corpus\n        for line in corpus.splitlines():\n            words = re.findall(r\"[\\w']+\", line)\n            # using counter method to have the frequency of each word in the line\n            words_frequency = Counter(words)\n            row = {}\n            row['Text'] = line\n            row['Count_Vector'] = [words_frequency[feature] for feature in features]\n            row['Probability'] = [words_frequency[feature]/len(words) for feature in features]\n            # As the append function is deprecated, we use the concat function to add the row to the result dataframe\n            result = pd.concat([result, pd.DataFrame([row])], ignore_index=True)\n\n    return result.to_csv('count_and_probability.csv', index=False)\ncreate_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "from random import shuffle\n# a) parsing the input data and returning a dataframe with the related labels\ndef create_dataset(shuffle_data=True):\n    parsed_data = []\n\n    # Read the files and append the data to the parsed_data list with the related label\n    with open('rural.txt', 'r') as f:\n        for line in f:\n            parsed_data.append({'Document': line, 'Label': 'rural'})\n    with open('science.txt', 'r') as f:\n        for line in f:\n            parsed_data.append({'Document': line, 'Label': 'science'})\n\n    # Shuffle the data for easier data splitting, it's better to be moved in data_splitting function\n    if shuffle_data:\n        shuffle(parsed_data)\n        \n    return pd.DataFrame(parsed_data)\n# b) splitting the data into train and test sets with a desired ratio here for example 70% train and 30% test\ndef split_train_test_data(df, ratio=0.7):\n    train_length = int(len(df) * ratio)\n    return df[:train_length], df[train_length:]\n# Generating the dataset and splitting it into train and test sets\ndataset = create_dataset()\ntrain_data, test_data = split_train_test_data(dataset)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# Initializing the Tfidf vectorizer and the models\nvectorizer = TfidfVectorizer()\ngaussianNB_model = GaussianNB()\nlinearSVC_model = LinearSVC()\n\n# Fitting the vectorizer and the models\ntrain_vectorizer = vectorizer.fit_transform(train_data['Document'])\ngaussianNB_model.fit(train_vectorizer.toarray(), train_data['Label'])\nlinearSVC_model.fit(train_vectorizer.toarray(), train_data['Label'])\n# C) Models Evaluation Report\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# We have use the same vectorizer that we have used for the training data\ntest_vectorizer = vectorizer.transform(test_data['Document'])\n\n# Predicting the test data using the models\ngaussianNB_predictions = gaussianNB_model.predict(test_vectorizer.toarray())\nlinearSVC_predictions = linearSVC_model.predict(test_vectorizer.toarray())\n\n\n# Printing the evaluation report for the models using the sklearn classification_report and confusion_matrix\n\nprint('GaussianNB Evaluation:')\nprint(classification_report(test_data['Label'], gaussianNB_predictions))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_data['Label'], gaussianNB_predictions))\n\nprint('=======================================================')\n\nprint('LinearSVC Evaluation:')\nprint(classification_report(test_data['Label'], linearSVC_predictions))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_data['Label'], linearSVC_predictions))\n"
    },
    {
        "file_name": "Assignment_2_Ilhom_Khalimov.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\ndef read_polarity(filename: str) -> pd.DataFrame:\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        tagged: list[tuple[str, str]] = []\n        for line in lines:\n            words = line.split()\n            label = words[-1]\n            sentence = ' '.join(words[:-1])\n            tagged.append([sentence, label])\n\n        df = pd.DataFrame(tagged, columns=[\"Text\", \"Label\"])\n        return df\n\ndf = read_polarity(\"polarity.txt\")\nprint(df)"
    },
    {
        "file_name": "Assignment_2_Ilhom_Khalimov.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "def polarity_map(x: str) -> int | None:\n    MAP = {\"pos\": 1, \"neg\": 0}\n    return MAP.get(x, None)\n\ndf[\"Label\"] = df[\"Label\"].apply(polarity_map)\nprint(df)"
    },
    {
        "file_name": "Assignment_2_Ilhom_Khalimov.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "from collections import Counter\nimport re\n\ndef create_count_and_probability(file_name: str):\n    texts: list[str] = []\n    vectors: list[list[int]] = []\n    probs: list[str] = []\n    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n        vocab: list[str] = []\n        text_words: list[list[str]] = []\n        for line in f:\n            line = line.strip()\n            texts.append(line)\n            words_list: list[str] = re.findall(r\"[\\w']+\", line)\n\n            words_list = [word.lower() for word in words_list]\n            for word in words_list:\n                if word not in vocab:\n                    vocab.append(word)\n            text_words.append(words_list)\n\n            cnt = Counter(words_list)\n            dct = dict(cnt)\n            total = cnt.total()\n            prob: list[str] = []\n            for word in words_list:\n                prob.append(f\"{dct[word]}/{total}\")\n            probs.append(prob)\n        \n        for words in text_words:\n            cnt = Counter(words)\n            dct = dict(cnt)\n            vector: list[int] = []\n            for key in vocab:\n                vector.append(cnt.get(key, 0))\n            vectors.append(vector)\n        \n        data = {\"Text\": texts, \"Count_Vector\": vectors, \"Probability\": probs}\n        df = pd.DataFrame(data)\n            \n    # here comes your code\n    return(df.to_csv())\n\ncsv = create_count_and_probability(\"corpus.txt\")\nprint(csv)"
    },
    {
        "file_name": "Assignment_2_Ilhom_Khalimov.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "from sklearn import metrics\nfrom sklearn.calibration import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\n\n\n\ndef read(filename: str, classname: str) -> pd.DataFrame:\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        return pd.DataFrame({\"Document\": f.readlines(), \"Class\": classname})\n\nrural = read(\"rural.txt\", \"rural\")\nscience = read(\"science.txt\", \"science\")\n\ncombined = pd.concat([rural, science], ignore_index=True)\n\ntrain, test = train_test_split(combined, train_size=0.7)\n\nvectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\nfeatures_train = vectorizer.fit_transform(train[\"Document\"])\nfeatures_test = vectorizer.transform(test[\"Document\"])\n\n\ngauss = GaussianNB()\ngauss.fit(features_train.toarray(), train[\"Class\"])\ngauss_pred = gauss.predict(features_test.toarray())\n\nsvc = LinearSVC(dual=True)\nsvc.fit(features_train, train[\"Class\"])\nsvc_pred = svc.predict(features_test)\n\ndef print_scores(name: str, classifier) -> None:\n    print(f\"Classifier: {name}\")\n    print(f'Accuracy: {metrics.accuracy_score(test[\"Class\"], classifier)}')\n    print('Recall: {}'.format(metrics.recall_score(test[\"Class\"], classifier, average=\"weighted\")))\n    print('Precision: {}'.format(metrics.precision_score(test[\"Class\"], classifier, average=\"weighted\")))\n    print('F1 Score: {}'.format(metrics.f1_score(test[\"Class\"], classifier, average=\"weighted\")))\n    print('Confusion:')\n    print(metrics.confusion_matrix(test[\"Class\"], classifier))\n\nprint_scores(\"Gaussian\", gauss_pred)\nprint()\nprint_scores(\"SVM\", svc_pred)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#Import the relevant libraries\nimport pandas as pd\nimport re\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n# Define the file path\npath_of_file = 'polarity.txt'\n\n# Define the column names\ncolumn_names = ['Text', 'Label']\n\n#Create a Dataframe\npolarity_df = pd.read_csv(path_of_file, sep='\\t', header=None, names=column_names)\n\nprint(polarity_df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# Function that converts labels to numeric values\ndef numeric_values(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n\n# Create a new column 'Numeric Label'\npolarity_df['Numeric_label'] = polarity_df['Label'].apply(numeric_values)\n\n# Drop the original Label column with textual values\npolarity_df = polarity_df.drop(columns=['Label'])\n\n# Print the DataFrame\nprint(polarity_df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "# Define function to create count and probabilities\ndef create_count_and_probability(file):\n    \n    # Read the input file\n    with open(file, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    data = {'Text': [], 'Count_Vector': [], 'Probability': []}\n\n    for line in lines:\n        # Extract words from the line\n        words = re.findall(r\"[\\w']+\", line)\n\n        # Count the number of times each word appears\n        word_counts = Counter(words)\n\n        # Count the total number of words\n        total_words = sum(word_counts.values())\n\n        # Create count vector\n        count_vector = [word_counts[word] for word in words]\n\n        # Create probability vector\n        probability = [count / total_words for count in count_vector]\n\n        # Append data to the dictionary\n        data['Text'].append(line.strip())\n        data['Count_Vector'].append(count_vector)\n        data['Probability'].append(probability)\n\n    # Create a Dataframe\n    df = pd.DataFrame(data)\n\n    # Save DataFrame to a CSV file\n    df.to_csv(\"output_file.csv\", index=False)\n    return df\n\ndf = create_count_and_probability('corpus.txt')\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Function to create a dataframe as per requirements\ndef create_dataframe(file, label):\n    with open(file, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n\n    data = [line.strip() for line in lines]\n    df = pd.DataFrame({'Document': data, 'Class': label})\n    return df\n\nrural = create_dataframe('rural.txt', 'rural')\nscience = create_dataframe('science.txt', 'science')\n\n# Join the DataFrames\ndocument = pd.concat([rural, science], ignore_index=True)\n\n# Display the final DataFrame\nprint(document)\n# Split the Data into 30% Test and 70% Train\nX_train, X_test, y_train, y_test = train_test_split(document['Document'], document['Class'], test_size=0.3, random_state=42)\n\n# Initiate TFIDF Vectorizer\nTF_IDF = TfidfVectorizer(max_features=1000)\n\n# Transform training and testing data\nX_train_tfidf = TF_IDF.fit_transform(X_train)\nX_test_tfidf = TF_IDF.transform(X_test)\n\n# Train a Gaussian Naive Bayes model\nNB = GaussianNB()\nNB.fit(X_train_tfidf.toarray(), y_train)\n\n# Make predictions and assess the performance of the Gaussian Naive Bayes classifier\nNB_Prediction = NB.predict(X_test_tfidf.toarray())\nprint('\\nNaive Bayes Classifier Performance:')\nprint('Accuracy:', metrics.accuracy_score(y_test, NB_Prediction))\nprint('Precision:', metrics.precision_score(y_test, NB_Prediction, pos_label='science'))\nprint('Recall:', metrics.recall_score(y_test, NB_Prediction, pos_label='science'))\nprint('F1 Score:', metrics.f1_score(y_test, NB_Prediction, pos_label='science'))\nprint('\\nConfusion Matrix:')\nprint(metrics.confusion_matrix(y_test, NB_Prediction))\n\n# Train a Linear SVC classifier\nSVM = LinearSVC(dual=False)\nSVM.fit(X_train_tfidf, y_train)\n\n# Predict and evaluate LinearSVC classifier\nSVM_Prediction = SVM.predict(X_test_tfidf)\nprint('\\nSVM Classifier Performance:')\nprint('Accuracy:', metrics.accuracy_score(y_test, SVM_Prediction))\nprint('Precision:', metrics.precision_score(y_test, SVM_Prediction, pos_label='science'))\nprint('Recall:', metrics.recall_score(y_test, SVM_Prediction, pos_label='science'))\nprint('F1 Score:', metrics.f1_score(y_test, SVM_Prediction, pos_label='science'))\nprint('\\nConfusion Matrix:')\nprint(metrics.confusion_matrix(y_test, SVM_Prediction))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\nimport re\n\n## Read the polarity.txt file\npolarity_file = open('polarity.txt', 'r')\n\n## Initialize lists to store the values\nlabels = []\ntexts = []\n\nfor line in polarity_file.readlines():\n  curr_line = re.sub('[\\t\\n]', '', line) ## Remove unwanted characters\n  labels.append(curr_line[-3:]) ## Get the labels from the end of the line\n  texts.append(curr_line[:-3].strip()) ## Get text from the line\n\n## Create a pandas dataframe from the created lists\npolarity_df = pd.DataFrame(list(zip(texts, labels)), columns=[\"Text\", \"Label\"])\n\npolarity_df"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "polarity_df['Label'] = polarity_df['Label'].replace(['pos'], 1) ## Change pos to 1\npolarity_df['Label'] = polarity_df['Label'].replace(['neg'], 0) ## Change neg to 0\n\npolarity_df"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import csv\n\ndef create_count_and_probability(input_file):\n    ## Open and read the input file\n    corpus_file = open(input_file, 'r')\n    text = corpus_file.read()\n\n    ## Remove punctutation and convert to lowercase\n    clean_text = re.sub(r'[^\\w\\s]', '', text.lower())\n\n    ## Get all words\n    words = clean_text.split()\n\n    ## Create a dict of words and their counts as key-value pairs\n    word_counts = {}\n    for word in words:\n        word_counts[word] = word_counts.get(word, 0) + 1\n\n    ## Get total number of words for probability calculation\n    total_words = len(words)\n\n    ## Initailize list to store data (sentence, count_vector and probabilit)\n    data = []\n\n    # print(clean_text)\n    for sentence in text.split('.'):\n        ## Clean sentence to use for calculations\n        clean_sentence = re.sub(r'[^\\w\\s]', '', sentence.lower())\n\n        ## Get all words of the\n        sentence_words = clean_sentence.split()\n\n        ## Create a count vector for the current sentence\n        count_vector = [0] * len(word_counts)\n        for word in sentence_words:\n            count_vector[word_counts[word]] += 1\n\n        ## Calculate the probability vector for the current sentence\n        probability_vector = [count / total_words for count in count_vector]\n\n        ## Add the calculations to the data list\n        data.append((sentence, count_vector, probability_vector))\n\n    ## Write the data to a CSV file\n    with open('corpus_output.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Text', 'Count_Vector', 'Probability'])\n        for row in data:\n            writer.writerow(row)\n\ncreate_count_and_probability('corpus.txt')"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "## Read the rural.txt file and create a list for the lines\nrural_data = []\nwith open('rural.txt', 'r') as f:\n    for line in f:\n        rural_data.append((line, 'rural'))\n\n## Read the science.txt file and create a list for the lines\nscience_data = []\nwith open('science.txt', 'r') as f:\n    for line in f:\n        science_data.append((line, 'science'))\n\n## Create the dataframe from the 2 lists\ncomplete_df = pd.DataFrame(data=rural_data + science_data, columns=['Document', 'Class'])\n\"\"\"\n  This is part b)\n\"\"\"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\n\n## Split the dataframe 70/30 from train and test\nx_train, x_test, y_train, y_test = train_test_split(complete_df['Document'], complete_df['Class'], test_size=0.3, random_state=42)\n\nvectorizer = TfidfVectorizer()\n\n# Transform the train and test sets into TF-IDF vectors\nX_train_tfidf = vectorizer.fit_transform(x_train)\nX_train_tfidf_dense = X_train_tfidf.toarray()\n\nX_test_tfidf = vectorizer.transform(x_test)\nX_test_tfidf_dense = X_test_tfidf.toarray()\n\n## Train teh Gaussian Naive Bayes\ngaussian_nb = GaussianNB()\ngaussian_nb.fit(X_train_tfidf_dense, y_train)\n\n## Train the Linear SVM\nlinear_svm = LinearSVC()\nlinear_svm.fit(X_train_tfidf_dense, y_train)\n\"\"\"\n  This is part c)\n\"\"\"\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n## Evaluate the Naive Bayes classifier on the test set\nnb_predictions = gaussian_nb.predict(X_test_tfidf_dense)\nnb_accuracy = accuracy_score(y_test, nb_predictions)\nnb_precision = precision_score(y_test, nb_predictions, pos_label='science')\nnb_recall = recall_score(y_test, nb_predictions, pos_label='science')\nnb_f1 = f1_score(y_test, nb_predictions, pos_label='science')\nnb_confusion_matrix = confusion_matrix(y_test, nb_predictions)\n\nprint('Naive Bayes:')\nprint('Accuracy:', nb_accuracy)\nprint('Precision:', nb_precision)\nprint('Recall:', nb_recall)\nprint('F1:', nb_f1)\nprint('Confusion Matrix:')\nprint(nb_confusion_matrix)\n\n## Evaluate the SVM classifier on the test set\nsvm_predictions = linear_svm.predict(X_test_tfidf_dense)\nsvm_accuracy = accuracy_score(y_test, svm_predictions)\nsvm_precision = precision_score(y_test, svm_predictions, pos_label='science')\nsvm_recall = recall_score(y_test, svm_predictions, pos_label='science')\nsvm_f1 = f1_score(y_test, svm_predictions, pos_label='science')\nsvm_confusion_matrix = confusion_matrix(y_test, svm_predictions)\n\nprint('\\nSVM:')\nprint('Accuracy:', svm_accuracy)\nprint('Precision:', svm_precision)\nprint('Recall:', svm_recall)\nprint('F1:', svm_f1)\nprint('Confusion Matrix:')\nprint(svm_confusion_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Yagmur_Caglar.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\nimport pandas as pd\nwith open(file_path + \"polarity.txt\", 'r') as file:\n    lines = file.readlines()\n\ndf = pd.DataFrame([line.strip().split('\\t') for line in lines], columns=['Text', 'Label'])\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Yagmur_Caglar.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\ndef label_to_numeric(label):\n  if label == 'pos':\n    return 1\n  else:\n    return 0\n\ndf['Numeric'] = df['Label'].apply(label_to_numeric)\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Yagmur_Caglar.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nfrom collections import Counter\n\ndef create_count_and_probability(file_name):\n    # here comes your code\n    with open(file_name, 'r') as file:\n        lines = file.readlines()\n\n    texts = []\n    count_vectors = []\n    probabilities = []\n\n    for line in lines:\n        words = re.findall(r'\\b\\w+\\b', line.lower())  # convert to lowercase for case sensitivity\n        count_vector = list(Counter(words).values())\n        probability = [count / len(words) for count in count_vector]\n\n        texts.append(line.strip())\n        count_vectors.append(count_vector)\n        probabilities.append(probability)\n\n    df = pd.DataFrame({\n        'Text': texts,\n        'Count_Vector': count_vectors,\n        'Probability': probabilities\n    })\n\n    csv_file = df.to_csv(index=False)\n\n    return(csv_file)\nwith open(file_path + 'output.csv', 'w') as file:\n      file.write(create_count_and_probability(file_path + \"corpus.txt\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Yagmur_Caglar.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Here comes your code\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n\ndef load_data(file_path, class_label):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n    return pd.DataFrame({'Document': lines, 'Class': class_label})\n\nrural_data = load_data(file_path + 'rural.txt', 'rural')\nscience_data = load_data(file_path + 'science.txt', 'science')\ndf = pd.concat([rural_data, science_data], ignore_index=True)\nprint(df)\ndef evaluate_classifier(predictions, true_labels):\n    accuracy = metrics.accuracy_score(true_labels, predictions)\n    precision = metrics.precision_score(true_labels, predictions, average='weighted')\n    recall = metrics.recall_score(true_labels, predictions, average='weighted')\n    f1 = metrics.f1_score(true_labels, predictions, average='weighted')\n    confusion_matrix = metrics.confusion_matrix(true_labels, predictions)\n\n    return accuracy, precision, recall, f1, confusion_matrix\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\nvectorizer = TfidfVectorizer(stop_words='english', sublinear_tf=True, ngram_range=(1, 4), smooth_idf=True)\n#max_features yield worse performance\n\nX_train = vectorizer.fit_transform(train_df['Document'])\nX_test = vectorizer.transform(test_df['Document'])\n# Gaussian NB\nX_train_dense = X_train.toarray()\nX_test_dense = X_test.toarray()\n\ngaussian_nb = GaussianNB()\ngaussian_nb.fit(X_train_dense, train_df['Class'])\nnb_predictions = gaussian_nb.predict(X_test_dense)\naccuracy_nb, precision_nb, recall_nb, f1_nb, confusion_matrix_nb = evaluate_classifier(nb_predictions, test_df['Class'])\n\nprint(\"Gaussian Naive Bayes Classifier:\")\nprint(f\"Accuracy: {accuracy_nb}\")\nprint(f\"Precision: {precision_nb}\")\nprint(f\"Recall: {recall_nb}\")\nprint(f\"F1 Score: {f1_nb}\")\nprint(f\"Confusion Matrix:\\n{confusion_matrix_nb}\\n\")\n# SVC\nsvc = LinearSVC()\nsvc.fit(X_train, train_df['Class'])\nsvm_predictions = svc.predict(X_test)\naccuracy_svm, precision_svm, recall_svm, f1_svm, confusion_matrix_svm = evaluate_classifier(svm_predictions, test_df['Class'])\n\nprint(\"Linear Support Vector Classifier:\")\nprint(f\"Accuracy: {accuracy_svm}\")\nprint(f\"Precision: {precision_svm}\")\nprint(f\"Recall: {recall_svm}\")\nprint(f\"F1 Score: {f1_svm}\")\nprint(f\"Confusion Matrix:\\n{confusion_matrix_svm}\")\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Maria_Artemyeva.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\ndata = pd.read_csv('polarity.txt', sep='\\t', names=['Text', 'Label'], header=None)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Maria_Artemyeva.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "#pos=1, neg=0\ndata['Label']=data['Label'].apply(lambda x: 0 if x =='neg' else 1)"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Maria_Artemyeva.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import string\nimport re\nfrom collections import Counter\ndef create_count_and_probability(file_name):\n    word = open(file_name, \"r\")\n    wd = word.read()\n    word.close() \n    lines=wd.split('\\n') #get list of lines\n    csv_file = pd.DataFrame(lines,columns=[\"Text\"])#create dataframe with text\n    datalist = wd.lower().translate(str.maketrans('', '', string.punctuation))#lowercase and remove punctuation\n    unique=sorted(set(datalist.split()))#get unique words in alphabetical order\n    return(csv_file)\n\ncreate_count_and_probability(\"corpus.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Maria_Artemyeva.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "# Creating dataframe from 2 files\nfrom sklearn.utils import shuffle\ndocuments1 = pd.read_csv('rural.txt', sep='\\t', names=['Document'], header=None)\ndocuments1['Class']='rural'\ndocuments2 = pd.read_csv('science.txt', sep='\\t', names=['Document'], header=None)\ndocuments2['Class']='science'\ndocuments=shuffle(pd.concat([documents1, documents2], ignore_index=True, sort=False))\n"
    },
    {
        "file_name": "Assignment_2_Bondarenko_Nikita.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\ndf = pd.read_csv('polarity.txt', sep='\\t', header=None, names=['Text', 'Label'])"
    },
    {
        "file_name": "Assignment_2_Bondarenko_Nikita.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "import pandas as pd\n\ndef map_label_to_num(label):\n    if label == 'pos':\n        return 1\n    elif label == 'neg':\n        return 0\n    else:\n        return -1\n    \ndf['Label_num'] = df['Label'].apply(map_label_to_num)\n\ndf = df.drop('Label', axis=1)"
    },
    {
        "file_name": "Assignment_2_Bondarenko_Nikita.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import re\nimport csv\nfrom collections import Counter\nfrom fractions import Fraction\ndef create_count_and_probability(file_name):\n    \n    # Read the entire file to determine the unique set of words\n    with open(file_name, 'r') as file:\n        full_text = file.read().lower()\n        unique_words = sorted(set(re.findall(r\"[\\w']+\", full_text)))\n\n    # Process each line to generate count vectors and probabilities\n    with open(file_name, 'r') as file, open(\"output.csv\", 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Text', 'Count_Vector', 'Probability'])\n\n        for line in file:\n            words_list = re.findall(r\"[\\w']+\", line.lower())\n            count = Counter(words_list)\n            total_words = sum(count.values())\n\n            # generate count vector based on the unique set of words\n            count_vector = [count[word] for word in unique_words]\n            # generate probability vector based on unique words and output as fractions.\n            probability = [str(Fraction(count[word], total_words)) if count[word] else '0' for word in unique_words]\n\n            writer.writerow([line.strip(), count_vector, probability])\n    \n    return(csv_file)"
    },
    {
        "file_name": "Assignment_2_Bondarenko_Nikita.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\n# read files\nwith open('rural.txt', 'r') as file:\n    rural_data = file.readlines()\nwith open('science.txt', 'r') as file:\n    science_data = file.readlines()\n    \n# create dataframes\ndf_rural = pd.DataFrame({'Document': rural_data, 'Class': 'rural'})\ndf_science = pd.DataFrame({'Document': science_data, 'Class': 'science'})\n\ndf = pd.concat([df_rural, df_science]).reset_index(drop=True)\n\n\n# split dataset into training and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(df['Document'], df['Class'], test_size=0.3, random_state=192)\n\n\n# using tf-idf-vectorizer to convert text data to format used by classifiers\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train_tfidf.toarray(), Y_train)  # Converting to dense array\n\n# LinearSVC\nsvc = LinearSVC()\nsvc.fit(X_train_tfidf, Y_train)\n\n# function to evaluate a model\ndef evaluate_model(model, X_test, Y_test):\n    predictions = model.predict(X_test)\n    print(\"Accuracy:\", accuracy_score(Y_test, predictions))\n    print(\"Recall:\", recall_score(Y_test, predictions, average='weighted'))\n    print(\"Precision:\", precision_score(Y_test, predictions, average='weighted'))\n    print(\"F1 Score:\", f1_score(Y_test, predictions, average='weighted'))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, predictions))\n    \n# evaluating GaussianNB\nprint(\"GaussianNB Performance:\")\nevaluate_model(gnb, X_test_tfidf.toarray(), Y_test)\n\n# evaluating LinearSVC\nprint(\"\\nLinearSVC Performance:\")\nevaluate_model(svc, X_test_tfidf, Y_test)\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Dobberstein_Niklas.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "import pandas as pd\n\ndf_polarity = pd.read_csv(\"polarity.txt\", sep=\"\\t\")\n# !pip install scikit-learn pandas numpy matplotlib"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Dobberstein_Niklas.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\ndf_polarity[\"pos_num\"] = df_polarity[\"pos\"].apply(lambda e: 1 if e==\"pos\" else 0 )"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Dobberstein_Niklas.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "import csv\nimport re\nfrom typing import List\ndef get_word_list(txt : str) -> List[str]:\n    return re.findall(r\"[\\w']+\", txt)\n\n\ndef create_count_vector(corpus_entry, all_words):\n    corpus_entry = corpus_entry.lower()\n    entry_words = get_word_list(corpus_entry)\n    count_vector = [entry_words.count(word) for word in all_words]\n    return count_vector\n\ndef create_count_vectors(corpus, all_words):\n    count_vectors = []\n    for entry in corpus:\n        count_vector = create_count_vector(entry, all_words)\n        count_vectors.append(count_vector)\n    return count_vectors\n\ndef create_prob_vectors(corpus: List[str], count_vectors: List[List[int]], all_words : List[str]):\n    prob_vectors = []\n\n    for i, entry in enumerate(corpus):\n        words = get_word_list(entry)\n        prob_vector = []\n        for w in words:\n            j = all_words.index(w.lower())\n            prob_vector.append(count_vectors[i][j] / sum(count_vectors[i]))\n\n        prob_vectors.append(prob_vector)\n\n    return prob_vectors\n\ndef create_count_and_probability(file_name):\n    with open(file_name, \"r\") as f:\n        corpus = f.readlines()\n        corpus = [w.replace(\"\\n\", \"\").strip() for w in corpus]\n    all_words = [word.lower()  for line in corpus for word in get_word_list(line)]\n    all_words = list(set(all_words))\n\n    count_vectors = create_count_vectors(corpus, all_words)\n    prob_vectors = create_prob_vectors(corpus,count_vectors,all_words)\n    csv_file = \"out.csv\"\n\n    with open(csv_file, mode='w',newline=\"\") as file:\n        writer = csv.writer(file)\n\n        header = [\"Line\", \"Count\", \"Prob\"] \n        writer.writerow(header)\n\n        for i in range(len(corpus)):\n            row = [corpus[i],count_vectors[i],prob_vectors[i]]\n            writer.writerow(row)\n\n    return(csv_file)\n\ncreate_count_and_probability(\"corpus.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Dobberstein_Niklas.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n\nwith open('rural.txt', 'r') as file:\n    rural_data = file.readlines()\n\nwith open('science.txt', 'r') as file:\n    science_data = file.readlines()\n\nrural_df = pd.DataFrame({'Document': rural_data, 'Class': 'rural'})\nscience_df = pd.DataFrame({'Document': science_data, 'Class': 'science'})\n\ncombined_df = pd.concat([rural_df, science_df], ignore_index=True)\ncombined_df = combined_df.sample(frac=1.0)\ncombined_df.reset_index(drop=True, inplace=True)\ntrain_df, test_df = train_test_split(combined_df, test_size=0.3, random_state=42)\nvectorizer = TfidfVectorizer(stop_words='english')\n\nX_train = vectorizer.fit_transform(train_df['Document'])\n\nX_test = vectorizer.transform(test_df['Document'])\n\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train.toarray(), train_df['Class'])\n\nsvm_classifier = LinearSVC()\nsvm_classifier.fit(X_train, train_df['Class'])\n\nnb_predictions = nb_classifier.predict(X_test.toarray())\nsvm_predictions = svm_classifier.predict(X_test)\ndef evaluate_classifier(true_labels, predicted_labels, positive_class='science'):\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    precision = precision_score(true_labels, predicted_labels, pos_label=positive_class)\n    recall = recall_score(true_labels, predicted_labels, pos_label=positive_class)\n    f1 = f1_score(true_labels, predicted_labels, pos_label=positive_class)\n    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n    \n    evaluation_results = {\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1 Score': f1,\n        'Confusion Matrix': conf_matrix\n    }\n    \n    return evaluation_results\n\nnb_evaluation = evaluate_classifier(test_df['Class'], nb_predictions)\n\nprint(\"Naive Bayes Classifier Evaluation:\")\nfor metric, value in nb_evaluation.items():\n    print(f\"{metric}: {value}\")\n\nsvm_evaluation = evaluate_classifier(test_df['Class'], svm_predictions)\nprint(\"*\"*50)\nprint(\"Linear SVM Classifier Evaluation:\")\nfor metric, value in svm_evaluation.items():\n    print(f\"{metric}: {value}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 1.1 (2 point)\nCreate a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")",
        "answer": "#here comes your code\ncolumns = [\"text\", \"label\"]\nmy_df = pd.read_csv(\"polarity.txt\", sep=\"\\t\", names=columns)\nmy_df"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 1.2 (2 point)\nCreate a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\nHint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0",
        "answer": "# here comes your code\nmy_df[\"label_as_num\"] = my_df[\"label\"].apply(lambda r: 1 if r==\"pos\" else 0)\nmy_df\nmy_df = my_df.drop(\"label\", axis=1) # axis=1 for column\nmy_df"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 2 (8 points)\nWrite a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n1. Text\n2. Count_Vector\n3. Probability\nExample:\nFor the line: `This document is the second document.`\nThe row in the csv file should contain:\n`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n**Note**:\n1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n```\nimport re\nTEXT = \"Hey, - How are you doing today!?\"\nwords_list = re.findall(r\"[\\w']+\", TEXT)\nprint(words_list)\n```\n3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n4. Please don't upload the output file. Your function should generate the file.",
        "answer": "def preprocess_text(text):\n    return re.findall(r\"[\\w']+\", text.lower())\n\n\ndef count_vectorizer(lines):\n    tokenized_documents = [word for line in lines for word in preprocess_text(line)]\n    vocab = list(dict.fromkeys(tokenized_documents))\n    return vocab\n\ndef create_count_and_probability(file_name):\n    # lines = open(file_name, \"r\").readlines()\n    with open(file_name, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    vocab = count_vectorizer(lines)\n    count_vectors = []\n    probabilities = []\n\n    for line in lines:\n        words_list = preprocess_text(line)\n        word_counts = Counter(words_list)\n        count_vectors.append([word_counts.get(word, 0) for word in vocab])\n        total_words = len(words_list)\n        probabilities.append([word_counts[word]/total_words for word in words_list])\n    csv_file = pd.DataFrame({'Text': lines, 'Count_Vector': count_vectors, 'Probability': probabilities})\n\n    ## if you want to make the csv file on your pc uncomment the next line\n    # csv_file.to_csv(file_name.split(\".\")[0]+\".csv\", index=False)\n\n    return(csv_file)\n\ncreate_count_and_probability(\"corpus.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_2_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 3 (8 points)\nThe goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\na) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n|Document                             |Class |\n| ------------------------------------|----- |\n|PM denies knowledge of AWB kickbacks | rural |\n|The crocodile ancestor fossil, found...| science |\nb) Split the data into train (70%) and test (30%) sets and use the tf-idf-vectorizer to train following classifiers provided by scikit-learn:\n- naive_bayes.GaussianNB()\n- svm.LinearSVC().\nc) Evaluate both classifiers using the test set, report accuracy, recall, precision, f1 scores and confusion matrix.\n**Hints:**\n1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n2. You can play around with various parameters in both the tf-idf-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)",
        "answer": "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n\n# step a)\nwith open(\"rural.txt\", 'r', encoding='utf-8') as file:\n    rural_data = file.readlines()\n\nwith open(\"science.txt\", 'r', encoding='utf-8') as file:\n    science_data = file.readlines()\n\n\nfull_data = rural_data + science_data\nfull_labels = ['rural'] * len(rural_data) + ['science'] * len(science_data)\n\ndf = pd.DataFrame({'Document': full_data, 'Class': full_labels})\n\n# step b)\nX_train, X_test, y_train, y_test = train_test_split(df['Document'], df['Class'], test_size=0.3, random_state=42)\n\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# naive_bayes.GaussianNB()\ngnb = GaussianNB()\ngnb.fit(X_train_tfidf.toarray(), y_train)\n\n# svm.LinearSVC()\nsvm_model = LinearSVC()\nsvm_model.fit(X_train_tfidf, y_train)\n\n# step c)\n# naive_bayes.GaussianNB()\ngnb_pred = gnb.predict(X_test_tfidf.toarray())\nprint(\"GNB metrics:\")\nprint(metrics.classification_report(y_test, gnb_pred))\nprint(\"confusion matrix:\")\nprint(metrics.confusion_matrix(y_test, gnb_pred))\n\n# svm.LinearSVC()\nsvm_pred = svm_model.predict(X_test_tfidf)\nprint(\"\\nLinear SVM metrics:\")\nprint(metrics.classification_report(y_test, svm_pred))\nprint(\"confusion matrix:\")\nprint(metrics.confusion_matrix(y_test, svm_pred))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aylin_Gheisar.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\n\ndef extract_proper_nouns(my_file_name):\n    # We will use the part of speech recognition attribute of en_core_web_sm model of the spacy module\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    # We will read the whole file as one document and input it into the nlp model\n    with open(my_file_name, 'r') as file:\n        inp = file.read()\n        \n    doc = nlp(inp)\n\n    several_token_propn = []\n    tmp = []\n    for tok in doc:\n        # Consecutive proper noun parts of speech are saved into a temporary list\n        if tok.pos_ == \"PROPN\":\n            tmp.append(tok.text)\n        else:\n            if len(tmp) > 1:\n                # If the chain of nouns has more than one token, it is saved in several_token_propn\n                several_token_propn.append(' '.join(tmp))\n                \n            # We discard the list if we encounter a non-proper noun\n            tmp.clear()\n            \n    return(several_token_propn)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aylin_Gheisar.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    # We will use both part of speech and lemma attribute of en_core_web_sm model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # We will read the whole file as one document and input it into the nlp model\n    with open(my_file_name, 'r') as file:\n        inp = file.read()\n        \n    doc = nlp(inp)\n    \n    # We cache the lemma of every noun and verb in lemmas_pos dictionary\n    lemmas_pos = {}\n    for tok in doc:\n        if tok.pos_ in [\"NOUN\", \"VERB\"]:\n            # lemma will be the key and (token, pos) pair will be the value in this dictionary\n            if tok.lemma_ in lemmas_pos:\n                lemmas_pos[tok.lemma_].append((tok.text, tok.pos_))\n            else:\n                lemmas_pos[tok.lemma_] = [(tok.text, tok.pos_)]\n                \n    # We select only the tokens with common lemma that have both NOUN and VERB parts of speech\n    tokens_with_common_lemma = {}\n    for lemma in lemmas_pos:\n        # The token and pos pair is splitted\n        tokens = [pair[0] for pair in lemmas_pos[lemma]]\n        parts_of_speech = [pair[1] for pair in lemmas_pos[lemma]]\n                \n        if \"NOUN\" in parts_of_speech and \"VERB\" in parts_of_speech:\n            tokens_with_common_lemma[lemma] = tokens\n    \n    return(tokens_with_common_lemma)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aylin_Gheisar.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\n# We use pandas to read and store the data\ndf = pd.read_csv(\"bbc-news.csv\")\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aylin_Gheisar.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\n# We plot the counts of each category as a bar chart\ndf[\"category\"].value_counts().plot(kind='bar')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aylin_Gheisar.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\nimport string\n\n\ndef remove_punctuation(corpus):\n    # A string with all punctuations is found in string.punctuation object\n    # We can easily substitute those characters with empty string using regex\n    cleaned_corpus = re.sub(f\"[{re.escape(string.punctuation)}]\", '', corpus)\n    \n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # numbers are matched via \\d+ pattern and substituted with empty string\n    cleaned_corpus = re.sub(\"\\d+\", '', corpus)\n    \n    return(cleaned_corpus)\n\n\n# The two preprocessing functions are applied consecutively to the text column\ndf[\"text\"] = df[\"text\"].apply(remove_punctuation).apply(remove_numbers)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aylin_Gheisar.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\n\n\n# Loading the large language model\nnlp = spacy.load(\"en_core_web_lg\")\n\n# We split the dataset according to the instructions\nX = df[\"text\"]\ny = df[\"category\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\n# We get the vector representations of the articles using the large language model\nX_train = [nlp(article).vector for article in X_train]\nX_test = [nlp(article).vector for article in X_test]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aylin_Gheisar.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# We have chosen to use Multi Layer Perceptron, Gaussian Naive Bayes and Linear Support Vector Machine classifier models to fit and test\n# on the data\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n# We fit a mlp classifier with default hyper parameters and evaluate its performance\nmlp_clf = MLPClassifier(hidden_layer_sizes=(100,))\nmlp_clf.fit(X_train, y_train)\nmlp_pred = mlp_clf.predict(X_test)\nprint(\"Metrics for MLP Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, mlp_pred))\nprint(\"Recall:\", recall_score(y_test, mlp_pred, average='micro'))\nprint(\"Precision:\", precision_score(y_test, mlp_pred, average='micro'))\nprint(\"F1_score:\", f1_score(y_test, mlp_pred, average='micro'))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, mlp_pred))\n# We fit a naive bayes classifier with default hyper parameters and evaluate its performance\nnb_clf = GaussianNB()\nnb_clf.fit(X_train, y_train)\nnb_pred = nb_clf.predict(X_test)\nprint(\"Metrics for Naive Bayes Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, nb_pred))\nprint(\"Recall:\", recall_score(y_test, nb_pred, average='micro'))\nprint(\"Precision:\", precision_score(y_test, nb_pred, average='micro'))\nprint(\"F1_score:\", f1_score(y_test, nb_pred, average='micro'))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_pred))\n# We fit a support vector machine classifier with default hyper parameters and evaluate its performance\nsvm_clf = LinearSVC()\nsvm_clf.fit(X_train, y_train)\nsvm_pred = svm_clf.predict(X_test)\nprint(\"Metrics for Support Vector Machine Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, svm_pred))\nprint(\"Recall:\", recall_score(y_test, svm_pred, average='micro'))\nprint(\"Precision:\", precision_score(y_test, svm_pred, average='micro'))\nprint(\"F1_score:\", f1_score(y_test, svm_pred, average='micro'))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, svm_pred))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport string\n\nnlp = spacy.load(\"en_core_web_sm\")\ndef extract_proper_nouns(my_file_name):\n  several_token_propn = []\n\n  with open(my_file_name) as f:\n\n    text = f.read()\n    doc = nlp(text)\n    propNFlag = False # flag to check if the last token is a proper noun\n\n    for tok in doc:\n      if len(several_token_propn) == 0 and tok.pos_ == 'PROPN':\n        several_token_propn.append(tok.text)\n        propNFlag = True\n      elif tok.pos_ == 'PROPN' and propNFlag == True:\n        propNFlag = True\n        several_token_propn[-1] = several_token_propn[-1] + \" \" + tok.text\n      elif tok.pos_ == 'PROPN':\n        several_token_propn.append(tok.text)\n        propNFlag = True\n      else:\n        propNFlag = False\n\n     # remove proper nouns having only one token\n    several_token_propn = list(filter(lambda x: len(x.split(\" \")) != 1, several_token_propn))\n\n  return(several_token_propn)\n# example with a text file sample_1.txt\nextract_proper_nouns(\"sample_1.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    tokens_pos = {} # storing the part of the speeck of the tokens corresponding to tokens_with_common_lemma\n\n    with open(my_file_name) as f:\n      text = f.read()\n      doc = nlp(text)\n\n      for tok in doc:\n        if tok.pos_ == \"NOUN\" or tok.pos_ == \"VERB\":\n          # if lemma not already in the dictionary, create the key value pair\n          if tokens_with_common_lemma.get(tok.lemma_, None):\n            tokens_with_common_lemma[tok.lemma_].append(tok.text)\n            tokens_pos[tok.lemma_].append(tok.pos_)\n          # otherwise add the token to the existing list.\n          else:\n            tokens_with_common_lemma[tok.lemma_] = [tok.text]\n            tokens_pos[tok.lemma_] = [tok.pos_]\n\n    # remove those key value pairs that don't contain verbs and nouns sharing the same lemma\n    for key in tokens_pos:\n      value =  tokens_pos[key]\n      if \"NOUN\" not in value or \"VERB\" not in value:\n        del tokens_with_common_lemma[key]\n\n\n    return(tokens_with_common_lemma)\n# example with a text file corpus.txt\ncommon_lemma(\"sample_2.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\ndf = pd.read_csv(\"bbc-news.csv\")\nprint(\"data shape: \", df.shape)\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\ncount_df = df.groupby([\"category\"]).count().reset_index()\n\nplt.figure(figsize=(10, 6))\nplt.bar(count_df['category'], count_df['text'], color='skyblue')\nplt.title('Number of Articles per Topical Area/class label')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)  # Rotate x-axis labels for better readability if needed\nplt.tight_layout()\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### V1: Remove punctuation and numbers through Spacy (Slow)",
        "answer": "def remove_punctuation_and_numbers(text, isPunc = True):\n  doc = nlp(text)\n  if isPunc:\n    cleaned_text = ' '.join(token.text for token in doc if not token.is_punct)\n  else:\n    cleaned_text = ' '.join(token.text for token in doc if not token.is_digit)\n  return cleaned_text\n\ndef remove_punctuation(corpus):\n    cleaned_corpus = corpus.apply(lambda x: remove_punctuation_and_numbers(x))\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    cleaned_corpus = corpus.apply(lambda x: remove_punctuation_and_numbers(x, False))\n    return(cleaned_corpus)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### V2: Remove punctuation and numbers through String Module (Faster)",
        "answer": "def remove_punctuation_and_numbers(text, isPunc = True):\n  if isPunc:\n    remove_punc = str.maketrans('', '', string.punctuation)\n    cleaned_text = text.translate(remove_punc)\n  else:\n    remove_dgts = str.maketrans('', '', string.digits)\n    cleaned_text = text.translate(remove_dgts)\n  return cleaned_text\n\ndef remove_punctuation(corpus):\n    cleaned_corpus = corpus.apply(lambda x: remove_punctuation_and_numbers(x))\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    cleaned_corpus = corpus.apply(lambda x: remove_punctuation_and_numbers(x, False))\n    return(cleaned_corpus)\ndf_cleaned = df.copy()\ndf_cleaned['text'] = remove_punctuation(df_cleaned['text'])\ndf_cleaned['text'] = remove_numbers(df_cleaned['text'])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "!python -m spacy download en_core_web_lg\n# Here comes your code\nnlp = spacy.load(\"en_core_web_lg\")\n\n# separate the innput\nX = df_cleaned['text']\n\n# convert class labels to numerical values\ncat_class = pd.Categorical(df_cleaned['category'])\ny = cat_class.codes\n\n#split training and testing data with 70% and 30%, respectivily\ntext_train, text_test, label_train, label_test = train_test_split(X, y,\n                                                                  test_size=0.30,\n                                                                  random_state=101, shuffle=True)\n\ntext_train.shape, text_test.shape\n# convert each article in the training data splits to a vector representation\ntext_train_tokenized = text_train.apply(nlp)\ntext_train_vectors = text_train_tokenized.apply(lambda x: x.vector) # get the vector representation\n\n# Convert the list of arrays to a 2D NumPy array\nvectors_arr = np.vstack(text_train_vectors) # convert it to array to able to train models on that data\nprint(vectors_arr.shape)\nvectors_arr\n# convert each article in the test data splits to a vector representation\ntext_test_tokenized = text_test.apply(nlp)\ntext_test_vectors = text_test_tokenized.apply(lambda x: x.vector) # get the vector representation\n\n# Convert the list of arrays to a 2D NumPy array\nvectors_test = np.vstack(text_test_vectors) # convert it to array to able to test models on that data\nprint(vectors_test.shape)\nvectors_test"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Model 1: MLP Classifier",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Initialize the MLP Classifier\nmlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=101)\n\n# Train the classifier\nmlp.fit(vectors_arr, label_train)\n\n# Predict on the test set\npredictions_mlp = mlp.predict(vectors_test)\nprint(\"----- MLP Classifier -----\")\n\n# Calculate the accuracy\naccuracy_mlp = accuracy_score(label_test, predictions_mlp)\nprint(f\"Test Accuracy: {accuracy_mlp:.4f}\")\n\n# Generate confusion matrix\nconf_matrix_mlp = confusion_matrix(label_test, predictions_mlp)\n\n# Display the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_mlp, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=cat_class.categories, yticklabels=cat_class.categories)\nplt.xlabel(\"Predicted labels\")\nplt.ylabel(\"True labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Model 2: Logistic Regression",
        "answer": "from sklearn.linear_model import LogisticRegression\n\n# Initialize the Logistic Regression Model\nclf = LogisticRegression(max_iter=2000)\n\n# Train the classifier\nclf.fit(vectors_arr,label_train)\n\n# Predict on the test set\npredictions_lr = clf.predict(vectors_test)\nprint(\"----- Logistic Regression -----\")\n\n# Calculate accuracy\naccuracy_lr = accuracy_score(label_test, predictions_lr)\nprint(f\"Test Accuracy: {accuracy_lr:.4f}\")\n\n# Generate confusion matrix\nconf_matrix_lr = confusion_matrix(label_test, predictions_lr)\n\n# Display the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_lr, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=cat_class.categories, yticklabels=cat_class.categories)\nplt.xlabel(\"Predicted labels\")\nplt.ylabel(\"True labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Aksa_Aksa.ipynb",
        "question": "### Model 3: Random Forest Classifier",
        "answer": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed\n\n# Train the classifier\nclf.fit(vectors_arr, label_train)\n\n# Predict on the test set\npredictions_rf = clf.predict(vectors_test)\nprint(\"----- Random Forest Classifier -----\")\n\n# Calculate accuracy\naccuracy_rf = accuracy_score(label_test, predictions_rf)\nprint(f\"Test Accuracy: {accuracy_rf:.4f}\")\n\n# Generate confusion matrix\nconf_matrix_rf = confusion_matrix(label_test, predictions_rf)\n\n# Display the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_rf, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=cat_class.categories, yticklabels=cat_class.categories)\nplt.xlabel(\"Predicted labels\")\nplt.ylabel(\"True labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Valdrin_Smakaj.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\ndef extract_proper_nouns(my_file_name):\n    # Load the English NLP model from spaCy\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    several_token_proper = []\n    \n    # read file content\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n    \n    # Process the text through the NLP pipeline \n    doc = nlp(text)\n    \n    # Iterate over the named entities\n    for ent in doc.ents:\n        # Check if the entity is a proper noun and has more than one token\n        if ent.label_ in ['PERSON', 'GPE', 'ORG'] and len(ent.text.split()) > 1:\n            several_token_proper.append(ent.text)\n    \n    return several_token_proper\n\n\nproper_nouns = extract_proper_nouns('./my_file.txt')\nprint(proper_nouns)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Valdrin_Smakaj.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name):\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    tokens_with_common_lemma = {}\n    lemma_pos_pair = {}\n\n    # Read file content\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    # Process the text through the NLP pipeline\n    doc = nlp(text)\n\n    # collect all tokens and their parts of speech by lemma\n    for token in doc:\n        if token.pos_ in ['NOUN', 'VERB']:\n            if token.lemma_ not in lemma_pos_pair:\n                lemma_pos_pair[token.lemma_] = set()\n            lemma_pos_pair[token.lemma_].add(token.pos_)\n\n    # build dictionary only for those lemmas that have both a noun and a verb form\n    for token in doc:\n        if token.lemma_ in lemma_pos_pair and 'NOUN' in lemma_pos_pair[token.lemma_] and 'VERB' in lemma_pos_pair[token.lemma_]:\n            if token.lemma_ in tokens_with_common_lemma:\n                if token.text not in tokens_with_common_lemma[token.lemma_]:\n                    tokens_with_common_lemma[token.lemma_].append(token.text)\n            else:\n                tokens_with_common_lemma[token.lemma_] = [token.text]\n\n    return tokens_with_common_lemma\n\nlemmas = common_lemma('./my_file.txt')\nprint(lemmas)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Valdrin_Smakaj.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\nfile_path = './bbc-news.csv'\n\n# Load the data\ndata = pd.read_csv(file_path)\n\n# print data\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Valdrin_Smakaj.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# file path\nfile_path = './bbc-news.csv'\n\n# Load data\ndata = pd.read_csv(file_path)\n\n# Count the occurrences of each unique label in the 'category' column\ncategory_counts = data['category'].value_counts()\n\n# Plot results\nplt.figure(figsize=(10, 5))\ncategory_counts.plot(kind='bar')\nplt.title('Number of Articles per Topical Area')\nplt.xlabel('Category')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Valdrin_Smakaj.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import pandas as pd\nimport re\nimport string\n\n# remove punctuation from a single document\ndef remove_punctuation(corpus):\n    translator = str.maketrans('', '', string.punctuation)\n    return corpus.translate(translator)\n\n# remove numbers from a single document\ndef remove_numbers(corpus):\n    return re.sub(r'\\d+', '', corpus)\n\n# Load the data\ndata = pd.read_csv(\"./bbc-news.csv\")\n\n# Apply the preprocessing functions \ndata['text'] = data['text'].apply(remove_punctuation)\ndata['text'] = data['text'].apply(remove_numbers)\n\n# Display the cleaned text\nprint(data['text'])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Valdrin_Smakaj.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\n\n\n# Load the large English model from spaCy\nnlp = spacy.load('en_core_web_lg')\n\n# Dataset\ndata = pd.read_csv('./bbc-news.csv')\n\n# Use only the first 1000 rows and the first 150 tokens of each article for vectorization\ndata_subset = data.iloc[:1000].copy() \ndata_subset.loc[:, 'text'] = data_subset['text'].apply(lambda x: ' '.join(x.split()[:150]))\n\n\n# Split data into training and test set\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    data_subset['text'],\n    data_subset['category'],\n    test_size=0.3,\n    random_state=101,\n    shuffle=True\n)\n\n# Vectorize the text using the spaCy model\ndef vectorize_text(text):\n    return nlp(text).vector\n\n# Vectorize the training and testing data\ntrain_vectors = np.array([vectorize_text(text) for text in train_data])\ntest_vectors = np.array([vectorize_text(text) for text in test_data])\n\nprint(train_vectors.shape, test_vectors.shape)\nprint(train_vectors, test_vectors)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Valdrin_Smakaj.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "import spacy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport numpy as np\nimport pandas as pd\n\n# Load the large English model from spaCy\nnlp = spacy.load('en_core_web_lg')\n\n# Dataset\ndata = pd.read_csv('./bbc-news.csv')\n\n# Use only the first 1000 rows and the first 150 tokens of each article for vectorization\ndata_subset = data.iloc[:1000].copy() \ndata_subset.loc[:, 'text'] = data_subset['text'].apply(lambda x: ' '.join(x.split()[:150]))\n\n\n# Split data into training and test set\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    data_subset['text'],\n    data_subset['category'],\n    test_size=0.3,\n    random_state=101,\n    shuffle=True\n)\n\n# Vectorize the text using the spaCy model\ndef vectorize_text(text):\n    return nlp(text).vector\n\n# Vectorize the training and testing data\ntrain_vectors = np.array([vectorize_text(text) for text in train_data])\ntest_vectors = np.array([vectorize_text(text) for text in test_data])\n\n# Initialize the three classifiers\nclf_mlp = MLPClassifier(max_iter=1000)\nclf_logistic_regression = LogisticRegression(max_iter=1000)\nclf_random_forest = RandomForestClassifier()\n\n# Train MLPClassifier\nclf_mlp.fit(train_vectors, train_labels)\n# Train LogisticRegression\nclf_logistic_regression.fit(train_vectors, train_labels)\n# Train RandomForestClassifier\nclf_random_forest.fit(train_vectors, train_labels)\n\n# Predict the labels for the test set\npredictions_mlp = clf_mlp.predict(test_vectors)\npredictions_lr = clf_logistic_regression.predict(test_vectors)\npredictions_rf = clf_random_forest.predict(test_vectors)\n\n# Calculate accuracy for each model\naccuracy_mlp = accuracy_score(test_labels, predictions_mlp)\naccuracy_lr = accuracy_score(test_labels, predictions_lr)\naccuracy_rf = accuracy_score(test_labels, predictions_rf)\n\n# Generate confusion matrix for each model\nconf_matrix_mlp = confusion_matrix(test_labels, predictions_mlp)\nconf_matrix_lr = confusion_matrix(test_labels, predictions_lr)\nconf_matrix_rf = confusion_matrix(test_labels, predictions_rf)\n\n# Print the results\nprint(\"MLPClassifier Accuracy:\", accuracy_mlp)\nprint(\"Confusion Matrix:\\n\", conf_matrix_mlp)\n\nprint(\"\\nLogisticRegression Accuracy:\", accuracy_lr)\nprint(\"Confusion Matrix:\\n\", conf_matrix_lr)\n\nprint(\"\\nRandomForestClassifier Accuracy:\", accuracy_rf)\nprint(\"Confusion Matrix:\\n\", conf_matrix_rf)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import nltk\nnltk.download('averaged_perceptron_tagger')\nfrom nltk import pos_tag, word_tokenize\n\ndef extract_proper_nouns(file_name):\n    with open(file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Tokenize the text and perform part-of-speech tagging\n    words = word_tokenize(text)\n    tagged_words = pos_tag(words)\n\n    # Extract proper nouns with more than one token\n    proper_nouns = [tagged_words[i][0] + \" \" + tagged_words[i + 1][0]\n                    for i in range(len(tagged_words) - 1)\n                    if tagged_words[i][1] == 'NNP' and tagged_words[i + 1][1] == 'NNP']\n\n    return proper_nouns\n\nfile_name = \"nlp_countries.txt\"  \nresult = extract_proper_nouns(file_name)\n\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import nltk\nnltk.download('punkt')\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.stem import WordNetLemmatizer \n\ndef clean_word(word):\n    # Remove punctuation characters\n    cleaned_word = ''.join(char for char in word if char.isalnum())\n    return cleaned_word\n\ndef common_lemma(my_file_name):\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Tokenize the text and perform part-of-speech tagging\n    words = word_tokenize(text)\n    tagged_words = pos_tag(words)\n\n    # Initialize the WordNet Lemmatizer\n    lemmatizer = WordNetLemmatizer()\n\n    # Dictionary to store lemmas and their associated words\n    tokens_with_common_lemma = {}\n\n    for word, pos in tagged_words:\n        # Use WordNet lemmatizer to get the base form (lemma) of the word\n        # Specify 'n' as the default POS if the POS tag is not available\n        \n        # Check if the lemma is a noun or a verb\n        if pos.startswith('N') or pos.startswith('V'):\n            lemma = lemmatizer.lemmatize(word, pos=pos[0].lower() if pos else 'n')\n            # If the lemma is not in the dictionary, add it with an empty list\n            if lemma not in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma] = []\n             \n # Clean the word and append it to the lemma's list\n            cleaned_word = clean_word(word)\n            if cleaned_word:  # Check if the word is not empty after cleaning\n                tokens_with_common_lemma[lemma].append(cleaned_word)\n\n    # Filter out lemmas with only one associated word\n    tokens_with_common_lemma = {lemma: words for lemma, words in tokens_with_common_lemma.items() if len(words) > 1}\n\n    return tokens_with_common_lemma\n\n\nfile_name = \"query_show_nlp.txt\"\nresult = common_lemma(file_name)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\n# Replace 'your_file_path/bbc-text.csv' with the actual path to your CSV file\nfile_path = 'bbc-news.csv'\n\n# Load the CSV file into a pandas DataFrame\ndataset = pd.read_csv(file_path)\n\n# Display the first few rows of the DataFrame\nprint(dataset.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming 'category' is the column containing class labels\n# Replace it with the actual column name from your DataFrame\ncategory_counts = dataset['category'].value_counts()\n\n# Plotting\nplt.figure(figsize=(10, 6))\ncategory_counts.plot(kind='bar', color='skyblue')\nplt.title('Number of Articles for Each Topical Area')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45, ha='right')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(text):\n    # Remove punctuation using string.punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    cleaned_text = text.translate(translator)\n    return cleaned_text\n\ndef remove_numbers(text):\n    # Remove numbers using str.isdigit()\n    cleaned_text = ' '.join(word for word in text.split() if not word.isdigit())\n    return cleaned_text\n\n# Assuming 'text' is the column containing the text data in your DataFrame\n# Replace it with the actual column name from your DataFrame\ndataset['text'] = dataset['text'].apply(remove_punctuation)\ndataset['text'] = dataset['text'].apply(remove_numbers)\n\n# Display the preprocessed DataFrame\nprint(dataset.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# Here comes your code\n# WARNING! use this line to download the pretrained model\n# !python -m spacy download en_core_web_lg\nimport spacy\nfrom sklearn.model_selection import train_test_split\n\n# Load model\nmodel = spacy.load(\"en_core_web_lg\")\n\n# Split the dataset into 70/30 train-test datasets\ntext_train, text_test, category_train, category_test = train_test_split(dataset['text'], dataset['category'],\n                                                                  test_size=0.30,\n                                                                  random_state=101, shuffle=True)\n\n# Convert training and test data to vector representation\ntext_train_vector = [model(document).vector for document in text_train]\ntext_test_vector = [model(document).vector for document in text_test]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_LeylaHashimli_SonaJabrayilova.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Training 3 different multiclass classification models\n# Gradient Boosting\ngradient_boost_model = GradientBoostingClassifier()\ngradient_boost_model.fit(text_train_vector, category_train)\n\n# Random Forest\nrandom_forest_model = RandomForestClassifier(random_state=101)\nrandom_forest_model.fit(text_train_vector, category_train)\n\n# MLPClassifier\nmlp_model = MLPClassifier(random_state=101, max_iter=1000)\nmlp_model.fit(text_train_vector, category_train)\n\n# Predict on test data\ngradient_boost_predictions = gradient_boost_model.predict(text_test_vector)\nrandom_forest_predictions = random_forest_model.predict(text_test_vector)\nmlp_predictions = mlp_model.predict(text_test_vector)\n\n# Calculate evaluation metrics\n# Accuracy scores\ngradient_boost_accuracy = accuracy_score(category_test, gradient_boost_predictions)\nrandom_forest_accuracy = accuracy_score(category_test, random_forest_predictions)\nmlp_accuracy = accuracy_score(category_test, mlp_predictions)\n\nprint(\"Accuracy Scores:\")\nprint(f\"\"\"Gradient Boosting: {round(gradient_boost_accuracy, 3)}\nRandom Forest: {round(random_forest_accuracy, 3)}\nMLP Classifier: {round(mlp_accuracy, 3)}\"\"\")\n\n# Confusion Matrix\ngradient_boost_matrix = confusion_matrix(category_test, gradient_boost_predictions)\nrandom_forest_conf_matrix = confusion_matrix(category_test, random_forest_predictions)\nmlp_conf_matrix = confusion_matrix(category_test, mlp_predictions)\n\nprint(\"Confusion Matrix:\")\nprint(f\"\"\"Gradient Boosting\\n: {gradient_boost_matrix}\nRandom Forest\\n: {random_forest_conf_matrix}\nMLP Classifier\\n: {mlp_conf_matrix}\"\"\")\n\n# Confusion Matrix plots for better display purposes\ndef plot_confusion_matrix(confusion_matrix, category_names):\n    plt.figure(figsize=(4, 4))\n    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Greens',\n                cbar=False, xticklabels=category_names, yticklabels=category_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted values')\n    plt.ylabel('True Values')\n    plt.show()\n\ncategory_names = (list(set(dataset.category)))\ncategory_names.sort()\nprint(\"Gradient Boosting:\")\nplot_confusion_matrix(gradient_boost_matrix, category_names)\nprint(\"Random Forest:\")\nplot_confusion_matrix(random_forest_conf_matrix, category_names)\nprint(\"MLP:\")\nplot_confusion_matrix(mlp_conf_matrix, category_names)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_BrunoScheider.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "!python -m spacy download en_core_web_sm\n###setup\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ndef read_file(file_name):\n    with open(file_name) as f:\n        data = f.read()\n    return data\nimport spacy \n\n\ndef extract_proper_nouns(my_file_name):\n    text = read_file(my_file_name)\n    several_token_propn = []\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n    \n    #proper_nouns = [token.text for token in doc if token.pos_ == 'PROPN' and len(token.text.split()) > 1]\n    proper_nouns = [ent.text.strip() for ent in doc.ents if len(ent.text.split()) > 1]\n    print(doc.ents)\n    \n    for i,token in enumerate(doc):\n        print(token.text.split())\n        if (i==0):\n            continue\n        if (token.pos_ == 'PROPN') and (doc[i-1].pos_ == 'PROPN'):\n            several_token_propn.append(str(doc[i-1])+\" \"+str(token)) \n        #print(token.has_annotation('PROPN'))\n    #print(doc[0])\n    \n    \n    return(several_token_propn)\n\n#text = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nfile_name = \"corpus.txt\"\nprint(extract_proper_nouns(file_name) )"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_BrunoScheider.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndef common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    helper = {}\n    #text = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\n    #text = \"I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\"\n    text = read_file(my_file_name)\n    doc = nlp(text)\n    for token in doc:\n        pos = token.pos_\n        \n        if (not (pos == 'NOUN' or pos=='VERB')):\n            continue\n        key = str(token.lemma_)\n        \n        if (key in tokens_with_common_lemma):\n            if (not (str(token) in tokens_with_common_lemma[key]) ):\n                tokens_with_common_lemma[key].append(str(token))\n                helper[key].append(pos)\n        else:\n            tokens_with_common_lemma[key] = [str(token)]\n            helper[key] = [pos]\n\n    for key, values in helper.items():\n        if (not('NOUN' in values and 'VERB' in values)):\n            del tokens_with_common_lemma[key]\n        \n    \n    return(tokens_with_common_lemma)\n\nprint(common_lemma('corpus.txt'))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_BrunoScheider.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\n\n#in the files there is only an bbc-news.csv\ndf = pd.read_csv('bbc-news.csv')\n\n## in case it is named like in the desciption\n#df = pd.read_csv('bbc-text.csv')\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_BrunoScheider.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "#plot = df.plot.pie(y='category', figsize=(5, 5))\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nx = Counter(list(df['category']))\nplt.bar(x.keys(), x.values())\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_BrunoScheider.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "from string import punctuation\n\ndef remove_punctuation(corpus):\n    # Here comes your code\n    for p in set(punctuation):\n        corpus = corpus.replace(p, \"\")\n    return(corpus)\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    number = [0,1,2,3,4,5,6,7,8,9]\n    for n in number:\n        corpus = corpus.replace(str(n), \"\")\n    return(corpus)\n\ndf['text'] = df['text'].apply(remove_punctuation)\ndf['text'] = df['text'].apply(remove_numbers)\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_BrunoScheider.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n####a)\nlg_model = spacy.load('en_core_web_lg')\n\n####b)\nX_train_str, X_test_str, y_train_str, y_test_str = train_test_split(df['text'], df['category'], test_size=0.3, shuffle=True, random_state=101)\n\n\nprint(X_train_str.shape)\n#print(len(X_test))\n####c)\nx_train = []\nx_test = []\nfor i,x in enumerate(X_train_str):\n    x_train.append(np.array(lg_model(x).vector, dtype=np.float32))\n\nfor i,x in enumerate(X_test_str):\n    x_test.append(np.array(lg_model(x).vector, dtype=np.float32))\n\nprint(len(x_train))\nprint(len(x_test))\nx_test = np.stack( x_test, axis=0)\nx_train = np.stack( x_train, axis=0)\nprint(x_test.shape)\nprint(x_train.shape)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_BrunoScheider.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n###a)\nclf1 = MLPClassifier(random_state=1, max_iter=300)\nclf1.fit(x_train, y_train)\n\nclf2 = RandomForestClassifier(max_depth=2, random_state=1)\nclf2.fit(x_train, y_train)\n\nclf3 = GaussianNB()\nclf3.fit(x_train, y_train)\n###b)\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    PrecisionRecallDisplay\n)\n\ny_pred1 = clf1.predict(x_test)\naccuracy = accuracy_score(y_pred1, y_test)\nprint('Accuracy for MLPClassifier: ', accuracy)\ncm = confusion_matrix(y_test, y_pred1)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\n\ny_pred2 = clf2.predict(x_test)\naccuracy = accuracy_score(y_pred2, y_test)\nprint('Accuracy for MLPClassifier: ', accuracy)\ncm = confusion_matrix(y_test, y_pred2)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\n\ny_pred3 = clf3.predict(x_test)\naccuracy = accuracy_score(y_pred3, y_test)\nprint('Accuracy for MLPClassifier: ', accuracy)\ncm = confusion_matrix(y_test, y_pred3)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Timon_Oerder.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_proper_nouns(my_file_name=None, my_text=None):\n    textstring = \"\"\n    if my_file_name is not None: # if a file is provided use the file\n        with open(my_file_name, 'r') as file:\n            for line in file:\n                textstring += line\n    elif my_text is not None: # if a text is provided use the text\n        textstring = my_text\n    else: # abort if no file or text is provided\n        print(\"Please provide a file name or a text string\")\n        return None\n\n    several_token_propn = []\n    doc = nlp(textstring) # create a spacy doc\n    nounpt = \"\" # create a string to store the noun phrase\n    for token in doc:\n        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\": # if the token is a noun or proper noun, add it to the noun phrase\n            nounpt = nounpt+token.text + \" \"\n        else :\n            if len(nounpt.split()) > 1: # if the noun phrase is more than one word, add it to the list\n                several_token_propn.append(nounpt.rstrip()) # remove the trailing space\n            nounpt = \"\" # reset the noun phrase\n\n\n    return(several_token_propn)\n\nextract_proper_nouns(my_text=\"Hong Kong and Japan are two countries in Asia and New York is the largest city in the world\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Timon_Oerder.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name=None, my_text=None):\n    textstring = \"\"\n    if my_file_name is not None: # if a file is provided use the file\n        with open(my_file_name, 'r') as file:\n            for line in file:\n                textstring += line\n    elif my_text is not None: # if a text is provided use the text\n        textstring = my_text\n    else: # abort if no file or text is provided\n        print(\"Please provide a file name or a text string\")\n        return None\n\n\n    tokens_with_common_lemma = {}\n    doc = nlp(textstring) # create a spacy doc\n    for token in doc:\n        if token.lemma_ in tokens_with_common_lemma: # if the lemma is a key in the dictionary, test if the token is already in the list\n            if token.text not in tokens_with_common_lemma[token.lemma_]:\n                tokens_with_common_lemma[token.lemma_].append(token.text) # if the token is not in the list, add it\n        else:\n            tokens_with_common_lemma[token.lemma_] = [token.text] # create a new key in the dictionary and add the token to the list\n    return(tokens_with_common_lemma)\n\ncommon_lemma(my_text=\"When users google for a word or a query their system internally runs a pipeline in order to process what the person is querying.\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Timon_Oerder.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\ndf = pd.read_csv('bbc-news.csv', sep=',', header=0)\nprint(df.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Timon_Oerder.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import plotly.express as px\n\nfig = px.bar(df, x='category',  title='BBC News Categories')\nfig.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Timon_Oerder.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\n\n# I assume that the dataframe is passed as an argument, I am not entilrely sure what corpus otherwise refers to\n\ndef remove_punctuation(corpus):\n    corpus['text'] = corpus['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) # return only alphanumeric characters and spaces\n    return(corpus)\n\ndef remove_numbers(corpus):\n    corpus['text'] = corpus['text'].apply(lambda x: re.sub(r\"\\d\", \"\", x)) # replace all digits with an empty strings\n    return(corpus)\n\n\n## Test the functions\nprint(df.loc[12, 'text'])\ndf = remove_punctuation(df)\nprint(df.loc[12, 'text'])\ndf = remove_numbers(df)\nprint(df.loc[12, 'text'])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Timon_Oerder.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "nlpnew = spacy.load(\"en_core_web_lg\")\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n\ndef test_train_split(df, factorizey=False, samplefrac=None):\n    newdf = df.copy() #create a copy of the dataframe\n\n    # other possible preprocessing step\n    #newdf = newdf.assign(textvector=newdf['text'].apply(lambda x: nlpnew(x).vector)) # create a new column with the vector representation of the text\n    \n    if samplefrac is not None and type(samplefrac) == float and samplefrac > 0 and samplefrac < 1:\n        # we can sample a fraction of the dataframe\n        newdf = newdf.sample(frac=samplefrac).reset_index(drop=True)\n\n    #print(newdf.shape, newdf.head()) # print the head of the new dataframe\n\n    X = newdf['text'].to_numpy() # create a numpy array of the text\n    X = np.array([nlpnew(x).vector for x in X]) # create a numpy array of the vector representation of the text\n    y = newdf['category'].to_numpy() # create a numpy array of the categories\n\n    if factorizey: #if we want to factorize the categories\n        vals = pd.factorize(y)\n        y = vals[0]\n        print(vals)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) # split the data into training and test sets\n\n    return(X_train, X_test, y_train, y_test)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Timon_Oerder.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, multilabel_confusion_matrix\n\ndef testclassifier(classifier, X_train, X_test, y_train, y_test):\n    classifier.fit(X_train, y_train) # fit the classifier\n    y_pred = classifier.predict(X_test) # predict the test data\n\n    accuracy = accuracy_score(y_test, y_pred)\n    conf_matrix = multilabel_confusion_matrix(y_test, y_pred, labels=classifier.classes_)\n\n    print(\"Classifier: \", classifier)\n    print(\"Accuracy: \", accuracy)\n    print(\"Confusion Matrix: \", conf_matrix)\n    return accuracy, conf_matrix\n\nX_train, X_test, y_train, y_test = test_train_split(df)\n\nnb_classifier = GaussianNB() \nsvc_classifier = LinearSVC()\nmlp_classifier = MLPClassifier()\n\ntestclassifier(nb_classifier, X_train, X_test, y_train, y_test)\ntestclassifier(svc_classifier, X_train, X_test, y_train, y_test)\ntestclassifier(mlp_classifier, X_train, X_test, y_train, y_test)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_RaoRohilPrakash.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import nltk\nfrom nltk import word_tokenize, pos_tag\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\ndef extract_proper_nouns(my_file_name):\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Tokenize the text and perform part-of-speech tagging\n    tokens = word_tokenize(text)\n    tagged_words = pos_tag(tokens)\n\n    proper_nouns = []\n    current_proper_noun = \"\"\n\n    for word, pos in tagged_words:\n        if pos == 'NNP':  # NNP indicates a proper noun\n            current_proper_noun += word + \" \"\n        elif current_proper_noun:\n            proper_nouns.append(current_proper_noun.strip())\n            current_proper_noun = \"\"\n\n    # Add the last proper noun if any\n    if current_proper_noun:\n        proper_nouns.append(current_proper_noun.strip())\n\n    # Filter proper nouns with more than one token\n    result = [noun for noun in proper_nouns if len(noun.split()) > 1]\n\n    return result\n\n# Example usage\nfile_name = \"untitled.txt\"  # Replace with the actual file name\nresult = extract_proper_nouns(file_name)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_RaoRohilPrakash.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import nltk\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\n\ndef common_lemma(my_file_name):\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Tokenize the text and perform part-of-speech tagging\n    tokens = word_tokenize(text)\n    tagged_words = pos_tag(tokens)\n\n    lemmatizer = WordNetLemmatizer()\n    lemma_dict = {}\n\n    for word, pos in tagged_words:\n        # Consider only verbs and nouns\n        if pos.startswith('N') or pos.startswith('V'):\n            lemma = lemmatizer.lemmatize(word, pos=pos[0].lower())\n            if lemma in lemma_dict:\n                lemma_dict[lemma].append(word)\n            else:\n                lemma_dict[lemma] = [word]\n\n    # Filter lemmas with more than one token\n    result = {lemma: words for lemma, words in lemma_dict.items() if len(set(words)) > 1}\n\n    return result\n\n# Example usage\nfile_name = \"untitled1.txt\"  # Replace with the actual file name\nresult = common_lemma(file_name)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_RaoRohilPrakash.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "pip install pandas -U --user\n# Here comes your code\nimport pandas as pd\n\n# Reading data from a CSV file\ndf = pd.read_csv('bbc-news.csv')\n\nprint(df.shape)\n\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_RaoRohilPrakash.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\ndf['category'].hist()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_RaoRohilPrakash.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(corpus):\n    # Here comes your code\n    # Use str.translate() to remove punctuation\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n    cleaned_corpus = corpus.translate(translator)\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    translator = str.maketrans(\"\", \"\", string.digits)\n    cleaned_corpus = corpus.translate(translator)\n    return(cleaned_corpus)\nremove_numbers(remove_punctuation(\"a cat, and 2 dogs\"))\ndf[\"processed_text\"] = df[\"text\"].apply(remove_punctuation).apply(remove_numbers)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_RaoRohilPrakash.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "pip install -U spacy thinc --user\n# Here comes your code\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport spacy\n\n# Load spaCy model\nnlp = spacy.load('en_core_web_lg')\n\n# a) Load the large model trained on web text provided by spaCy\ndf['vector'] = df['processed_text'].apply(lambda x: nlp(x).vector)\n\n# b) Split the data into training and test sets (70% and 30%)\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=101, shuffle=True)\n\n# c) Convert each article to a vector representation using the pre-trained spaCy model\n# For training data\nX_train = list(train_df['vector'])\ny_train = train_df['category']  \n\n# For test data\nX_test = list(test_df['vector'])\ny_test = test_df['category']"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_RaoRohilPrakash.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\n\n# Scaling the vectors (optional but often beneficial for some models)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# a) Train 3 different models\n# Model 1: Support Vector Machine (SVM)\nsvm_model = SVC(random_state=101)\nsvm_model.fit(X_train_scaled, y_train)\n\n# Model 2: Random Forest\nrf_model = RandomForestClassifier(random_state=101)\nrf_model.fit(X_train_scaled, y_train)\n\n# Model 3: MLPClassifier (Neural Network)\nmlp_model = MLPClassifier(random_state=101)\nmlp_model.fit(X_train_scaled, y_train)\n\n# b) Evaluate the classifiers on the test set\n# Make predictions\nsvm_predictions = svm_model.predict(X_test_scaled)\nrf_predictions = rf_model.predict(X_test_scaled)\nmlp_predictions = mlp_model.predict(X_test_scaled)\n\n# Calculate accuracy\nsvm_accuracy = accuracy_score(y_test, svm_predictions)\nrf_accuracy = accuracy_score(y_test, rf_predictions)\nmlp_accuracy = accuracy_score(y_test, mlp_predictions)\n\n# Confusion Matrix\nsvm_conf_matrix = confusion_matrix(y_test, svm_predictions)\nrf_conf_matrix = confusion_matrix(y_test, rf_predictions)\nmlp_conf_matrix = confusion_matrix(y_test, mlp_predictions)\n\n# Report results\nprint(\"Support Vector Machine Accuracy:\", svm_accuracy)\nprint(\"Support Vector Machine Confusion Matrix:\\n\", svm_conf_matrix)\n\nprint(\"\\nRandom Forest Accuracy:\", rf_accuracy)\nprint(\"Random Forest Confusion Matrix:\\n\", rf_conf_matrix)\n\nprint(\"\\nMLPClassifier Accuracy:\", mlp_accuracy)\nprint(\"MLPClassifier Confusion Matrix:\\n\", mlp_conf_matrix)\n"
    },
    {
        "file_name": "Assignment3Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "pip install spacy\nimport spacy\n\ndef extract_proper_nouns(my_file_name):\n    # Load the spaCy English language model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Process the text using spaCy\n    doc = nlp(text)\n\n    # Initialize a list to store proper nouns with more than one token\n    several_token_propn = []\n\n    # Iterate through named entities in the document\n    for ent in doc.ents:\n        # Check if the entity is a proper noun and has more than one token\n        if ent.label_ == \"GPE\" or ent.label_ == \"LOC\" or ent.label_ == \"FAC\":\n            if len(ent) > 1:\n                several_token_propn.append(ent.text)\n\n    return several_token_propn\n\n# Example\nfile_name = \"test.txt\"\nresult = extract_proper_nouns(file_name)\nprint(result)"
    },
    {
        "file_name": "Assignment3Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name):\n    # Load the spaCy English language model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Process the text using spaCy\n    doc = nlp(text)\n\n    # Initialize a dictionary to store lemmas and their associated words (both nouns and verbs)\n    tokens_with_common_lemma = {}\n\n    # Iterate through tokens in the document\n    for token in doc:\n        # Check if the token is a noun or a verb\n        if token.pos_ in ['NOUN', 'VERB']:\n            # Get the lemma of the token\n            lemma = token.lemma_\n\n            # If the lemma is not already in the dictionary, create an entry\n            if lemma not in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma] = [token.text]\n            else:\n                # If the lemma is already in the dictionary, add the token to the list\n                tokens_with_common_lemma[lemma].append(token.text)\n\n    # Filter out lemmas with only one associated word\n    tokens_with_common_lemma = {lemma: words for lemma, words in tokens_with_common_lemma.items() if len(words) > 1}\n\n    return tokens_with_common_lemma\n\n# Example usage\nfile_name = \"exercise2.txt\"\nresult = common_lemma(file_name)\nprint(result)"
    },
    {
        "file_name": "Assignment3Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\ndef load_bbc_data(file_path):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Display the first few rows of the DataFrame\n    print(df.head())\n\n    return df\n\nfile_path = \"bbc-news.csv\"\n\n# load the data\nbbc_data = load_bbc_data(file_path)"
    },
    {
        "file_name": "Assignment3Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef load_bbc_data(file_path):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Display the first few rows of the DataFrame \n    print(df.head())\n\n    return df\n\ndef plot_article_counts(data_frame):\n    # Count the number of articles for each topical area\n    article_counts = data_frame['category'].value_counts()\n\n    # Plot the counts using a bar plot\n    article_counts.plot(kind='bar', figsize=(10, 6), color='skyblue')\n    \n    # Add labels and title\n    plt.xlabel('Topical Area')\n    plt.ylabel('Number of Articles')\n    plt.title('Number of Articles for Each Topical Area')\n    \n    # Show the plot\n    plt.show()\n\n\nfile_path = \"bbc-news.csv\"\n\n\nbbc_data = load_bbc_data(file_path)\n\n# Plot the article counts\nplot_article_counts(bbc_data)"
    },
    {
        "file_name": "Assignment3Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\nimport re\n\ndef remove_punctuation(corpus):\n    # Remove punctuation using the string library\n    translator = str.maketrans('', '', string.punctuation)\n    cleaned_corpus = [text.translate(translator) for text in corpus]\n\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    # Remove numbers using regular expressions\n    cleaned_corpus = [re.sub(r'\\d', '', text) for text in corpus]\n\n    return cleaned_corpus\n\n# Example \nbbc_data['text'] = remove_punctuation(bbc_data['text'])\nbbc_data['text'] = remove_numbers(bbc_data['text'])\n\nprint(bbc_data['text'])"
    },
    {
        "file_name": "Assignment3Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\n\n# Load the spaCy English language model (large)\nnlp = spacy.load(\"en_core_web_lg\")\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(bbc_data['text'], test_size=0.3, random_state=101, shuffle=True)\n\ndef vectorize_text(text):\n    # Process the text using the spaCy model\n    doc = nlp(text)\n\n    # Return the vector representation of the document\n    return doc.vector\n\n# Apply the vectorization function to the training and test sets\nX_train_vectors = [vectorize_text(article) for article in X_train]\nX_test_vectors = [vectorize_text(article) for article in X_test]\n\n\n# Modify the text to use only the first 150 tokens\nX_train_subset = [article[:150] for article in X_train]\nX_test_subset = [article[:150] for article in X_test]\n\n# Apply the vectorization function to the modified sets\nX_train_vectors = [vectorize_text(article) for article in X_train_subset]\nX_test_vectors = [vectorize_text(article) for article in X_test_subset]"
    },
    {
        "file_name": "Assignment3Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(bbc_data['text'], bbc_data['category'], test_size=0.3, random_state=101, shuffle=True)\n\n\n# Convert class labels to numerical encoding\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\n# Train MLPClassifier\nmlp_classifier = MLPClassifier(random_state=101)\nmlp_classifier.fit(X_train_vectors, y_train_encoded)\n\n# Train RandomForestClassifier\nrf_classifier = RandomForestClassifier(random_state=101)\nrf_classifier.fit(X_train_vectors, y_train_encoded)\n\n# Train Support Vector Classifier\nsvc_classifier = SVC(random_state=101)\nsvc_classifier.fit(X_train_vectors, y_train_encoded)\n\n# Evaluate MLPClassifier\nmlp_predictions = mlp_classifier.predict(X_test_vectors)\nmlp_accuracy = accuracy_score(y_test_encoded, mlp_predictions)\nmlp_conf_matrix = confusion_matrix(y_test_encoded, mlp_predictions)\n\n# Evaluate RandomForestClassifier\nrf_predictions = rf_classifier.predict(X_test_vectors)\nrf_accuracy = accuracy_score(y_test_encoded, rf_predictions)\nrf_conf_matrix = confusion_matrix(y_test_encoded, rf_predictions)\n\n# Evaluate Support Vector Classifier\nsvc_predictions = svc_classifier.predict(X_test_vectors)\nsvc_accuracy = accuracy_score(y_test_encoded, svc_predictions)\nsvc_conf_matrix = confusion_matrix(y_test_encoded, svc_predictions)\n\n# Print results\nprint(\"MLP Classifier:\")\nprint(f\"Accuracy: {mlp_accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(mlp_conf_matrix)\nprint(\"\\n\")\n\nprint(\"Random Forest Classifier:\")\nprint(f\"Accuracy: {rf_accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\n\nprint(\"Support Vector Classifier:\")\nprint(f\"Accuracy: {svc_accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(svc_conf_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muskaan_Chopra.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "!pip install nltk\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag, ne_chunk\n\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    tokenized_text = word_tokenize(text)\n    tagged_words = pos_tag(tokenized_text)\n    named_entities = ne_chunk(tagged_words)\n\n    for entity in named_entities:\n        if isinstance(entity, nltk.tree.Tree) and entity.label() == 'PERSON':\n            entity_name = \" \".join([word for word, tag in entity.leaves()])\n            if len(entity_name.split()) > 1:\n                several_token_propn.append(entity_name)\n\n    return several_token_propn"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muskaan_Chopra.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return ''\n\ndef common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    lemmatizer = WordNetLemmatizer()\n\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    tokenized_text = word_tokenize(text)\n    tagged_words = nltk.pos_tag(tokenized_text)\n\n    for word, tag in tagged_words:\n        wordnet_pos = get_wordnet_pos(tag)\n        if wordnet_pos in [wordnet.NOUN, wordnet.VERB]:  # Only process nouns and verbs\n            lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n            if lemma not in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma] = []\n            tokens_with_common_lemma[lemma].append(word)\n\n    # Remove entries where the lemma only appears once\n    tokens_with_common_lemma = {k: v for k, v in tokens_with_common_lemma.items() if len(v) > 1}\n\n    return tokens_with_common_lemma"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muskaan_Chopra.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf=pd.read_csv('bbc-news.csv')\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muskaan_Chopra.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\n\n# Count the number of texts in each category\ncategory_counts = df['category'].value_counts()\n\n# Plot the counts\nplt.figure(figsize=(10,6))\ncategory_counts.plot(kind='bar')\nplt.title('Number of Texts for Each Category')\nplt.xlabel('Category')\nplt.ylabel('Number of Texts')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muskaan_Chopra.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\nimport re\n\ndef remove_punctuation(corpus):\n    cleaned_corpus = [doc.translate(str.maketrans('', '', string.punctuation)) for doc in corpus]\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    cleaned_corpus = [re.sub(r'\\d+', '', doc) for doc in corpus]\n    return cleaned_corpus"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muskaan_Chopra.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the large English NLP model\nspacy.cli.download(\"en_core_web_lg\")\nnlp = spacy.load('en_core_web_lg')\n\n# Split the data into training and test sets\ntrain, test = train_test_split(df, test_size=0.3, random_state=101)\n\n# Convert articles to vectors\ndef articles_to_vectors(data):\n    vectors = []\n    for article in data:\n        # Use only the first 150 tokens\n        article = article[:150]\n        doc = nlp(article)\n        vectors.append(doc.vector)\n    return vectors\n\ntrain_vectors = articles_to_vectors(train['text'])\ntest_vectors = articles_to_vectors(test['text'])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muskaan_Chopra.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Convert labels to numerical values\ntrain_labels = train['category'].astype('category').cat.codes\ntest_labels = test['category'].astype('category').cat.codes\n\n# Train MLPClassifier\nmlp = MLPClassifier(random_state=1, max_iter=300).fit(train_vectors, train_labels)\n\n# Train RandomForestClassifier\nrf = RandomForestClassifier(random_state=1).fit(train_vectors, train_labels)\n\n# Train SVC\nsvc = SVC().fit(train_vectors, train_labels)\n\n# Evaluate the classifiers\nfor model in [mlp, rf, svc]:\n    predictions = model.predict(test_vectors)\n    accuracy = accuracy_score(test_labels, predictions)\n    confusion = confusion_matrix(test_labels, predictions)\n    print(f\"Model: {model.__class__.__name__}\")\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"Confusion Matrix:\\n{confusion}\\n\")"
    },
    {
        "file_name": "Assignment_3_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "def extract_proper_nouns(my_file_name):\n    # Load English Language Model\n    nlp = spacy.load(\"en_core_web_sm\")\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n    # Process the text\n    doc = nlp(text)\n\n    # Create the list to store all proper tokens\n    several_token_propn = []\n    current_proper_noun = \"\"\n\n    for token in doc:\n        # Filter proper nouns\n        if token.pos_ == 'PROPN':  \n            current_proper_noun += token.text + \" \"\n        else:\n            # Check for a proper noun to add\n            if current_proper_noun.strip():  \n                several_token_propn.append(current_proper_noun.strip())\n                current_proper_noun = \"\"\n\n    # Check for the last potential proper noun\n    if current_proper_noun.strip():\n        several_token_propn.append(current_proper_noun.strip())\n\n    # Filter proper nouns with >1 tokens\n    several_token_propn = [noun for noun in several_token_propn if len(noun.split()) > 1]\n\n    return(several_token_propn)"
    },
    {
        "file_name": "Assignment_3_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    nlp = spacy.load(\"en_core_web_sm\")\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    doc = nlp(text)\n    tokens_with_common_lemma = {}\n    \n    for token in doc:\n        # Filter verbs and nouns\n        if token.pos_ in ('VERB', 'NOUN'):\n            lemma = token.lemma_\n            if lemma not in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma] = [token.text]\n            else:\n                if token.text not in tokens_with_common_lemma[lemma]:\n                    tokens_with_common_lemma[lemma].append(token.text)\n\n    # Filter 1 form lemmas\n    tokens_with_common_lemma = {lemma: forms for lemma, forms in tokens_with_common_lemma.items() if len(forms) > 1}\n\n    return(tokens_with_common_lemma)"
    },
    {
        "file_name": "Assignment_3_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "bbc_df = pd.read_csv('bbc-news.csv')\nprint(bbc_df.columns)"
    },
    {
        "file_name": "Assignment_3_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "category_counts = bbc_df['category'].value_counts()\n\nplt.figure(figsize=(10, 6))\ncategory_counts.plot(kind='bar')\nplt.title('Number of Articles per Category')\nplt.xlabel('Category')\nplt.ylabel('# of Articles')\nplt.show()"
    },
    {
        "file_name": "Assignment_3_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(corpus):\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n    cleaned_corpus = [text.translate(translator) for text in corpus]\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    cleaned_corpus = [' '.join(word for word in text.split() if not word.isdigit()) for text in corpus]\n    return cleaned_corpus\nbbc_df['text'] = remove_punctuation(bbc_df['text'])\nbbc_df['text'] = remove_numbers(bbc_df['text'])"
    },
    {
        "file_name": "Assignment_3_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# a) \nnlp = spacy.load(\"en_core_web_lg\")\n\n# b) \ntrain_data, test_data = train_test_split(bbc_df, test_size=0.3, random_state=101)\n\n# c) \ntrain_vectors = [nlp(article).vector for article in train_data['text']]\ntest_vectors = [nlp(article).vector for article in test_data['text']]"
    },
    {
        "file_name": "Assignment_3_AmirhosseinBarari_YeganehsadatHosseinisereshgi.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "train_labels = train_data['category']\ntest_labels = test_data['category']\n\n# Logistic Regression\nlogistic_model = LogisticRegression(random_state=101)\nlogistic_model.fit(train_vectors, train_labels)\n\n# SVM\nsvm_model = SVC(random_state=101)\nsvm_model.fit(train_vectors, train_labels)\n\n# MLP\nmlp_model = MLPClassifier(random_state=101)\nmlp_model.fit(train_vectors, train_labels)\n\n# Evaluation\nlogistic_predictions = logistic_model.predict(test_vectors)\nsvm_predictions = svm_model.predict(test_vectors)\nmlp_predictions = mlp_model.predict(test_vectors)\n\n# Accuracy\nlogistic_accuracy = accuracy_score(test_labels, logistic_predictions)\nsvm_accuracy = accuracy_score(test_labels, svm_predictions)\nmlp_accuracy = accuracy_score(test_labels, mlp_predictions)\n\nprint(f\"Logistic Regression Accuracy: {logistic_accuracy:.4f}\")\nprint(f\"SVM Accuracy: {svm_accuracy:.4f}\")\nprint(f\"MLP Accuracy: {mlp_accuracy:.4f}\")\n\n# Confusion Matrix\nlogistic_confusion_matrix = confusion_matrix(test_labels, logistic_predictions)\nsvm_confusion_matrix = confusion_matrix(test_labels, svm_predictions)\nmlp_confusion_matrix = confusion_matrix(test_labels, mlp_predictions)\n\nprint(\"\\nLogistic Regression Confusion Matrix:\")\nprint(logistic_confusion_matrix)\n\nprint(\"\\nSVM Confusion Matrix:\")\nprint(svm_confusion_matrix)\n\nprint(\"\\nMLP Confusion Matrix:\")\nprint(mlp_confusion_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ayush_Mishra.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy.cli\nimport spacy\n\n\ndef extract_proper_nouns(my_file_name):\n    try:\n        # Try to load the spaCy English model\n        nlp = spacy.load(\"en_core_web_sm\")\n    except OSError:\n        # Download the model if it's not available\n        spacy.cli.download(\"en_core_web_sm\")\n        nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the contents of the file\n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n\n    # Process the text with spaCy\n    doc = nlp(text)\n\n    # Extract proper nouns with more than one token\n    several_token_propn = [\n        ent.text\n        for ent in doc.ents\n        if ent.label_ == \"GPE\" and len(ent.text.split()) > 1\n    ]\n\n    return several_token_propn\n\n\n# file contains the text\n# \"Hong Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nresult = extract_proper_nouns(\"task1.txt\") # file contains the text\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ayush_Mishra.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name):\n    try:\n        # Try to load the spaCy English model\n        nlp = spacy.load(\"en_core_web_sm\")\n    except OSError:\n        # Download the model if it's not available\n        spacy.cli.download(\"en_core_web_sm\")\n        nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the contents of the file and process with spaCy\n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        doc = nlp(file.read())\n\n    # Find common lemmas\n    tokens_with_common_lemma = {token.lemma_: [t.text for t in doc if t.lemma_ == token.lemma_]\n                                for token in doc\n                                if token.pos_ in (\"NOUN\", \"VERB\") and not token.is_stop and token.lemma_}\n\n    # Filter out lemmas with only one occurrence\n    tokens_with_common_lemma = {lemma: tokens for lemma, tokens in tokens_with_common_lemma.items() if len(tokens) > 1}\n\n    # Filter out lemmas that correspond to nouns only\n    # Adjusted to correctly check the POS tag of the tokens in the document\n    tokens_with_common_lemma = {lemma: tokens for lemma, tokens in tokens_with_common_lemma.items()\n                                if any(t for t in doc if t.lemma_ == lemma and t.pos_ == \"VERB\")}\n\n    return tokens_with_common_lemma\n\n# file contains the text\n# \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nresult1 = common_lemma(\"task2a.txt\")\nprint(result1)\n\n# file contains the text\n# \"I really loved the movie and show, the movie was showing reality but it showed sometimes nonsense!\"\nresult2 = common_lemma(\"task2b.txt\")\nprint(result2)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ayush_Mishra.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\n\nbbc_news_df = pd.read_csv('bbc-news.csv')\n\nbbc_news_df.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ayush_Mishra.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\nimport matplotlib.pyplot as plt\n\n# Count the number of articles in each category\ncategory_counts = bbc_news_df['category'].value_counts()\n\n# Plot the counts\nplt.figure(figsize=(6, 4))\ncategory_counts.plot(kind='bar')\nplt.title('Number of Articles per Category in BBC News Dataset')\nplt.xlabel('Category')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ayush_Mishra.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(corpus):\n    # Define a translation table to remove punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    # Apply the translation table to the corpus\n    cleaned_corpus = corpus.translate(translator)\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    # Remove digits using a translation table\n    translator = str.maketrans('', '', string.digits)\n    # Apply the translation table to the corpus\n    cleaned_corpus = corpus.translate(translator)\n    return cleaned_corpus\n\n# Apply the functions to the dataset\nbbc_news_df['cleaned_text'] = bbc_news_df['text'].apply(remove_punctuation).apply(remove_numbers)\n\nbbc_news_df.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ayush_Mishra.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Task 6 (a)\nmodel_name = 'en_core_web_lg'\ntry:\n    nlp = spacy.load(model_name)\nexcept OSError:\n    print(f\"Downloading language model for the spaCy POS tagger: {model_name}\")\n    spacy.cli.download(model_name)\n    nlp = spacy.load(model_name)\n\n# Function to vectorize text using the spaCy model efficiently\ndef vectorize_texts(texts, nlp_model):\n    return np.array([text.vector for text in nlp_model.pipe(texts)])\n\n# Task 6 (b)\nX_train, X_test, y_train, y_test = train_test_split(\n    bbc_news_df['cleaned_text'],  # The texts to split\n    bbc_news_df['category'],      # The labels\n    test_size=0.3,                # 30% of the data for testing\n    random_state=101,             # Seed for reproducibility\n    shuffle=True                  # Shuffle the data before splitting\n)\n\n# Task 6 (c)\nX_train_vectors = vectorize_texts(X_train, nlp)\nX_test_vectors = vectorize_texts(X_test, nlp)\n\n# Check the shape of the vectors\nprint(X_train_vectors.shape, X_test_vectors.shape)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ayush_Mishra.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Train three different models\n# 1. MLPClassifier\nmlp = MLPClassifier(random_state=101)\nmlp.fit(X_train_vectors, y_train)\n\n# 2. RandomForestClassifier\nrf = RandomForestClassifier(random_state=101)\nrf.fit(X_train_vectors, y_train)\n\n# 3. SVC (Support Vector Classifier)\nsvc = SVC(random_state=101)\nsvc.fit(X_train_vectors, y_train)\n\n# Evaluate the classifiers\nmodels = {'MLPClassifier': mlp, 'RandomForestClassifier': rf, 'SVC': svc}\n\nfor name, model in models.items():\n    y_pred = model.predict(X_test_vectors)\n    accuracy = accuracy_score(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"Model: {name}\\nAccuracy: {accuracy}\\n\")\n\n    # Plotting confusion matrix\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='g')\n    plt.title(f'Confusion Matrix for {name}')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "def extract_proper_nouns(my_file_name):\n\n    with open(my_file_name) as file:\n        text = file.read().replace('\\n',' ')\n    print(text)\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n\n    pos = [t.i for t in doc if t.pos_ == \"PROPN\"]\n    current = []\n    consecutives = []\n    for i in pos:\n        if len(current) == 0:\n            current.append(i)\n        else:\n            if current[-1] == i - 1:\n                current.append(i)\n            else:\n                if len(current) < 2:\n                    current = [i]\n                else:\n                    consecutives.append(current)\n                    current = [i]\n    if len(current) > 1:\n        consecutives.append(current)\n\n    several_token_propn = [doc[consecutive[0]:consecutive[-1]+1] for consecutive in consecutives]\n    return several_token_propn\nprint(extract_proper_nouns(\"test.txt\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Elwakeel_Wedermann.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n\n    with open(my_file_name) as file:\n        text = file.read().replace('\\n',' ')\n    print(text)\n\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    uniq_tok = dict()\n    uniq_pos = dict()\n\n    doc = nlp(text)\n    for tok in doc:\n        if tok.pos_ == \"NOUN\" or tok.pos_ == \"VERB\":\n            if tok.lemma_ in uniq_tok:\n                if uniq_pos[tok.lemma_] == tok.pos_:\n                    for x in uniq_tok[tok.lemma_]:\n                        if tok.text != x.text:\n                            uniq_tok[tok.lemma_].append(tok)\n                else:\n                    if tok.lemma_ in tokens_with_common_lemma:\n                        tokens_with_common_lemma[tok.lemma_].append(tok)\n                    else:\n                        tokens_with_common_lemma[tok.lemma_] = [tok]\n\n            else:\n                uniq_tok[tok.lemma_] = [tok]\n                uniq_pos[tok.lemma_] = tok.pos_\n\n    for key in uniq_tok:\n        if key in tokens_with_common_lemma:\n            if type(uniq_tok[key]) == list:\n                tokens_with_common_lemma[key] = tokens_with_common_lemma[key] + uniq_tok[key]\n            else:\n                tokens_with_common_lemma[key].append(uniq_tok[key])\n\n    return tokens_with_common_lemma"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Elwakeel_Wedermann.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "df = pd.read_csv('bbc-news.csv')\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Elwakeel_Wedermann.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nx = Counter(df[\"category\"])\nprint(x)\n\nplt.bar(x.keys(), x.values())\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Elwakeel_Wedermann.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n  for ele in corpus:\n      if ele in punc:\n          cleaned_corpus = corpus.replace(ele, \" \")\n  return cleaned_corpus\n\ndf[\"text\"].apply(remove_punctuation)\ndef remove_numbers(corpus):\n  cleaned_corpus = ''.join((x for x in corpus if not x.isdigit()))\n  return(cleaned_corpus)\n\ndf[\"text\"].apply(remove_numbers)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Elwakeel_Wedermann.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import sklearn\nfrom sklearn.model_selection import train_test_split\n#import spacy.cli\n#spacy.cli.download(\"en_core_web_lg\")\nNLP = spacy.load(\"en_core_web_lg\")\ndf[\"vector\"] = df[\"text\"].apply(lambda text:NLP(text).vector)\ndf.head()\nX_train, X_test, Y_train, Y_test = train_test_split(df[\"vector\"].values, df.category.values, test_size=0.3, random_state=101,shuffle = True)\nX_train_2d = np.stack(X_train)\nX_test_2d = np.stack(X_test)\nprint(len(X_train))\nprint(len(Y_test))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Elwakeel_Wedermann.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nclf = MLPClassifier()\nclf.fit(X_train_2d,Y_train)\ny_pred = clf.predict(X_test_2d)\nprint(classification_report(Y_test,y_pred))\ncm = confusion_matrix(Y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=clf.classes_)\ndisp.plot()\nplt.show()\nfrom sklearn.tree import DecisionTreeClassifier\nD_T = DecisionTreeClassifier()\nD_T.fit(X_train_2d,Y_train)\ny_pred_D_T = D_T.predict(X_test_2d)\nprint(classification_report(Y_test,y_pred_D_T))\ncm = confusion_matrix(Y_test, y_pred_D_T)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=D_T.classes_)\ndisp.plot()\nplt.show()\nfrom sklearn.ensemble import RandomForestClassifier\nRandom_forest = RandomForestClassifier()\nRandom_forest.fit(X_train_2d,Y_train)\ny_pred_RF = Random_forest.predict(X_test_2d)\nprint(classification_report(Y_test,y_pred_RF))\ncm = confusion_matrix(Y_test, y_pred_RF)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=Random_forest.classes_)\ndisp.plot()\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "!python -m spacy download en_core_web_sm\nimport spacy\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    #Load the spaCy small English model\n    nlp = spacy.load('en_core_web_sm')\n    #Read contents of file\n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n\n    # Process the text with spaCy\n    doc = nlp(text)\n\n    #Extract proper nouns with more than one token \n    for chunk in doc.noun_chunks:\n        if len(chunk) > 1 and all(token.pos_ == 'PROPN' for token in chunk):\n            several_token_propn.append(chunk.text)\n    \n    return(several_token_propn)\n#Example\nmy_file_name = \"my_file_name.txt\"\nresult = extract_proper_nouns(my_file_name)\nprint (result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "from collections import defaultdict\ndef common_lemma(my_file_name):\n    tokens_with_common_lemma = defaultdict(list)\n    #Load the spaCy small English model\n    nlp = spacy.load('en_core_web_sm')\n    #Read contents of file\n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n    doc = nlp(text)\n\n    # Iterate through the tokens in the processed text\n    for token in doc:\n        # Check if the token is a verb or a noun and not already present in the dictionary, if already present then dont add \n        if token.pos_ == 'VERB' and (not token.lemma_ in tokens_with_common_lemma or not token.text in tokens_with_common_lemma):\n            tokens_with_common_lemma[token.lemma_].append(token.text)\n        elif token.pos_ == 'NOUN' and (not token.lemma_ in tokens_with_common_lemma or not token.text in tokens_with_common_lemma):\n            tokens_with_common_lemma[token.lemma_].append(token.text)\n    \n    updated_dict = {}\n    # Remove all values from dictioanry that dont have both noun and verb with the same lemma\n    for lemma, tokens in tokens_with_common_lemma.items():\n        if len(tokens) > 1:\n            updated_dict[lemma] = tokens\n    tokens_with_common_lemma = updated_dict   \n    \n    return tokens_with_common_lemma\n#Example\nmy_file_name = \"my_file_name.txt\"\nresult = common_lemma(my_file_name)\nprint (result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\ndata = pd.read_csv(\"bbc-news.csv\")\ndata"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\nimport matplotlib.pyplot as plt\n\n#Get all the unique cateogires from the data\ncategories = data['category'].unique()\n#Get pandas series with categories and their count\ncount_per_value = data['category'].value_counts()\n#Get list of counts of each category\ncounts_list = count_per_value.tolist()\n\n# Create a bar chart\nplt.figure(figsize=(8, 6))\nplt.bar(categories, counts_list, color=['blue', 'green', 'red', 'purple', 'orange'])\nplt.xlabel(\"Category\")\nplt.ylabel(\"Number of Articles\")\nplt.title(\"Number of Articles per Category\")\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Show the plot\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\nimport string\ndef remove_punctuation(corpus):\n    # Remove punctuation from the text\n    cleaned_corpus = ''.join([char for char in corpus if char not in string.punctuation])\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # Remove numbers from the text using regular expression\n    cleaned_corpus = re.sub(r'\\d+', '', corpus)\n    return(cleaned_corpus)\ndata['text'] = data['text'].apply(remove_punctuation)\ndata['text'] = data['text'].apply(remove_numbers)\n\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_lg\")\n\n# Load dataset \ndf = pd.read_csv(\"bbcnews.csv\", encoding='utf-8')\n\n\n# Taking only 1000 dataset\ndf = df.head(1000)\n\n# using 150 tokens of each article\ndf['text'] = df['text'].apply(lambda x: ' '.join(x.split()[:150]))\n\n# Split the data into training and test sets into 70% and 30% using the scikit-learn\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=101, shuffle=True)\n\n# Convert each article to a vector representation\ndef text_to_vector(text):\n    # Process the text using spaCy\n    doc = nlp(text)\n    # Return the vector representation of the entire document\n    return doc.vector\n\n# Apply the text_to_vector function to the training and test sets\ntrain_vectors = train_df['text'].apply(text_to_vector).to_list()\ntest_vectors = test_df['text'].apply(text_to_vector).to_list()\n\n# Convert vectors to arrays\ntrain_vectors = pd.DataFrame(train_vectors).to_numpy()\ntest_vectors = pd.DataFrame(test_vectors).to_numpy()\n\n# shapes of the resulting arrays\nprint(\"Shape of the training vectors:\", train_vectors.shape)\nprint(\"Shape of the test vectors:\", test_vectors.shape)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "import spacy\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\n\n#the first 1000 rows)\ndf = df.head(1000)\n\n#150 tokens of each article\ndf['text'] = df['text'].apply(lambda x: ' '.join(x.split()[:150]))\n\n# Split the data into training and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=101, shuffle=True)\n\n# Vectorize text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=1000)\ntrain_vectors = tfidf_vectorizer.fit_transform(train_df['text']).toarray()\ntest_vectors = tfidf_vectorizer.transform(test_df['text']).toarray()\n\n\n\n\n# Task a) Now lets Train 3 different models\n\n# Model 1: MLPClassifier\nmlp_model = MLPClassifier(random_state=101)\nmlp_model.fit(train_vectors, train_df['category'])\n\n# Model 2: RandomForestClassifier\nrf_model = RandomForestClassifier(random_state=101)\nrf_model.fit(train_vectors, train_df['category'])\n\n# Model 3: Support Vector Classifier (SVC)\nsvc_model = SVC(random_state=101)\nsvc_model.fit(train_vectors, train_df['category'])\n\n# Task b) Evaluate the classifiers\n\n# Predictions on the test set\nmlp_predictions = mlp_model.predict(test_vectors)\nrf_predictions = rf_model.predict(test_vectors)\nsvc_predictions = svc_model.predict(test_vectors)\n\n\n# Accuracy and Confusion Matrix\nmlp_accuracy = accuracy_score(test_df['category'], mlp_predictions)\nrf_accuracy = accuracy_score(test_df['category'], rf_predictions)\nsvc_accuracy = accuracy_score(test_df['category'], svc_predictions)\n\n\n\nmlp_confusion_matrix = confusion_matrix(test_df['category'], mlp_predictions)\nrf_confusion_matrix = confusion_matrix(test_df['category'], rf_predictions)\nsvc_confusion_matrix = confusion_matrix(test_df['category'], svc_predictions)\n\n# Display results\nprint(\"MLP Classifier Accuracy:\", mlp_accuracy)\nprint(\"Random Forest Classifier Accuracy:\", rf_accuracy)\nprint(\"Support Vector Classifier Accuracy:\", svc_accuracy)\n\nprint(\"\\nMLP Classifier Confusion Matrix:\")\nprint(mlp_confusion_matrix)\n\nprint(\"\\nRandom Forest Classifier Confusion Matrix:\")\nprint(rf_confusion_matrix)\n\nprint(\"\\nSupport Vector Classifier Confusion Matrix:\")\nprint(svc_confusion_matrix)\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mauel_Maximilian.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\nfrom spacy.matcher import Matcher\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.model_selection import train_test_split\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, \"r\") as file:\n      text = file.read()\n\n    doc = nlp(text)\n    matcher = Matcher(nlp.vocab)\n\n    pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n    matcher.add(\"MULTI_TOKEN_PROPN\", [pattern])\n\n    for match_id, start, end in matcher(doc):\n        multi_token_propn = doc[start:end]\n        if len(multi_token_propn) > 1:\n            several_token_propn.append(multi_token_propn.text)\n\n    return(several_token_propn)\n#s = extract_proper_nouns(\"sample_data/README.md\")\n#print(s)\n!echo \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\" > yoo.txt\ns = extract_proper_nouns(\"yoo.txt\")\nprint(s)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mauel_Maximilian.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, \"r\") as file:\n        text = file.read()\n\n    doc = nlp(text)\n\n    for token in doc:\n      if token.pos_ in [\"VERB\", \"NOUN\"]:\n        lemma = token.lemma_\n\n        if lemma in tokens_with_common_lemma:\n          if token.text not in tokens_with_common_lemma[lemma]:\n            tokens_with_common_lemma[lemma].append(token.text)\n        else:\n          tokens_with_common_lemma[lemma] = [token.text]\n\n    # bases on the examples i guess i should just return entries of length > 1\n    tokens_with_common_lemma = {lemma: tokens for lemma, tokens in tokens_with_common_lemma.items() if len(tokens) > 1}\n\n    return(tokens_with_common_lemma)\n!echo \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\" > yoo.txt\nr = common_lemma(\"yoo.txt\")\nprint(r)\n\n!echo \"I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\" > yoo.txt\nr = common_lemma(\"yoo.txt\")\nprint(r)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mauel_Maximilian.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "df = pd.read_csv(\"bbc-news.csv\")\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mauel_Maximilian.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "df['category'].value_counts().plot(kind='bar')\nplt.title('Number of Articles per Category')\nplt.xlabel('Category')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mauel_Maximilian.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    cleaned_corpus = corpus.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    cleaned_corpus = corpus.apply(lambda x: re.sub(r'[-+]?\\d*\\.?\\d+[eE][-+]?\\d+', '', x))\n    return(cleaned_corpus)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mauel_Maximilian.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "!python -m spacy download en_core_web_lg\n\"\"\"\nnlp = spacy.load(\"en_core_web_lg\")\n\nsubset_df = df.head(1000)\n\nX = subset_df['text']\ny = subset_df['category']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\ndef spacy_tokenizer(text):\n    doc = nlp(text)\n    return doc[:150].vector\n\nX_train_vectors = [spacy_tokenizer(text) for text in X_train]\nX_test_vectors = [spacy_tokenizer(text) for text in X_test]\n\"\"\"\nnlp = spacy.load(\"en_core_web_lg\")\n\nX = df['text']\ny = df['category']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\n#X_train_vectors = list(nlp.pipe(X_train))\n#X_test_vectors = list(nlp.pipe(X_test))\n\nX_train_vectors = [nlp(text).vector for text in X_train]\nX_test_vectors = [nlp(text).vector for text in X_test]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mauel_Maximilian.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nsvm_classifier = SVC()\nrandom_forest_classifier = RandomForestClassifier()\nmlp_classifier = MLPClassifier(max_iter=1000)\n\nsvm_classifier.fit(X_train_vectors, y_train)\nrandom_forest_classifier.fit(X_train_vectors, y_train)\nmlp_classifier.fit(X_train_vectors, y_train)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, multilabel_confusion_matrix\ndef eval(classifier, X_test, y_test, classifier_name):\n  predictions = classifier.predict(X_test)\n  accuracy = accuracy_score(y_test, predictions)\n  # conf_matrix = confusion_matrix(y_test, predictions)\n  conf_matrix = multilabel_confusion_matrix(y_test, predictions)\n\n  print(f\"\\n{classifier_name}:\")\n  print(f\"Accuracy: {accuracy:.5f}\")\n  print(\"Confusion Matrix:\")\n  print(conf_matrix)\n\neval(svm_classifier, X_test_vectors, y_test, \"svm\")\neval(random_forest_classifier, X_test_vectors, y_test, \"random forest\")\neval(mlp_classifier, X_test_vectors, y_test, \"mlp\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\ndef extract_proper_nouns(my_file_name):\n    \"\"\"\n    Extract proper nouns with multiple tokens from a text file. (e.g. New York, Hong Kong)\n    \"\"\"\n    \n    several_token_propn = []\n    # Load spacy model.\n    nlp = spacy.load('en_core_web_sm')\n    # Open file.\n    with open(my_file_name, 'r') as file:\n        # Read file.\n        doc = nlp(file.read())\n        # Extract proper nouns.\n        i = 0\n        while i < len(doc) - 1:\n            token = doc[i]\n            if token.pos_ == 'PROPN' and token.dep_ == 'compound':\n                compound = token.text\n                # Check if the next token is a proper noun.\n                # The last word of the compound has to have a different dep_ than 'compound'.\n                while i + 1 < len(doc) and doc[i + 1].dep_ == 'compound':\n                    compound += ' ' + doc[i + 1].text\n                    i += 1\n                # Add the last word.\n                compound += ' ' + doc[i + 1].text\n                # Add the compound to the list.\n                several_token_propn.append(compound)\n            i += 1\n\n\n    return several_token_propn"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    \"\"\"\n    Return dictionary of lemmata which are shared by a noun AND a verb at the same time.\n    \"\"\"\n    tokens_with_common_lemma = {}\n    # Read text from file.\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n    # Load spacy model.\n    nlp = spacy.load('en_core_web_sm')\n    # Create doc object.\n    doc = nlp(text)\n    # Iterate over the tokens.\n    for token in doc:\n        # Check if the token is a noun or a verb.\n        if token.pos_ == 'NOUN' or token.pos_ == 'VERB':\n            lemma = token.lemma_\n            text = token.text\n            # Check if the lemma is already in the dictionary.\n            if lemma in tokens_with_common_lemma:\n                # Add the token to the list of tokens with the same lemma.\n                tokens_with_common_lemma[lemma].append(token)\n            else:\n                # Add the lemma to the dictionary.\n                tokens_with_common_lemma[lemma] = [token]\n    # Lambda expression to check if a list contains at least two distinct elements.\n    distinct = lambda x: len(set(map(lambda y: y.pos_, x))) > 1\n\n    # Create new dict with only lemmata that have at least one noun AND at the same time at least one verb.\n    tokens_with_common_lemma = {lemma: tokens for lemma, tokens in tokens_with_common_lemma.items() if distinct(tokens)}\n\n\n\n\n\n    return(tokens_with_common_lemma)\n# Test the function.\nprint(common_lemma('test.txt'))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Load bbc-news.csv with pandas.\nimport pandas as pd\ncorpus = pd.read_csv('bbc-news.csv')\ncorpus.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Show amount of entries for each category in the dataset in graph.\nimport matplotlib.pyplot as plt\nprint(corpus['category'].value_counts())\ncorpus['category'].value_counts().plot(kind='bar')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus: pd.DataFrame) -> pd.DataFrame:\n    # Remove punctuation from column 'text'.\n    filtered = corpus['text'].str.replace('[^\\w\\s]','', regex=True)\n    # Remove 'text' column from corpus.\n    corpus = corpus.drop(columns=['text'])\n    # Add filtered text to corpus.\n    corpus['text'] = filtered\n    return(corpus)\n\ndef remove_numbers(corpus: pd.DataFrame) -> pd.DataFrame:\n    # Remove numbers from column 'text'.\n    filtered = corpus['text'].str.replace('\\d+', '', regex=True)\n    # Remove 'text' column from corpus.\n    corpus = corpus.drop(columns=['text'])\n    # Add filtered text to corpus.\n    corpus['text'] = filtered    \n\n    return(corpus)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\nimport tqdm\n\n# Load en_core_web_lg model.\nnlp = spacy.load('en_core_web_lg')\n\n# Remove punctuation and numbers from column 'text'.\ncorpus = remove_punctuation(corpus)\ncorpus = remove_numbers(corpus)\n\n# Split the data into training and test sets with random seed 101.\nX_train, X_test, y_train, y_test = train_test_split(corpus['text'], corpus['category'], test_size=0.3, random_state=101)\n\n# Use nlp to create turn each article to vector representation.\nX_train_vectors = []\nfor article in tqdm.tqdm(X_train):\n    X_train_vectors.append(nlp(article).vector)\n\n# Same for test set.\nX_test_vectors = []\nfor article in tqdm.tqdm(X_test):\n    X_test_vectors.append(nlp(article).vector)\n\n# Print first few vectors.\nprint(X_train_vectors[:3])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Create MLPClassifier.\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.neural_network import MLPClassifier\nimport seaborn as sn\nmlp = MLPClassifier(hidden_layer_sizes=(2048, 1024, 512), max_iter=500, learning_rate='adaptive', verbose=True)\nmlp.fit(X_train_vectors, y_train)\n# Display confusion matrix.\npredictions = mlp.predict(X_test_vectors)\ncm = confusion_matrix(y_test, predictions)\ndf_cm = pd.DataFrame(cm, index = corpus['category'].unique(),\n                  columns = corpus['category'].unique())\nplt.figure(figsize = (10,7))\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=sn.color_palette('crest'), fmt='g')\n# Add comment to bottom of the figure showing accuracy.\nplt.annotate('Accuracy: ' + str(round(mlp.score(X_test_vectors, y_test), 2)), xy=(0, -0.1), xycoords='axes fraction', fontsize=16)\n# Add title \nplt.title('MLP', fontsize=16)\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import LinearSVC\n\n# Create LinearSVC with crammer_singer as multi_class keyword\nsvc = LinearSVC(multi_class='crammer_singer', max_iter=50000000)\nsvc.fit(X_train_vectors, y_train)\n\n# Make predictions\npredictions = svc.predict(X_test_vectors)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, predictions)\n\n# Create DataFrame from confusion matrix\ndf_cm = pd.DataFrame(cm, index = corpus['category'].unique(),\n                  columns = corpus['category'].unique())\n\n# Plot confusion matrix\nplt.figure(figsize = (10,7))\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=sn.color_palette('crest'), fmt='g')\n\n# Add annotation showing accuracy\nplt.annotate('Accuracy: ' + str(round(svc.score(X_test_vectors, y_test), 2)), xy=(0, -0.1), xycoords='axes fraction', fontsize=16)\n\n# Add title\nplt.title('LinearSVC', fontsize=16)\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000, random_state=101, verbose=True)\nrf.fit(X_train_vectors, y_train)\n\n# Make predictions\npredictions = rf.predict(X_test_vectors)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, predictions)\n\n# Create DataFrame from confusion matrix\ndf_cm = pd.DataFrame(cm, index=corpus['category'].unique(),\n                     columns=corpus['category'].unique())\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 7))\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=sn.color_palette('crest'), fmt='g')\n\n# Add annotation showing accuracy\nplt.annotate('Accuracy: ' + str(round(rf.score(X_test_vectors, y_test), 2)), xy=(0, -0.1), xycoords='axes fraction', fontsize=16)\n\n# Add title\nplt.title('RandomForestClassifier', fontsize=16)"
    },
    {
        "file_name": "Assignment_3_Naman_Jain.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n\n    # load model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # read file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # process\n    doc = nlp(text)\n    \n    # extract consecutive proper nouns with more than one token\n    current_noun_phrase = []\n    for token in doc:\n        if token.pos_ == 'PROPN':\n            current_noun_phrase.append(token.text)\n        elif current_noun_phrase:\n            if len(current_noun_phrase) > 1:\n                several_token_propn.append(\" \".join(current_noun_phrase))\n            current_noun_phrase = []\n\n    if len(current_noun_phrase) > 1:\n        several_token_propn.append(\" \".join(current_noun_phrase))\n    \n    return(several_token_propn)"
    },
    {
        "file_name": "Assignment_3_Naman_Jain.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "from collections import defaultdict\n\ndef common_lemma(my_file_name):\n    # load model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # read file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # process\n    doc = nlp(text)\n\n    # create dictionaries to store lemmas and their corresponding nouns and verbs\n    noun_dict = defaultdict(set)\n    verb_dict = defaultdict(set)\n\n    # iterate through tokens\n    for token in doc:\n        # consider only nouns and verbs\n        if token.pos_ == 'NOUN':\n            noun_dict[token.lemma_].add(token.text)\n        elif token.pos_ == 'VERB':\n            verb_dict[token.lemma_].add(token.text)\n\n    # common lemmas between nouns and verbs\n    common_lemmas = set(noun_dict.keys()).intersection(verb_dict.keys())\n\n    # dictionary with lemmas and their corresponding words\n    tokens_with_common_lemma = {lemma: list(noun_dict[lemma].union(verb_dict[lemma])) for lemma in common_lemmas}\n\n    return(tokens_with_common_lemma)"
    },
    {
        "file_name": "Assignment_3_Naman_Jain.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\n# csv file\ncsv_file_path = 'bbc-news.csv'\n\n# DataFrame\ndf = pd.read_csv(csv_file_path)\n\ndf.head()"
    },
    {
        "file_name": "Assignment_3_Naman_Jain.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\ntopical_counts = df['category'].value_counts()\n\n# plot\nplt.figure(figsize=(8, 6))\nax = topical_counts.plot(kind='bar', color='skyblue')\nplt.title('Number of Articles in Each Topical Area')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\n\n# add count on top of each bar \nfor i, v in enumerate(topical_counts):\n    ax.text(i, v + 5, str(v), color='black', ha='center', va='bottom', fontsize=8)\n\nplt.show()"
    },
    {
        "file_name": "Assignment_3_Naman_Jain.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(corpus):\n    translator = str.maketrans('', '', string.punctuation)\n    cleaned_corpus = corpus.translate(translator)\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    cleaned_corpus = ''.join([char for char in corpus if not char.isdigit()])\n    return cleaned_corpus\n\n# apply\ndf['text'] = df['text'].apply(remove_punctuation)\ndf['text'] = df['text'].apply(remove_numbers)\n\ndf.head()"
    },
    {
        "file_name": "Assignment_3_Naman_Jain.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# load the model\nnlp_lg = spacy.load('en_core_web_lg')\n\n# split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(\n    df['text'], df['category'], test_size=0.3, random_state=101, shuffle=True)\n\n# convert each article to a vector representation\ndef spacy_vectorize(text):\n    doc = nlp_lg(text)\n    return doc.vector\n\nX_train_vectors = np.array([spacy_vectorize(text) for text in X_train])\nX_test_vectors = np.array([spacy_vectorize(text) for text in X_test])\n\nprint(\"Train vectors:\\n\", X_train_vectors)\nprint(\"\\nTest vectors:\\n\", X_test_vectors)"
    },
    {
        "file_name": "Assignment_3_Naman_Jain.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# models\nmodels = {\n    'MLP Classifier': MLPClassifier(random_state=101),\n    'Random Forest Classifier': RandomForestClassifier(),\n    'SVM': SVC()\n}\n\n# train\nfor model_name, model in models.items():\n    model.fit(X_train_vectors, y_train)\n\n# evaluate classifiers\nresults = {}\nfor model_name, model in models.items():\n    y_pred = model.predict(X_test_vectors)\n    accuracy = accuracy_score(y_test, y_pred)\n    confusion_mat = confusion_matrix(y_test, y_pred)\n    results[model_name] = {'accuracy': accuracy, 'confusion_matrix': confusion_mat}\n\n# results\nfor model_name, result in results.items():\n    print(f'{model_name}:\\nAccuracy: {result[\"accuracy\"]:.4f}\\nConfusion Matrix:\\n{result[\"confusion_matrix\"]}\\n')"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sinem_Dnmez.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\nfrom spacy.lang.en import English\nfrom spacy.matcher import Matcher\n\n\n#this function assumes that proper nouns start with a capital letter. one can also make use of en_core_web_sm and use the proper labels to select.\n\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n\n    nlp = English()\n    matcher = Matcher(nlp.vocab)\n\n    # Pattern to identify proper nouns (starting with a capital letter)\n    pattern = [{\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+\"}}, {\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+\"}}]\n    matcher.add(\"PROPER_NOUN\", [pattern])\n\n\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    doc = nlp(text)\n\n    matches = matcher(doc)\n    for match_id, start, end in matches:\n        span = doc[start:end]  # The matched span\n        several_token_propn.append(span.text)\n\n\n    return several_token_propn\n\nextract_proper_nouns(\"text.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sinem_Dnmez.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the file\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    # Process the text with spaCy\n    doc = nlp(text)\n\n    # Create a dictionary to track lemmas for nouns and verbs\n    lemmas_for_nouns = {}\n    lemmas_for_verbs = {}\n\n\n    for word in doc:\n        if word.pos_ == \"NOUN\":\n            noun_lemma = word.lemma_\n            lemmas_for_nouns.setdefault(noun_lemma, set()).add(word.text)\n\n        elif word.pos_ == \"VERB\":\n            verb_lemma = word.lemma_\n            lemmas_for_verbs.setdefault(verb_lemma, set()).add(word.text)\n\n    # Identifying common lemmas\n    for lemma, words in lemmas_for_nouns.items():\n        if lemma in lemmas_for_verbs:\n            tokens_with_common_lemma[lemma] = list(words.union(lemmas_for_verbs[lemma]))\n\n    return(tokens_with_common_lemma)\n\ncommon_lemma(\"corpus.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sinem_Dnmez.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\n\n# Load the dataset\n#It was named bbc-news.csv in e-campus\ndf = pd.read_csv('bbc-news.csv')"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sinem_Dnmez.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\nimport matplotlib.pyplot as plt\n\ncategory_counts = df['category'].value_counts()\n\n# Plotting\nplt.figure(figsize=(10, 6))\ncategory_counts.plot(kind='bar')\nplt.title('Number of Articles per Category')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sinem_Dnmez.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\nimport string\n\ndef remove_punctuation(corpus):\n    # Here comes your code\n    cleaned_corpus = []\n\n    for doc in corpus:\n      doc_no_punct = doc.translate(str.maketrans('', '', string.punctuation))\n      cleaned_corpus.append(doc_no_punct)\n\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    cleaned_corpus = []\n    for doc in corpus:\n        doc_no_numbers = re.sub(r'\\d+', '', doc)\n        cleaned_corpus.append(doc_no_numbers)\n\n    return(cleaned_corpus)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sinem_Dnmez.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "#google colab could not find the file\n\n!python -m spacy download en_core_web_lg\n# Here comes your code\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n\n\nnlp = spacy.load('en_core_web_lg')\n\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.3, random_state=101, shuffle=True)\n\nX_train_vectors = np.array([nlp(article).vector for article in X_train])\nX_test_vectors = np.array([nlp(article).vector for article in X_test])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sinem_Dnmez.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the models\nmlp = MLPClassifier(max_iter=1000)\nrf = RandomForestClassifier()\nsvm = SVC()\n\n# Train MLPClassifier\nmlp.fit(X_train_vectors, y_train)\ny_pred_mlp = mlp.predict(X_test_vectors)\nprint(\"MLPClassifier Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n\n# Train RandomForestClassifier\nrf.fit(X_train_vectors, y_train)\ny_pred_rf = rf.predict(X_test_vectors)\nprint(\"RandomForestClassifier Accuracy:\", accuracy_score(y_test, y_pred_rf))\n\n# Train SVM\nsvm.fit(X_train_vectors, y_train)\ny_pred_svm = svm.predict(X_test_vectors)\nprint(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import pandas as pd\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize, pos_tag\nfrom nltk.chunk import ne_chunk\nfrom nltk.tree import Tree\nimport spacy\nimport numpy as np\n# nltk.download('punkt')\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download('maxent_ne_chunker')\n# nltk.download('words')\n# !python -m spacy download en_core_web_sm\n!python -m spacy download en_core_web_lg\nnlp = spacy.load(\"en_core_web_sm\")\n \ndef extract_proper_nouns_spacy(text, nlp= nlp):\n    # Load spaCy model\n    \n    # replace all text with mr. to Mr. using regex\n    text = text.replace('mr.', 'Mr.')\n\n    # Process the input text\n    doc = nlp(text)\n\n    # Extract proper nouns with more than one token\n    proper_nouns = set()\n    current_proper_noun = []\n    \n    for token in doc:\n        normalized_token = token.text.lower()\n        \n        if token.ent_type_ in ('PERSON', 'ORG', 'GPE'):\n            # If it's a relevant named entity, add it to the current_proper_noun list\n            current_proper_noun.append(token.text)\n        elif normalized_token in ('mr.', 'ms.', 'mrs.'):\n            # If it's an honorific, add it to the current_proper_noun list\n            current_proper_noun.append(token.text)\n        elif current_proper_noun:\n            # If the previous token was part of a proper noun, add it to the list\n            if len(current_proper_noun) > 1:\n                proper_nouns.add(' '.join(current_proper_noun))\n            current_proper_noun = []\n\n    # Check for the last potential proper noun\n    if current_proper_noun and len(current_proper_noun) > 1:\n        proper_nouns.add(' '.join(current_proper_noun))\n\n    return proper_nouns\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    # here comes your code\n    with open (my_file_name, \"r\") as myfile:\n        data=myfile.read().replace('\\n', '')\n\n    # read csv file using pandas\n    df = pd.read_csv(my_file_name)\n    # remove all new lines from text column \n    \n    df['proper_nouns'] = df['text'].apply(extract_proper_nouns_spacy)\n    \n    # get all s\n    \n    # text remove all new lines\n    # text = text.replace('\\n', '')\n    # proper_nouns = extract_proper_nouns_spacy(text)\n    \n    return df['proper_nouns'].tolist()\n\nextract_proper_nouns('./bbc-news.csv')"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import nltk\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    else:\n        return None\n\ndef common_lemma(my_file_name):\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    tokens = nltk.word_tokenize(text)\n    tagged_tokens = pos_tag(tokens)\n\n    lemmatizer = WordNetLemmatizer()\n    lemma_dict = {}\n\n    for token, pos in tagged_tokens:\n        wordnet_pos = get_wordnet_pos(pos)\n        if wordnet_pos:\n            lemma = lemmatizer.lemmatize(token, pos=wordnet_pos)\n            if lemma in lemma_dict:\n                lemma_dict[lemma].append(token)\n            else:\n                lemma_dict[lemma] = [token]\n\n    # Filter only lemmas with both noun and verb forms\n    filtered_lemma_dict = {}\n    for lemma, words in lemma_dict.items():\n        pos_tags = np.array(pos_tag(words))[:,1]\n        pos_tags_set = set(pos_tags)\n        # check it pos_tags_set has at least one element with V and one with N\n        verb_set = {'VBP', 'VBZ', 'VBD', 'VBG', 'VBN', 'VB'}\n        noun_set = {'NNP', 'NNPS', 'NN', 'NNS'}\n        if len(verb_set.intersection(pos_tags_set))>0 and len(noun_set.intersection(pos_tags_set))>0 :\n            filtered_lemma_dict[lemma] = words\n             \n            \n\n        \n        \n        \n    # filtered_lemma_dict = {lemma: words for lemma, words in lemma_dict.items() if 'V' in pos_tag(words) and 'N' in pos_tag(words)}\n\n    return filtered_lemma_dict\n\n# Example usage:\nfile_name = './test.txt'\nresult = common_lemma(file_name)\nprint(result)\nimport numpy as np\na = np.array(pos_tag(['show', 'showing', 'showed']))[:,1]\nverb_set = {'VBP', 'VBZ', 'VBD', 'VBG', 'VBN', 'VB'}\nnoun_set = {'NNP', 'NNPS', 'NN', 'NNS'}\ns = set(a)\nlen(verb_set.intersection(s))>0 and len(noun_set.intersection(s))>0"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\n# a) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005. load into a dataframe \ndf = pd.read_csv('./bbc-news.csv')\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\n# a) Show how many articles we have for each topical area (class label) in the dataset using a plot.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nax = sns.countplot(x=\"category\", data=df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\ndef remove_punctuation(corpus):\n    # Here comes your code\n    # remove punctuation using regex \n    cleaned_corpus = re.sub(r'[^\\w\\s]','', corpus)\n    # remove all double spaces\n    cleaned_corpus = re.sub(r'\\s+', ' ', cleaned_corpus)\n    return cleaned_corpus.strip()\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    cleaned_corpus = re.sub(r'[0-9]','', corpus)\n    return(cleaned_corpus)\n\nprint(remove_punctuation('test, test, test:  1;! @ #'))\nprint(remove_numbers('test 1 2 3'))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# Here comes your code\n\n# We will be encoding only the first 500 documents to save time and memory\ndf_sample = df.sample(500, random_state=42)\ndf_sample.describe()\n# Load spaCy model\nlarge_model = spacy.load(\"en_core_web_lg\")\n\n# apply remove punctuation and remove numbers to text column\ndf_sample['clean_text'] = df_sample['text'].apply(remove_punctuation)\ndf_sample['clean_text'] = df_sample['clean_text'].apply(remove_numbers)\ndf_sample.head()\nprint( 'Latent Encoding Dimension: ' , df_sample.iloc[0, 3].shape)\n# use large spacy model to obtain latent vectors for each document\ndf_sample['vectors'] = df_sample['clean_text'].apply(large_model).apply(lambda x: x.vector)\n\n# test train split \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df_sample['vectors'], df_sample['category'], test_size=0.3, random_state=101)\nprint( 'train data size', x_train.shape)\nprint( 'test data size', x_test.shape)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\n# train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`) \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\n\n# Logistic Regression\nlogmodel = LogisticRegression()\nlogmodel.fit(x_train.tolist(),y_train.tolist())\n\n# Support Vector Machine\nsvcmodel = SVC()\nsvcmodel.fit(x_train.tolist(),y_train.tolist())\n\n# Multi-layer Perceptron\nmlpmodel = MLPClassifier()\nmlpmodel.fit(x_train.tolist(),y_train.tolist())\n# evaluate on test sets,report accuracy and multiclass confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Logistic Regression\npredictions = logmodel.predict(x_test.tolist())\nprint('Logistic Regression')\nprint(classification_report(y_test.tolist(),predictions))\n# multi-class confusion matrix\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test.tolist(),predictions))\n\n#visualize confusion matrix as heatmap \nplot_confusion_matrix(logmodel, x_test.tolist(), y_test.tolist())\nplt.title('Logistic Regression Confusion Matrix')\n# increase plot size \n# plt.rcParams[\"figure.figsize\"] = (20,10)\nplt.show()\n# Support Vector Machine\npredictions = svcmodel.predict(x_test.tolist())\nprint('Support Vector Machine')\nprint(classification_report(y_test.tolist(),predictions))\n# multi-class confusion matrix\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test.tolist(),predictions))\n\n#visualize confusion matrix as heatmap\nplot_confusion_matrix(svcmodel, x_test.tolist(), y_test.tolist())\nplt.title('Support Vector Machine Confusion Matrix')\n# increase plot size\n# plt.rcParams[\"figure.figsize\"] = (20,10)\nplt.show()\n# multilayer perceptron\npredictions = mlpmodel.predict(x_test.tolist())\nprint('Multi-layer Perceptron')\nprint(classification_report(y_test.tolist(),predictions))\n# multi-class confusion matrix\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test.tolist(),predictions))\n\n#visualize confusion matrix as heatmap\nplot_confusion_matrix(mlpmodel, x_test.tolist(), y_test.tolist())\nplt.title('Multi-layer Perceptron Confusion Matrix')\n# increase plot size\nplt.rcParams[\"figure.figsize\"] = (20,10)\nplt.show()\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jan_Rogge.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "nlp = spacy.load(\"en_core_web_sm\")\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    with open(my_file_name) as file:#Read in the text file\n        string = nlp(file.read())\n    \n    for i in range(len(string)):#Iterate over tokens in text\n        if string[i].pos_ == \"PROPN\":#Check if the current token is a noun\n            further_nouns = True\n            j = i\n            fused_noun = False#Fuse strings with multiple tokens together by:\n            while further_nouns:#checking if the following tokens are also proper nouns\n                j += 1\n                if string[j].pos_ == \"PROPN\":#if so, put all nouns into one string\n                    fused_noun = string[i:j+1].text\n                else:#otherwise break\n                    further_nouns = False\n                    if fused_noun:# Only add the fused noun, if it has been set with multiple tokens\n                        several_token_propn.append(fused_noun)\n                    \n\n    return(several_token_propn)\n\n#I loaded example text from above into a seperate file, as \"my_file_name\" makes it seem like a file is the expected input\nprint(extract_proper_nouns(\"exercise3_1.txt\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jan_Rogge.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "nlp = spacy.load(\"en_core_web_sm\")\ndef common_lemma(my_file_name):\n    tokens = {}\n    tokens_with_common_lemma = {}\n\n    with open(my_file_name) as file:#Read in the text file\n        string = nlp(file.read())\n\n    #step 1, put all lemmas in to a dict called tokens:\n    for token in string:\n        if token.lemma_ not in tokens.keys():#add new lemmas to the dictionary, if the lemma isnt yet in the dict\n            tokens.update({token.lemma_ : [token]})\n        else:\n            tokens[token.lemma_].append(token)#for already existing lemmas, add the word that wasnt already in the value\n\n    \n    #step 2, filter out lemmas that actually have values with both a noun and a verb in the text:\n    for k,v in tokens.items():\n        check_verb = False\n        check_noun = False\n        for word in v:#go over all words for the current lemma \n            if word.pos_ == \"VERB\":\n                check_verb = True\n\n            if word.pos_ == \"NOUN\" or word.pos_ == \"PROPN\":\n                check_noun = True\n\n        if check_noun and check_verb:#if at least one verb and at least one noun was found:\n            v = [word.text for word in v]#add the value list with the words as text now that we are done with processing\n            tokens_with_common_lemma.update({k : v})\n\n    for k,v in tokens_with_common_lemma.items():\n        v = np.unique(np.array(v)).tolist()#use numpy unique to remove duplicates in the list (as we added duplicates in step 1)\n        tokens_with_common_lemma.update({k : v})\n\n    return(tokens_with_common_lemma)\n\n#I loaded example text from above into a seperate file, as \"my_file_name\" makes it seem like a file is the expected input\ncommon_lemma(\"exercise3_2.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jan_Rogge.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\ndf = pd.read_csv(\"bbc-news.csv\")#use read_cvs to read the csv into a dataframe object\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jan_Rogge.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\ndf[\"category\"].value_counts().plot.bar()# plot a bar chart (guess this is what is asked) for how many texts each category has"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jan_Rogge.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "#Just a heads up, this takes a while to run, took 3 minutes for me\nimport string\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef remove_punctuation(corpus):\n    #take in a corpus, will be the rows of text column:\n    corpus = nlp(corpus)\n    corpus_no_punct = []\n    for token in corpus:\n        if token.text not in string.punctuation:#check if the current token is a punctuation\n            corpus_no_punct.append(token.text)#if not add it to the list\n\n    cleaned_corpus = \" \".join(corpus_no_punct)#join the list back into text\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    corpus = nlp(corpus)\n    corpus_no_number = []\n    for token in corpus:\n        if not token.like_num:#check if token is not like a number\n            corpus_no_number.append(token.text)#if so, add it to the list\n    \n    cleaned_corpus = \" \".join(corpus_no_number)#join the list back into a text\n    return(cleaned_corpus)\n\n#apply the prepocessing to the dataset\ndf[\"text\"] = df[\"text\"].apply(remove_punctuation)\ndf[\"text\"] = df[\"text\"].apply(remove_numbers)\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jan_Rogge.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# Here comes your code\nfrom sklearn.model_selection import train_test_split\n#a)\nnlp = spacy.load(\"en_core_web_lg\")\n\n#b)\n#set the data into x and y\ndf['vectors'] = df['text'].apply(nlp).apply(lambda x: x.vector)#vectorize the data first\nX = df[\"vectors\"]\ny = df[\"category\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)#use sklearns split method"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jan_Rogge.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n#fit the models to the train data:\nMLP = MLPClassifier().fit(X_train.to_list(), y_train.to_list())\n\nSVC = SVC().fit(X_train.to_list(), y_train.to_list())\n\ngnb = GaussianNB().fit(X_train.to_list(), y_train.to_list())\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n#calculate the predictions\ny_preds = MLP.predict(X_test.to_list())\n\n#calculate accuracy:\naccuracy = sklearn.metrics.accuracy_score(y_test, y_preds)\nprint(\"accuracy:\", accuracy)\n\n#compute the confusion matrix:\ncm = confusion_matrix(y_test, y_preds)\n\ndisp = ConfusionMatrixDisplay(cm, display_labels=MLP.classes_)\n\ndisp.plot()\n#calculate the predictions\ny_preds = SVC.predict(X_test.to_list())\n\n#calculate accuracy:\naccuracy = sklearn.metrics.accuracy_score(y_test, y_preds)\nprint(\"accuracy:\", accuracy)\n\n#compute the confusion matrix:\ncm = confusion_matrix(y_test, y_preds)\n\ndisp = ConfusionMatrixDisplay(cm, display_labels=SVC.classes_)\n\ndisp.plot()\n#calculate the predictions\ny_preds = gnb.predict(X_test.to_list())\n\n#calculate accuracy:\naccuracy = sklearn.metrics.accuracy_score(y_test, y_preds)\nprint(\"accuracy:\", accuracy)\n\n#compute the confusion matrix:\ncm = confusion_matrix(y_test, y_preds)\n\ndisp = ConfusionMatrixDisplay(cm, display_labels=SVC.classes_)\n\ndisp.plot()"
    },
    {
        "file_name": "Assignment_3_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\ndef extract_proper_nouns(my_file_name):\n\n    # Load spacy with english language, tokenizer etc. in small\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Readin the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Use spacy to process the text \n    doc = nlp(text)\n\n    # Extract proper nouns with more than one token\n    several_token = [ent.text for ent in doc.ents if len(ent) >= 2]\n\n    # Alternative solution using labels of the words\n    # several_token = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'PERSON', 'ORG'] and len(ent) > 1]\n\n    # Delete newlines if a proper nouns is speratet by a new line \n    several_token_propn = []\n\n    for item in several_token: \n        several_token_propn.append(item.replace(\"\\n\",\"\"))\n\n    return several_token_propn\n\n# Example:\nfile_name = \"your_file.txt\"\nresult = extract_proper_nouns(file_name)\nprint(result)"
    },
    {
        "file_name": "Assignment_3_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name):\n\n    # Load spacy with english language, tokenizer etc. in small\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Readin the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Use spacy to process the text\n    doc = nlp(text)\n\n    # Initialize a dictionary to store lemmas and their associated words\n    lemma_dict = {}\n\n    # Go through all the words (tokens) in the text \n    for token in doc:\n        # If a word (token) is both a verb and a noun, save it in the dictionary under the lemma\n        if token.pos_ in ['VERB', 'NOUN']:\n            # Get the lemma of the word (token)\n            lemma = token.lemma_\n            # Check if the lemma is already in the dictionary\n            if lemma in lemma_dict:\n                # Add the word (token) to the existing list\n                lemma_dict[lemma].append(token.text)\n            else:\n                # Create a new list with the word\n                lemma_dict[lemma] = [token.text]\n\n    # Filter out lemmas with only one associated word\n    tokens_with_common_lemma = {lemma: words for lemma, words in lemma_dict.items() if len(set(words)) > 1}\n\n    return tokens_with_common_lemma\n\n# Example:\nfile_name = \"your_file_task2.txt\"\nresult = common_lemma(file_name)\nprint(result)"
    },
    {
        "file_name": "Assignment_3_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "#Task a) Load data in a dataframe\nimport pandas as pd\n\n# Import csv in a dataframe\ndf = pd.read_csv(\"bbc-news.csv\")\n\n# Print dataframe\ndisplay(df)"
    },
    {
        "file_name": "Assignment_3_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Count the number of articles for each topic\narticle_counts = df['category'].value_counts()\n\n# Print dataframe\ndisplay(article_counts)\n\n# Print dataframe \nax = article_counts.plot.bar()\nax.set_ylabel('Number of articles')\n\n# Add the number of articles above the bar \nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')"
    },
    {
        "file_name": "Assignment_3_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\n# Remove punctuation from the given text.\ndef remove_punctuation(corpus):\n    return corpus.translate(str.maketrans('', '', string.punctuation))\n\n# Remove numbers from the given text\ndef remove_numbers(corpus):\n    return ''.join([char for char in corpus if not char.isdigit()])\n\n# Apply the functions to the \"Text\" column of the data frame with the content of the CSV file to remove punctuation and numbers\ndf['text'] = df['text'].apply(remove_punctuation)\ndf['text'] = df['text'].apply(remove_numbers)\n\ndisplay(df)"
    },
    {
        "file_name": "Assignment_3_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "#Note: My Intel i5-8250u 4c worked great in converting the articles to vectors, but i would like to see how long my Ryzen 5700u 8c would need :)\nimport spacy\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Task a) Load spacy with english language, tokenizer etc. in large\nnlp = spacy.load('en_core_web_lg')\n\n# Task b) Split the data into training and test sets\ntrain_data, test_data = train_test_split(df, test_size=0.3, random_state=101)\n\n# Task c) Convert each article into a vector representation using spacy -> spacy performs tokenization, lemmatization, etc. -> Result for each article we get a vector with dimension 300 \ndef convert_to_vectors(texts):\n    vectors = []\n    for doc in nlp.pipe(texts, disable=['parser', 'tagger', 'ner']):\n        vectors.append(doc.vector)\n    return np.array(vectors)\n\n# Convert training data to vectors\nX_train = convert_to_vectors(train_data['text'])\n\n# Convert test data to vectors\nX_test = convert_to_vectors(test_data['text'])\n\n# Print the shape of the resulting arrays\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)"
    },
    {
        "file_name": "Assignment_3_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define the three classifiers\nclassifiers = {\n    #'Logistic Regression': LogisticRegression(max_iter=1000), -> Works only for small spacy vectors \n    'LinearSVC': LinearSVC(),\n    'Random Forest': RandomForestClassifier(random_state=101),\n    'MLP Classifier': MLPClassifier(random_state=101, max_iter=1000)\n}\n\n# Train and evaluate each classifier\nfor name, classifier in classifiers.items():\n    # Train the model\n    classifier.fit(X_train, train_data['category'])\n\n    # Make predictions on the test set\n    predictions = classifier.predict(X_test)\n\n    # Evaluate accuracy\n    accuracy = accuracy_score(test_data['category'], predictions)\n    print(f\"\\n{name} Classifier Evaluation:\")\n    print(f\"Accuracy: {accuracy}\")\n\n    # Print classification report\n    classification_rep = classification_report(test_data['category'], predictions)\n    print(\"Classification Report:\\n\", classification_rep)\n\n    # Print confusion matrix\n    cm = confusion_matrix(test_data['category'], predictions)\n    print(\"Confusion Matrix:\\n\", cm)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\ndef extract_proper_nouns(my_file_name):\n    # Load the spaCy English model\n    nlp = spacy.load(\"en_core_web_sm\")\n    # Read the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n    \n    doc = nlp(text)\n    # Extract proper nouns with more than one token\n    several_token_propn = []\n    current_proper_noun = ''\n    \n    for token in doc:\n        if token.pos_ == 'PROPN':\n            current_proper_noun += token.text + ' '\n        else:\n            if (len(current_proper_noun.strip().split()) > 1):  # Check if the current_proper_noun is not empty\n                several_token_propn.append(current_proper_noun.strip())\n            current_proper_noun = ''\n                \n    return several_token_propn\nprint(extract_proper_nouns(\"task1.txt\"))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    # here comes your code\n    # Load the spaCy English model\n    nlp = spacy.load(\"en_core_web_sm\")\n    # Read the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n    \n    doc = nlp(text)\n    \n    for token in doc:\n        if (token.lemma_ not in tokens_with_common_lemma.keys()):    \n            tokens_with_common_lemma[token.lemma_] = []\n        \n        if(token.pos_ in ['NOUN','VERB']):\n            tokens_with_common_lemma[token.lemma_].append(token)\n        \n        \n    return tokens_with_common_lemma\nprint(common_lemma(\"task2_1.txt\"))\nprint(common_lemma(\"task2_2.txt\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\nimport matplotlib.pyplot as plt\nbbc_df = pd.read_csv(\"bbc-news.csv\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "category_counts = bbc_df['category'].value_counts()\n# Bar chart\nplt.figure(figsize=(10, 6))\n#category_counts.plot(kind='bar', color='skyblue')\ncategories = category_counts.index\ncounts = category_counts.values\nplt.bar(categories, counts, color='skyblue')\nplt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\nplt.title('Category Counts')\nplt.xlabel('Category')\nplt.ylabel('Count')"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\ndef remove_punctuation(corpus):\n    cleaned_corpus = corpus.copy()\n    cleaned_corpus['text'] = cleaned_corpus['text'].apply(lambda x: ''.join(char for char in x if char not in string.punctuation))\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    cleaned_corpus = corpus.copy()\n    cleaned_corpus['text'] = cleaned_corpus['text'].apply(lambda x: ''.join(char for char in x if not char.isdigit()))\n    return(cleaned_corpus)\ncleaned1 = remove_punctuation(bbc_df)\nbbc_df.loc[12, 'text']\ncleaned1.loc[12, 'text']\ncleaned2 = remove_numbers(bbc_df)\nbbc_df.loc[12, 'text']\ncleaned2.loc[12, 'text']\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\nnlp = spacy.load(\"en_core_web_sm\")\ntext_train, text_test, category_train, category_test = train_test_split(bbc_df['text'], bbc_df['category'], train_size=0.7, random_state=101, shuffle=True)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Before",
        "answer": "text_train\ntext_test"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Conversion",
        "answer": "# Our model needs to implement vector() method for token - Okay\n# Each token has its vector representation defined by model - this is word/token embedding\n# Embedding of a document (Doc) is nothing but average of all its token.vector values\ntext_train = text_train.apply(lambda text: nlp(text).vector)\ntext_test = text_test.apply(lambda text: nlp(text).vector)\n\n# Should I convert category (i.e. single token) to vector representation as well? I think no, bcs makes no sense for labels"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### After",
        "answer": "# Now text_train and text_test are converted to vector representation with shape (n,)\ntext_train\ntext_test"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Note: For this specific model (eng_core_web_sm), word embedding is vector of length (96,)\n### Note2: I did not run large model (eng_core_web_sm). But as I remember, word embedding for that model is vector of length (300,)",
        "answer": "len(text_train.loc[920])\nlen(text_test.loc[2148])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Faris_Hajdarpasic.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nclassifier_1 = KNeighborsClassifier()\nclassifier_2 = SVC()\nclassifier_3 = RandomForestClassifier()\nclassifier_4 = MLPClassifier()\ndef calculate_accuracy(predicted_labels, gt_labels):\n    correct_predictions = np.sum(predicted_labels == gt_labels)\n    total_samples = len(gt_labels)\n    accuracy = correct_predictions / total_samples\n    return accuracy\n# Encode categories to nfrom string to numbers\n# shape: (num_examples,)\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(category_train)\ncategory_train = label_encoder.transform(category_train)\ncategory_test = label_encoder.transform(category_test)\ncategory_train.shape\n# Make input to shape (num_examples, num_features)\ntext_train = np.vstack(text_train.to_numpy())\ntext_test = np.vstack(text_test.to_numpy())\ntext_train.shape\nclassifier_1.fit(text_train, category_train);\npredicted_label_test_1 = classifier_1.predict(text_test)\naccuracy_1 = calculate_accuracy(predicted_label_test_1, category_test)\nclassifier_2.fit(text_train, category_train);\npredicted_label_test_2 = classifier_2.predict(text_test)\naccuracy_2 = calculate_accuracy(predicted_label_test_2, category_test)\nclassifier_3.fit(text_train, category_train);\npredicted_label_test_3 = classifier_3.predict(text_test)\naccuracy_3 = calculate_accuracy(predicted_label_test_3, category_test)\nclassifier_4.fit(text_train, category_train);\npredicted_label_test_4 = classifier_4.predict(text_test)\naccuracy_4 = calculate_accuracy(predicted_label_test_4, category_test)\nprint(\"Accuracy of the KNeighborsClassifier: \" + str(accuracy_1))\nprint(\"Accuracy of the SVC: \" + str(accuracy_2))\nprint(\"Accuracy of the RandomForestClassifier: \" + str(accuracy_3))\nprint(\"Accuracy of the MLPClassifier: \" + str(accuracy_4))\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Gjergj_Plepi.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import numpy as np\nimport spacy\nfrom itertools import groupby\nfrom operator import itemgetter\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    # here comes your code\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    with open(my_file_name, 'r', encoding='utf-8') as my_file: #read through the file line by line\n        for text in my_file:\n            \n            doc = nlp(text)\n\n            proper_nouns_tokens_indices = [] #stores the indices of all proper nouns\n\n            for token in doc:\n                if token.pos_ == 'PROPN':\n                    proper_nouns_tokens_indices.append(token.i)\n            \n            \"\"\"Group consecutive indices\"\"\"\n            for _, group in groupby(enumerate(proper_nouns_tokens_indices), lambda element : element[0] - element[1]):\n                consecutive_indices = list(map(itemgetter(1), group))\n                s = \"\"\n                \n                if len(consecutive_indices) > 1: # if the proper noun has more than 1 token\n                    for i in consecutive_indices:\n                        s = s + doc[i].text + \" \"\n\n                    several_token_propn.append(s[:-1]) #delete the last \" \" and store it\n    \n\n    return(several_token_propn)\nseveral_token_propn = extract_proper_nouns(my_file_name='task1_text.txt')\nprint(several_token_propn)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Gjergj_Plepi.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    # here comes your code\n\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    lemmas = {} #stores all the lemmas in the file\n\n    \"\"\"Build the lemmas dictionary\"\"\"\n    with open(my_file_name, 'r', encoding='utf-8') as my_file: #read through the file line by line\n        for text in my_file:\n    \n            doc = nlp(text)\n\n            \"\"\"Store in the lemmas dict all the lemmas as a key, and as a value for each key, store a list of tokens that have that lemma\"\"\"\n            for token in doc:\n                if token.lemma_ not in lemmas.keys():\n                    lemmas[token.lemma_] = [] #init an empty list as the value for the newly added lemma\n                    \n                lemmas[token.lemma_].append(token) #append the token to the list\n            \n\n    for key in lemmas.keys():\n        contains_noun = False\n        contains_verb = False\n        for token in lemmas[key]: #go to all tokens that have the same lemma (key)\n            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\": #if a token it is a Noun\n                contains_noun = True\n            elif token.pos_ == \"VERB\": #if a token is a verb\n                contains_verb = True\n        \n        if contains_noun and contains_verb: #if at least a verb and a noun have the same lemma, add it to tokens_with_common_lemma\n            tokens_with_common_lemma[key] = lemmas[key]\n    \n\n    return(tokens_with_common_lemma)\ntokens_with_common_lemma = common_lemma(my_file_name='task2_text.txt')\nprint(tokens_with_common_lemma)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Gjergj_Plepi.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\n\ndf = pd.read_csv('bbc-news.csv')\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Gjergj_Plepi.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\n\n# labels = df[\"category\"].unique() # labels\n# print(labels)\n\noccurrences_of_labels = df[\"category\"].value_counts()\nprint(occurrences_of_labels)\n#print(sum(df[\"category\"].value_counts())) #sanity check, equals 2225\n\n\"\"\"Plot\"\"\"\noccurrences_of_labels.plot(kind = 'bar')"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Gjergj_Plepi.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(corpus):\n    # Here comes your code\n    \"\"\"\n    :corpus : a dataframe with two columns: 'category' and 'text'\n    \"\"\"\n    cleaned_corpus = pd.DataFrame()\n\n    for column in corpus.columns:\n        cleaned_corpus[column] = corpus[column].str.replace('[{}]'.format(string.punctuation), '', regex=True) # replace any punctuation defined in string.punctuation with ''\n    \n    return(cleaned_corpus)\n\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    cleaned_corpus = pd.DataFrame()\n\n    for column in corpus.columns:\n        cleaned_corpus[column] = corpus[column].str.replace('\\d+', '', regex=True) # replace any number with ''\n\n    return(cleaned_corpus)\ncorpus_no_punctuations = remove_punctuation(df)\ncorpus_preprocessed = remove_numbers(corpus_no_punctuations) #no punctuations and no numbers\ndf['text'][58]\ncorpus_no_punctuations['text'][58]\ncorpus_preprocessed['text'][58]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Gjergj_Plepi.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\n# Here comes your code\n\"\"\"Load the large model trained on the web text\"\"\"\nnlp = spacy.load(\"en_core_web_lg\")\n\n\"\"\"Dataset splitting\"\"\"\ntext_train, text_test, label_train, label_test = train_test_split(corpus_preprocessed['text'], corpus_preprocessed['category'], \n                                                                  test_size=0.3, \n                                                                  random_state=101, shuffle=True)\n\ntext_train_vectorized = []\ntext_test_vectorized = []\n\n\"\"\"Store each article/text as a vector\"\"\" \nfor element in text_train.iteritems(): \n    text_train_vectorized.append(nlp(element[1]).vector) #vector is the mean of the vectors of all the words in the document.\n\nfor element in text_test.iteritems():\n    text_test_vectorized.append(nlp(element[1]).vector) #vector is the mean of the vectors of all the words in the document.\ntext_train_vectorized = np.array(text_train_vectorized)\ntext_test_vectorized = np.array(text_test_vectorized)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Gjergj_Plepi.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn import svm, naive_bayes, neural_network\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\"\"\"Initialize the classifier\"\"\"\nnaive_bayes_classifier = naive_bayes.GaussianNB()\n\"\"\"Train the classifier\"\"\"\nnaive_bayes_classifier.fit(text_train_vectorized, label_train)\n\"\"\"Test the classifier\"\"\"\npredictions = naive_bayes_classifier.predict(text_test_vectorized)\n\"\"\"Evaluate the model\"\"\"\n#accuracy\ncorrect_answers = np.sum(np.equal(predictions, label_test))\naccuracy = correct_answers / (len(predictions)) * 100\n\n#confussion matrix\nlabels = corpus_preprocessed[\"category\"].unique()\nconf_matrix = confusion_matrix(label_test.values, predictions, labels=labels)\n\nprint(\"Accuracy: \", accuracy)\nprint(\"Labels:\", labels)\nconf_matrix\ndisp = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\ndisp.plot()\n\"\"\"Initialize the classifier\"\"\"\nsvm_classifier = svm.LinearSVC()\n\"\"\"Train the classifier\"\"\"\nsvm_classifier.fit(text_train_vectorized, label_train)\n\"\"\"Test the classifier\"\"\"\npredictions = svm_classifier.predict(text_test_vectorized)\n\"\"\"Evaluate the model\"\"\"\n#accuracy\ncorrect_answers = np.sum(np.equal(predictions, label_test))\naccuracy = correct_answers / (len(predictions)) * 100\n\n#confussion matrix\nlabels = corpus_preprocessed[\"category\"].unique()\nconf_matrix = confusion_matrix(label_test.values, predictions, labels=labels)\n\nprint(\"Accuracy: \", accuracy)\nprint(\"Labels:\", labels)\nconf_matrix\ndisp = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\ndisp.plot()\n\"\"\"Initialize the classifier\"\"\"\nmlp_classifier = neural_network.MLPClassifier()\n\"\"\"Train the classifier\"\"\"\nmlp_classifier.fit(text_train_vectorized, label_train)\n\"\"\"Test the classifier\"\"\"\npredictions = mlp_classifier.predict(text_test_vectorized)\n\"\"\"Evaluate the model\"\"\"\n#accuracy\ncorrect_answers = np.sum(np.equal(predictions, label_test))\naccuracy = correct_answers / (len(predictions)) * 100\n\n#confussion matrix\nlabels = corpus_preprocessed[\"category\"].unique()\nconf_matrix = confusion_matrix(label_test.values, predictions, labels=labels)\n\nprint(\"Accuracy: \", accuracy)\nprint(\"Labels:\", labels)\nconf_matrix\ndisp = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\ndisp.plot()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Vella_Rosario.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\nfrom spacy.matcher import Matcher\n\ndef extract_proper_nouns_from_txt(my_file_name):\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Use matcher\n    matcher = Matcher(nlp.vocab)\n    pattern = [\n        [\n            {'POS': 'PROPN', \"OP\": \"!\"},\n            {'POS': 'PROPN', \"DEP\": \"compound\", \"OP\": \"+\"}, \n            {'POS': 'PROPN'},\n            {'POS': 'PROPN', \"OP\": \"!\"},\n        ]\n    ]\n    matcher.add('multiPropn', pattern)\n\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    doc = nlp(text)\n    matches = matcher(doc)\n\n    several_token_propn = [doc[start+1:end-1].text for match_id, start, end in matches]\n\n    return several_token_propn\n\ntxt_file_path = \"my_file_name.txt\"  #\nresult = extract_proper_nouns_from_txt(txt_file_path)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Vella_Rosario.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name):\n    # Load the spaCy English model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the content of the file\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    doc = nlp(text)\n\n    tokens_with_common_lemma = {}\n\n    # Iterate through tokens\n    for token in doc:\n        # Check if the token is a verb or a noun\n        if token.pos_ in ['VERB', 'NOUN']:\n            \n            lemma = token.lemma_\n            # If the lemma is not in the dictionary, add it with an empty list\n            if lemma not in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma] = []\n            # Add the token text to the corresponding lemma's list\n            tokens_with_common_lemma[lemma].append(token.text)\n\n    return tokens_with_common_lemma\n\n\nwith open('sample_text.txt', 'w') as sample_file:\n    sample_file.write(\"text = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!.\")\n\nresult = common_lemma('sample_text.txt')\n\nprint('Output:', result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Vella_Rosario.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\ncsv_file_path = \"bbc-news.csv\"\n\n# Load the data into a DataFrame\ndf = pd.read_csv(csv_file_path)\n\nprint(df.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Vella_Rosario.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ncsv_file_path = \"bbc-news.csv\"\n\n# Load the data into a DataFrame\ndf = pd.read_csv(csv_file_path)\n\n# Count the number of articles for each topical area (class label)\nlabel_counts = df['category'].value_counts()\n\n# Plot the distribution\nplt.figure(figsize=(10, 6))\nlabel_counts.plot(kind='bar', color='skyblue')\nplt.title('Number of Articles for Each Topical Area')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Vella_Rosario.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(corpus):\n    translator = str.maketrans('', '', string.punctuation)\n    cleaned_corpus = [text.translate(translator) for text in corpus]\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    cleaned_corpus = [''.join(char for char in text if not char.isdigit()) for text in corpus]\n    return cleaned_corpus"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Vella_Rosario.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# Here comes your code\n\nimport spacy\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the large spaCy model trained on the web text\nnlp = spacy.load(\"en_core_web_lg\")\n\n# a) Load the dataset\ncsv_file_path = \"bbc-news.csv\"\ndf = pd.read_csv(csv_file_path)\n\n\n# b) Split the data into training and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=101, shuffle=True)\n\n# c) Convert each article to a vector representation using the pre-trained spaCy model\ntrain_vectors = [nlp(text).vector for text in train_df['text']]\ntest_vectors = [nlp(text).vector for text in test_df['text']]\n\ntrain_vectors = pd.DataFrame(train_vectors).to_numpy()\ntest_vectors = pd.DataFrame(test_vectors).to_numpy()\n\nprint(\"Train Vectors Shape:\", train_vectors.shape)\nprint(\"Test Vectors Shape:\", test_vectors.shape)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Vella_Rosario.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Extracting labels from the training and test sets\ntrain_labels = train_df['category']\ntest_labels = test_df['category']\n\n# a) Train three different models\n# Model 1: Decision Tree Classifier\ndt_model = DecisionTreeClassifier(random_state=101)\ndt_model.fit(train_vectors, train_labels)\n\n# Model 2: Random Forest Classifier\nrf_model = RandomForestClassifier(random_state=101)\nrf_model.fit(train_vectors, train_labels)\n\n# Model 3: Multi-layer Perceptron (MLP) Classifier\nmlp_model = MLPClassifier(random_state=101, max_iter=500)\nmlp_model.fit(train_vectors, train_labels)\n\n# b) Evaluate the classifiers on the test set\n# Prediction for each model\ndt_predictions = dt_model.predict(test_vectors)\nrf_predictions = rf_model.predict(test_vectors)\nmlp_predictions = mlp_model.predict(test_vectors)\n\n# Accuracy Scores\ndt_accuracy = accuracy_score(test_labels, dt_predictions)\nrf_accuracy = accuracy_score(test_labels, rf_predictions)\nmlp_accuracy = accuracy_score(test_labels, mlp_predictions)\n\n# Confusion Matrices\ndt_conf_matrix = confusion_matrix(test_labels, dt_predictions)\nrf_conf_matrix = confusion_matrix(test_labels, rf_predictions)\nmlp_conf_matrix = confusion_matrix(test_labels, mlp_predictions)\n\n# Plot the confusion matrices\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nsns.heatmap(dt_conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[0])\naxes[0].set_title('Decision Tree Confusion Matrix')\n\nsns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1])\naxes[1].set_title('Random Forest Confusion Matrix')\n\nsns.heatmap(mlp_conf_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[2])\naxes[2].set_title('MLP Confusion Matrix')\n\nplt.show()\n\nprint(\"Decision Tree Accuracy:\", dt_accuracy)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\nprint(\"MLP Accuracy:\", mlp_accuracy)"
    },
    {
        "file_name": "Assignment_3_Ali_Sehran.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**|",
        "answer": "!python -m spacy download en_core_web_sm\nimport spacy\n\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    # Load the spaCy model\n    eng_nlp = spacy.load('en_core_web_sm')\n    \n    with open(my_file_name, 'r') as file:\n        file = eng_nlp(file.read())\n    # Extract the proper nouns with more than one token\n    several_token_propn = [word.text for word in file.ents if len(word.text.split()) > 1]\n    \n    return(several_token_propn)\n# Testing\n#extract_proper_nouns(\"./test1.txt\")"
    },
    {
        "file_name": "Assignment_3_Ali_Sehran.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    \n    eng_nlp = spacy.load('en_core_web_sm')\n    \n    with open(my_file_name, 'r') as file:\n        file = eng_nlp(file.read())\n    \n    # Create a dictionary of lemmas\n    lemma_dict = {}\n    for token in file:\n        if token.pos_ in ['NOUN', 'VERB']:\n            #print(token.text, token.lemma_, token.pos_)\n            # If the lemma is not in the dictionary, add it\n            if token.lemma_ not in lemma_dict:\n                lemma_dict[token.lemma_] = [(token.text, token.pos_)]\n            # If the lemma is in the dictionary, but the token is not, add it\n            elif (token.text, token.pos_) not in lemma_dict[token.lemma_]:\n                lemma_dict[token.lemma_].append((token.text, token.pos_))\n\n    # Create a dictionary of tokens with common lemmas - only if there are more than one POS tag (noun or verb)\n    tokens_with_common_lemma = {key: [word for word, pos in value if len(set([pos for word, pos in value])) > 1]\n                                for key, value in lemma_dict.items() if len([word for word, pos in value \n                                if len(set([pos for word, pos in value])) > 1]) > 1}\n    return(tokens_with_common_lemma)\n# Testing\n#common_lemma(\"./test2.txt\")"
    },
    {
        "file_name": "Assignment_3_Ali_Sehran.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\nfile_path = './bbc-news.csv'\n\ndf = pd.read_csv(file_path)"
    },
    {
        "file_name": "Assignment_3_Ali_Sehran.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\nclass_counts = df['category'].value_counts()\n\n# Create a bar plot\nplt.figure(figsize=(10,6))\nclass_counts.plot(kind='bar')\nplt.xlabel('Category')\nplt.ylabel('Number of Articles')\nplt.title('Number of Articles for Class Label')\nplt.show()"
    },
    {
        "file_name": "Assignment_3_Ali_Sehran.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\nimport re\n\ndef remove_punctuation(corpus):\n    translator = str.maketrans('', '', string.punctuation)\n    return([doc.translate(translator) for doc in corpus])\n\ndef remove_numbers(corpus):\n    # Remove numbers - \\d+ means one or more digits from 0 to 9\n    return([re.sub(r'\\d+', '', doc) for doc in corpus])\n\n\n# Applying to the datasetr\ndf['text'] = remove_punctuation(df['text'])\ndf['text'] = remove_numbers(df['text'])"
    },
    {
        "file_name": "Assignment_3_Ali_Sehran.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\n\nnlp = spacy.load('en_core_web_sm')\n\n\n# Covert to vector representation\ndf['vectors'] = df['text'].apply(lambda x: nlp(x).vector)\n\n\n# Split the data into training and test sets\ndf_train, df_test = train_test_split(df , test_size=0.3, random_state=101, shuffle=True)\n\n# Create a list of the vectors\ntrain_vectors = df_train['vectors'].to_list()\ntest_vectors = df_test['vectors'].to_list()"
    },
    {
        "file_name": "Assignment_3_Ali_Sehran.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Create a list of the labels\ntrain_labels = df_train['category']\ntest_labels = df_test['category']\n\n# # Initialize the classifiers\n# mlp = MLPClassifier()\n# rf = RandomForestClassifier()\n# svc = SVC()\nmlp = MLPClassifier(random_state=1,  max_iter=300)\nrf = RandomForestClassifier(random_state=1)\nsvc = SVC(random_state=101)\n\n\n# Train the classifiers\nmlp.fit(train_vectors, train_labels)\nrf.fit(train_vectors, train_labels)\nsvc.fit(train_vectors, train_labels)\n\n# Predict the class labels for the test set\nmlp_preds = mlp.predict(test_vectors)\nrf_preds = rf.predict(test_vectors)\nsvc_preds = svc.predict(test_vectors)\n\n# Calculate the accuracy of the classifiers\nmlp_accuracy = accuracy_score(test_labels, mlp_preds)\nrf_accuracy = accuracy_score(test_labels, rf_preds)\nsvc_accuracy = accuracy_score(test_labels, svc_preds)\n\n# Print the accuracy of the classifiers\nprint(f'MLPClassifier Accuracy: {mlp_accuracy}')\nprint(f'RandomForestClassifier Accuracy: {rf_accuracy}')\nprint(f'SVC Accuracy: {svc_accuracy}')\n\n# Calculate and print the confusion matrix for the classifiers\nmlp_cm = confusion_matrix(test_labels, mlp_preds)\nrf_cm = confusion_matrix(test_labels, rf_preds)\nsvc_cm = confusion_matrix(test_labels, svc_preds)\n\nprint(f'MLPClassifier Confusion Matrix:\\n {mlp_cm}')\nprint(f'RandomForestClassifier Confusion Matrix:\\n {rf_cm}')\nprint(f'SVC Confusion Matrix:\\n {svc_cm}')\n"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "!pip install spacy\n!python -m spacy download en_core_web_sm\nimport spacy\n\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n\n    # Load the spaCy model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the file\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    # Process the text\n    doc = nlp(text)\n\n    # Extract proper nouns with more than one token\n    several_token_propn = [ent.text for ent in doc.ents if ' ' in ent.text]\n\n    return several_token_propn\n# Example usage\nproper_nouns = extract_proper_nouns(\"my_file.txt\")\nprint(proper_nouns)"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n\n    # Load English tokenizer, tagger, parser, NER and word vectors\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the file content\n    with open(my_file_name, \"r\") as file:\n        text = file.read()\n\n    # Process the text\n    doc = nlp(text)\n\n    # Dictionary to store lemmas and their corresponding words\n    lemma_dict = {}\n    # Dictionary to store the POS tags for each lemma\n    lemma_pos = {}\n\n    for token in doc:\n        # Only consider verbs and nouns\n        if token.pos_ in [\"VERB\", \"NOUN\"]:\n            lemma = token.lemma_\n            # Add the token text to the lemma's list\n            lemma_dict.setdefault(lemma, set()).add(token.text)\n            # Record the POS tag of this lemma\n            lemma_pos.setdefault(lemma, set()).add(token.pos_)\n\n    # Filter out lemmas that are not shared between verbs and nouns\n    tokens_with_common_lemma = {lemma: list(words) for lemma, words in lemma_dict.items() if \"VERB\" in lemma_pos[lemma] and \"NOUN\" in lemma_pos[lemma]}\n\n    return tokens_with_common_lemma\n# Example usage\nlemmas = common_lemma(\"my_file_2.txt\")\nprint(lemmas)"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\n# Load the dataset\nfile_path = 'bbc-news.csv'\nbbc_data = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset\nprint(bbc_data.head())"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\n# Counting the number of articles for each category\ncategory_counts = bbc_data['category'].value_counts()\n\n# Plotting the distribution of articles in each category\nplt.figure(figsize=(10, 6))\ncategory_counts.plot(kind='bar')\nplt.title('Number of Articles per Category in BBC News Dataset')\nplt.xlabel('Category')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.show()"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(corpus):\n    # Create a translation table for removing punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    # Apply the translation table to remove punctuation\n    cleaned_corpus = corpus.translate(translator)\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    # Remove digits using a translation table\n    translator = str.maketrans('', '', string.digits)\n    # Apply the translation table to remove numbers\n    cleaned_corpus = corpus.translate(translator)\n    return cleaned_corpus\n\ndata = pd.read_csv('bbc-news.csv')\n\n# Apply the functions\ndata['text'] = data['text'].apply(remove_punctuation).apply(remove_numbers)\n\n# Optional: Export the cleaned data\ndata.to_csv('NEW-bbc-news.csv', index=False)  # Export to a new CSV file"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "!python -m spacy download en_core_web_lg\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport numpy as np\n\n#Load spaCy large model\nnlp = spacy.load('en_core_web_lg')\n\n#Load the dataset\ndata = pd.read_csv('NEW-bbc-news.csv')\n\n# Split the data into training and test set (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(data['text'], data['category'],\n                                                    test_size=0.3, random_state=101, shuffle=True)\n\n#Convert articles to vector representation\ndef text_to_vector(text):\n    doc = nlp(text)\n    return doc.vector\n\nX_train_vectors = np.array([text_to_vector(text) for text in X_train])\nX_test_vectors = np.array([text_to_vector(text) for text in X_test])\nX_train_vectors, X_test_vectors"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Model 1: MLPClassifier\nmlp = MLPClassifier(random_state=1)\nmlp.fit(X_train_vectors, y_train)\n\n# Model 2: RandomForestClassifier\nrf = RandomForestClassifier(random_state=1)\nrf.fit(X_train_vectors, y_train)\n\n# Model 3: Support Vector Machine Classifier\nsvc = SVC(random_state=1)\nsvc.fit(X_train_vectors, y_train)\n\ndef evaluate_model(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    conf_matrix = confusion_matrix(y_test, predictions)\n    return accuracy, conf_matrix\n\nmlp_accuracy, mlp_conf_matrix = evaluate_model(mlp, X_test_vectors, y_test)\nrf_accuracy, rf_conf_matrix = evaluate_model(rf, X_test_vectors, y_test)\nsvc_accuracy, svc_conf_matrix = evaluate_model(svc, X_test_vectors, y_test)\n\n# Print the results\nprint(f\"MLPClassifier Accuracy: {mlp_accuracy}\\nConfusion Matrix:\\n{mlp_conf_matrix}\\n\")\nprint(f\"RandomForestClassifier Accuracy: {rf_accuracy}\\nConfusion Matrix:\\n{rf_conf_matrix}\\n\")\nprint(f\"SVC Accuracy: {svc_accuracy}\\nConfusion Matrix:\\n{svc_conf_matrix}\\n\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Weiberg.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    nlp = spacy.load(\"en_core_web_sm\")\n    with open(my_file_name, \"r\") as f:\n        for line in f.readlines():\n            doc = nlp(line.strip())\n            for i in range(len(doc)):\n                if doc[i].pos_ == \"PROPN\" and doc[i+1].pos_ == \"PROPN\":     #if we have two proper nouns in a row, add them\n                    several_token_propn.append(str(doc[i]) + \" \" + str(doc[i+1]))\n    return(several_token_propn)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Weiberg.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    nlp = spacy.load(\"en_core_web_sm\")\n    # Two dicts to save found nouns and verbs and their lemma\n    verbdict = {}\n    noundict = {}\n    with open(my_file_name, \"r\") as f:\n        for line in f.readlines():\n            doc = nlp(line.strip())\n            for token in doc:\n                if token.pos_==\"NOUN\":\n                    noundict.setdefault(token.lemma_, []).append(str(token))\n                if token.pos_==\"VERB\":\n                    verbdict.setdefault(token.lemma_, []).append(str(token))\n    for k in verbdict.keys() & noundict.keys(): #Get common keys (lemmas)\n        tokens_with_common_lemma[k] = verbdict[k] + noundict[k] #Merge lists\n    return(tokens_with_common_lemma)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Weiberg.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\ndata = pd.read_csv(\"bbc-news.csv\")\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Weiberg.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "from collections import Counter\nimport matplotlib.pyplot as plt\n\nc = Counter(data[\"category\"])\nplt.bar(c.keys(), c.values())\nplt.xlabel(\"Category\")\nplt.ylabel(\"Absolute frequency\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Weiberg.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "nlplite = spacy.load(\"en_core_web_sm\", disable = [\"tok2vec\", \"tagger\", \"parser\", \"lemmatizer\", \"ner\", \"attribute_ruler\"])\nprint(nlplite.pipeline)\n\ndef remove_punctuation(corpus):\n    cleaned_corpus = \" \".join(str(token) for token in nlplite(corpus) if not token.is_punct)\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    cleaned_corpus = \" \".join(str(token) for token in nlplite(corpus) if not token.like_num)\n    return(cleaned_corpus)\n\ndata[\"text\"] = data[\"text\"].apply(remove_punctuation)\ndata[\"text\"] = data[\"text\"].apply(remove_numbers)\n\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Weiberg.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "#a\nnlp = spacy.load(\"en_core_web_lg\")\n\ndata = data.head(1000) #Only use first 1000 rows\ndef reduce_tokens(corpus):      #Only returns the first 150 tokens of corpus\n    tokens = nlplite(corpus)\n    reduced_tokens = \" \".join(str(t) for t in tokens[:150])\n    return reduced_tokens\ndata[\"text\"] = data[\"text\"].apply(reduce_tokens)        #Only use first 150 tokens for each row\n\n#c          #Since we need to do this for train and test I thought it makes more sense to do it before b\ndata[\"vector\"] = data[\"text\"].apply(lambda x: nlp(x).vector)\ndata = data.drop([\"text\"], axis=1)\n\n#b\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(101)\ntrain, test = train_test_split(data, train_size=0.7, test_size=0.3, shuffle=True)\nprint(train)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Weiberg.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nmlp = MLPClassifier(max_iter=10000)\nmlp.fit(train[\"vector\"].tolist(), train[\"category\"])\nguess = mlp.predict(test[\"vector\"].tolist())\nprint(\"MLP:\")\nprint(\"Accuracy:\", accuracy_score(test[\"category\"], guess))\nprint(confusion_matrix(test[\"category\"], guess))\nprint()\n\ngaus = GaussianNB()\ngaus.fit(train[\"vector\"].tolist(), train[\"category\"])\nguess = gaus.predict(test[\"vector\"].tolist())\nprint(\"GaussianNB:\")\nprint(\"Accuracy:\", accuracy_score(test[\"category\"], guess))\nprint(confusion_matrix(test[\"category\"], guess))\nprint()\n\nlinear = LinearSVC(max_iter=100000)\nlinear.fit(train[\"vector\"].tolist(), train[\"category\"])\nguess = linear.predict(test[\"vector\"].tolist())\nprint(\"LinearSVC:\")\nprint(\"Accuracy:\", accuracy_score(test[\"category\"], guess))\nprint(confusion_matrix(test[\"category\"], guess))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ilaha_Manafova.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\nfrom spacy.lang.en import English\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n!python -m spacy download en_core_web_sm\n!python -m spacy download en_core_web_lg\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    nlp = spacy.load('en_core_web_sm')\n    doc = nlp(text)\n\n    for entity in doc.ents:\n        if len(entity) > 1 and entity.label_ in ['PERSON', 'ORG', 'GPE']:\n            several_token_propn.append(entity.text)\n\n    return(several_token_propn)\nextract_proper_nouns(\"example.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ilaha_Manafova.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    dict_all = {}\n\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    nlp = spacy.load('en_core_web_sm')\n    doc = nlp(text)\n\n    for token in doc:\n        if token.pos_ in [\"VERB\", \"NOUN\"]:\n            if token.lemma_ not in dict_all:\n                dict_all[token.lemma_] = {'VERB': [], 'NOUN': []}\n\n            dict_all[token.lemma_][token.pos_].append(token.text)\n\n    for key, value in dict_all.items():\n        if len(value['VERB']) != 0 and len(value['NOUN']) != 0:\n            tokens_with_common_lemma[key] = list(value['VERB']) + list(value['NOUN'])\n\n\n    return tokens_with_common_lemma\ncommon_lemma(\"my_file.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ilaha_Manafova.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "df = pd.read_csv(\"bbc-news.csv\")\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ilaha_Manafova.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Count the occurrences of each category\ncategory_counts = df['category'].value_counts()\n\n# Plotting the data\nplt.figure(figsize=(8, 6))\ncategory_counts.plot(kind='bar', color='lightblue')\nplt.xlabel('Topical Areas')\nplt.ylabel('Number of Articles')\nplt.title('Number of Articles per Topical Area')\nplt.xticks(rotation=45, ha='right')  # Rotating x-axis labels for better readability\nplt.tight_layout()\n\n# Displaying the plot\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ilaha_Manafova.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(corpus)\n    cleaned_corpus = ' '.join(token.text for token in doc if not token.is_punct)\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    cleaned_corpus = re.sub(r'\\d+', '', corpus)\n    return(cleaned_corpus)\n# Applying preprocessing functions to the 'text' column\ndf['text'] = df['text'].apply(remove_punctuation)\ndf['text'] = df['text'].apply(remove_numbers)\n\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ilaha_Manafova.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# I am using the subset of dataset\ndf_subset = df.head(1000).copy()\n\ndef extract_tokens(text):\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n    tokens = [token.text for token in doc[:150]]  # Extracting the first 150 tokens\n    return ' '.join(tokens)\n\n\ndf_subset['text'] = df_subset['text'].apply(extract_tokens)\n\ndf_subset\n# Load the large model\nnlp_lg = spacy.load('en_core_web_lg')\n\n# Split the dataset into train and test set\ntext_train, text_test, label_train, label_test = train_test_split(df_subset['text'], df_subset['category'],\n                                                                  test_size=0.30, random_state=101, shuffle=True)\n# Convert text data (articles) to vector representation for train and test sets\ntrain_vectors = [nlp_lg(article_train).vector for article_train in text_train]\ntest_vectors = [nlp_lg(article_test).vector for article_test in text_test]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ilaha_Manafova.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# MLP\nmlp_classifier = MLPClassifier(solver='lbfgs')\nmlp_classifier.fit(train_vectors, label_train)\n\n# SVC\nsvc_classifier = SVC()\nsvc_classifier.fit(train_vectors, label_train)\n\n# Random Forest\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(train_vectors, label_train)\ntest_mlp = mlp_classifier.predict(test_vectors)\ntest_svc = svc_classifier.predict(test_vectors)\ntest_rf = rf_classifier.predict(test_vectors)\nprint(\"MLP results:\")\nprint(\"Accuracy:\", accuracy_score(label_test, test_mlp))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(label_test, test_mlp))\nprint()\n\nprint(\"SVC results:\")\nprint(\"Accuracy:\", accuracy_score(label_test, test_svc))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(label_test, test_svc))\nprint()\n\nprint(\"Random Forest results:\")\nprint(\"Accuracy:\", accuracy_score(label_test, test_rf))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(label_test, test_rf))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import nltk\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\ndef extract_proper_nouns(my_file_name):\n    # Read file contents\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    # Tokenize the text into sentences\n    sentences = sent_tokenize(text)\n    \n    # List to hold multi-token proper nouns\n    multi_token_proper_nouns = []\n\n    # Process each sentence\n    for sentence in sentences:\n        tagged_words = nltk.pos_tag(nltk.word_tokenize(sentence))\n\n        # List to collect words of the current proper noun\n        current_proper_noun = []\n\n        # Iterate over tagged words\n        for word, tag in tagged_words:\n            if tag in ['NNP', 'NNPS']:\n                current_proper_noun.append(word)\n            elif current_proper_noun:\n                # Check if current proper noun has more than one token\n                if len(current_proper_noun) > 1:\n                    multi_token_proper_nouns.append(' '.join(current_proper_noun))\n                # Reset the current proper noun list\n                current_proper_noun = []\n\n        # Check at the end of the sentence\n        if len(current_proper_noun) > 1:\n            multi_token_proper_nouns.append(' '.join(current_proper_noun))\n\n    return multi_token_proper_nouns"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\n\ndef get_wordnet_pos(treebank_tag):\n    # Convert Treebank tags to WordNet POS tags\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None\n\ndef common_lemma(my_file_name):\n    # Read file contents\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    # Tokenize and tag the text\n    words = word_tokenize(text)\n    tagged_words = nltk.pos_tag(words)\n\n    lemmatizer = WordNetLemmatizer()\n    tokens_with_common_lemma = {}\n\n    # Process each word\n    for word, tag in tagged_words:\n        wntag = get_wordnet_pos(tag)\n        if wntag in [wordnet.NOUN, wordnet.VERB]:\n            lemma = lemmatizer.lemmatize(word, pos=wntag)\n            if lemma not in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma] = []\n            if word not in tokens_with_common_lemma[lemma]:\n                tokens_with_common_lemma[lemma].append(word)\n\n    return tokens_with_common_lemma"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This data consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\n\nfile_path = 'bbc-news.csv'\n\n# Loading the data\nbbc_data = pd.read_csv(file_path)\nbbc_data.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\nimport matplotlib.pyplot as plt\n\nclass_counts = bbc_data['category'].value_counts()\n\nplt.figure(figsize=(10, 6))\nclass_counts.plot(kind='bar')\nplt.title('Number of Articles per Topical Area in BBC News Dataset')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\nimport re\n\ndef remove_punctuation(doc):\n    # Here comes your code\n    # Remove punctuation from the doc.\n    translator = str.maketrans('', '', string.punctuation)\n    return doc.translate(translator)\n\ndef remove_numbers(doc):\n    # Here comes your code\n    # Remove numbers from the doc.\n\n    return re.sub(r'\\d+', '', doc)\n\nbbc_data['text'] = bbc_data['text'].apply(remove_punctuation).apply(remove_numbers)\n\nbbc_data.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# Here comes your code\nimport numpy as np\nimport spacy\nfrom sklearn.model_selection import train_test_split\n\nnlp = spacy.load('en_core_web_lg')\n\n# Assuming `bbc_data` is your DataFrame and 'text' is the column with articles\n# Assuming 'category' is the column with labels\nX_train, X_test, y_train, y_test = train_test_split(bbc_data['text'], bbc_data['category'], test_size=0.3, random_state=101, shuffle=True)\n\ndef vectorize_text(text, nlp_model, max_tokens=150):\n    doc = nlp_model(text[:max_tokens])\n    return doc.vector\n\n# Vectorize the training and testing sets\nX_train_vec = [vectorize_text(text, nlp) for text in X_train]\nX_test_vec = [vectorize_text(text, nlp) for text in X_test]\n\n# Convert to arrays if needed\n\nX_train_vec = np.array(X_train_vec)\nX_test_vec = np.array(X_test_vec)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\n#  MLPClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(random_state=101)\nmlp.fit(X_train_vec, y_train)  # Assuming y_train contains your training labels\n\n# RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=101)\nrf.fit(X_train_vec, y_train)\n\n# SVC\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train_vec, y_train)\n\n# Calculate accuracy\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nmlp_accuracy = accuracy_score(y_test, mlp.predict(X_test_vec))\nrf_accuracy = accuracy_score(y_test, rf.predict(X_test_vec))\nsvc_accuracy = accuracy_score(y_test, svc.predict(X_test_vec))\n\n# Confusion matrix\nmlp_cm = confusion_matrix(y_test, mlp.predict(X_test_vec))\nrf_cm = confusion_matrix(y_test, rf.predict(X_test_vec))\nsvc_cm = confusion_matrix(y_test, svc.predict(X_test_vec))\n\n# Report results\nprint(\"MLP Classifier Accuracy:\", mlp_accuracy)\nprint(\"MLP Classifier Confusion Matrix:\\n\", mlp_cm)\n\nprint(\"Random Forest Classifier Accuracy:\", rf_accuracy)\nprint(\"Random Forest Classifier Confusion Matrix:\\n\", rf_cm)\n\nprint(\"SVC Accuracy:\", svc_accuracy)\nprint(\"SVC Confusion Matrix:\\n\", svc_cm)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\ndef extract_proper_nouns(my_file_name): \n    nlp = spacy.load(\"en_core_web_sm\")\n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        text = file.read() \n    doc = nlp(text)\n    several_token_propn = []\n    \n    for ent in doc.ents: \n        # checking Geopolitical Entity label_\n        if ent.label_ == \"GPE\"  and len(ent.text.split()) > 1:\n            several_token_propn.append(ent.text)\n\n    return(several_token_propn)\n \nmy_file_name = \"my_file.txt\"\nresult = extract_proper_nouns(my_file_name)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name): \n    nlp = spacy.load(\"en_core_web_sm\")\n \n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n    doc = nlp(text)\n\n    # Dictionaries' initialization to store verbs and nouns seperated\n    verb_dict = {}\n    noun_dict = {}\n \n    for token in doc: \n        if token.pos_ == \"VERB\": \n            lemma = token.lemma_ \n            verb_dict[lemma] = verb_dict.get(lemma, []) + [token.text]\n        elif token.pos_ == \"NOUN\": \n            lemma = token.lemma_ \n            noun_dict[lemma] = noun_dict.get(lemma, []) + [token.text]\n \n    #lemmas common in both dictionaries\n    common_lemmas = set(verb_dict.keys()) & set(noun_dict.keys())\n \n    tokens_with_common_lemma = {}\n \n    for lemma in common_lemmas:\n        tokens_with_common_lemma[lemma] = verb_dict[lemma] + noun_dict[lemma]\n\n    return tokens_with_common_lemma\n  \nmy_file_name = \"my_file.txt\"\nresult = common_lemma(my_file_name)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd \ncsv_file_path = 'bbc-news.csv'\ndf = pd.read_csv(csv_file_path)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\ncategory_counts = df['category'].value_counts()\n\nplt.figure(figsize=(8, 5))\ncategory_counts.plot(kind='bar', color='orange')\nplt.title('Number of Articles in Each Topical Area')\nplt.ylabel('Articles')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    cleaned_text = text.translate(translator) \n    return cleaned_text\n\ndef remove_numbers(text): \n    cleaned_text = ''.join([char for char in text if not char.isdigit()])\n    return cleaned_text\n \ndf['text'] = df['text'].apply(remove_punctuation)\ndf['text'] = df['text'].apply(remove_numbers)\n# rewrite to csv file\ncsv_file_path = 'bbc-news.csv'   \ndf.to_csv(csv_file_path, index=False)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n# Load the large spaCy model\nnlp = spacy.load(\"en_core_web_lg\")\ndata = pd.read_csv('bbc-news.csv', nrows=1000)\ntrain_data, test_data = train_test_split(data, test_size=0.3, random_state=101)\n\n#shuffling data\ntrain_data = shuffle(train_data, random_state=101)\ntest_data = shuffle(test_data, random_state=101)\n \ntrain_vectors = [nlp(article).vector for article in train_data['text']]\ntest_vectors = [nlp(article).vector for article in test_data['text']]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Orlivskyi.ipynb",
        "question": "### Task 1 (1 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "from sys import stderr\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ndef read_file(file_name):\n    with open(file_name, 'r') as f:\n        text = f.read()\n    return text\n\n\ndef extract_proper_nouns(my_file_name):\n    # preprocess the text from the file\n    try:\n        text = read_file(my_file_name)\n    except:\n        print(f'File \"{my_file_name}\" not found!', file=stderr)\n        return\n\n    doc = nlp(text)\n    # filter out doc.ents which alraedy contains proper nouns to contain more than one token (word)\n    several_token_propn = [ent.text for ent in doc.ents if len(ent.text.split()) > 1]\n\n    return several_token_propn\n\n\nresult = extract_proper_nouns('task1.txt')\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Orlivskyi.ipynb",
        "question": "### Task 2 (2 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "# from sys import stderr\n# import spacy\n# nlp = spacy.load('en_core_web_sm') # I assume the sequuential execution of the cells, so these variables are already defined and imports made\n\n\n# transformation looks like the following:\n# [{'google': 'VERB'}, {'google': 'NOUN'}] -> ['google', 'google']\ndef transform_same_lemma_ds_to_list(list_of_dicts):\n    return [list(item.keys())[0] for item in list_of_dicts]\n\n# list of dicts looks like the following:\n# [{'google': 'VERB'}, {'google': 'NOUN'}]\ndef has_both_verb_and_noun(list_of_dicts):\n    return len([item for item in list_of_dicts if list(item.values())[0] == 'VERB']) > 0 and\\\n            len([item for item in list_of_dicts if list(item.values())[0] == 'NOUN']) > 0\n\n\n\ndef common_lemma(my_file_name):\n    # preprocess the text from the file\n    try:\n        text = read_file(my_file_name)\n    except:\n        print(f'File \"{my_file_name}\" not found!', file=stderr)\n        return\n        \n    \n    doc = nlp(text)\n\n    # words_with_same_lemmas_dict dict looks like the following:\n    # {'google': [{'google': 'VERB'}, {'google': 'NOUN'}], \"query\": [{'query': 'NOUN'}]} where key is a lemma (\"google\")\n    # and value is a list of dicts of words with the same lemma and their part of speach\n    words_with_same_lemmas_dict = {}\n\n    for token in doc:\n        # print(token, token.lemma, token.lemma_, token.pos_)\n        part_of_speach = token.pos_\n        word = token.text\n        lemma = token.lemma_\n\n        # process word only if it's a verb or noun\n        if part_of_speach not in ['VERB', 'NOUN']:\n            continue\n\n        # if it's the first insertion of the lemma init the list of words with the same lemma\n        if not words_with_same_lemmas_dict.get(lemma):\n            words_with_same_lemmas_dict[lemma] = [{word: part_of_speach}] # init list of words with the same lemma\n        # else add the word to the existing list of words with the same lemma\n        else:\n            list_of_words_for_lemma = words_with_same_lemmas_dict[lemma]\n            if not {word: part_of_speach} in list_of_words_for_lemma: # if word with the same part of speach is not in the list\n                list_of_words_for_lemma.append({word: part_of_speach})\n\n    # transform dict of list of dicts into dict of lists and filter out those that have both verbs and nouns\n    filtered_lemmas = {lemma: transform_same_lemma_ds_to_list(words_with_same_lemmas_dict[lemma])\\\n                            for lemma in words_with_same_lemmas_dict\\\n                                if has_both_verb_and_noun(words_with_same_lemmas_dict[lemma])}\n\n    tokens_with_common_lemma = filtered_lemmas # token is the same as \"word\"\n\n    # print(words_with_same_lemmas_dict)\n    # print(tokens_with_common_lemma)\n\n    return tokens_with_common_lemma\n\n\n\nresult = common_lemma('task2.txt')     \nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Orlivskyi.ipynb",
        "question": "### Task 3 (0.5 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\ndataframe = pd.read_csv('bbc-news.csv')\n# create a deepcopy of the dataframe for task 7\ndataframe_without_removals = dataframe.copy(deep=True)\n\n\npd.set_option('display.max_colwidth', 500)\ndataframe.head(10)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Orlivskyi.ipynb",
        "question": "### Task 4 (0.5 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\n\n# Count the number of articles for each topical area (class label)\narticle_counts = dataframe['category'].value_counts() # article_counts of class <class 'pandas.core.series.Series'>, so we can plot it directly\n\n# Display the article counts\nprint(article_counts) \n\n# Plot the article counts\narticle_counts.plot(kind='bar')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.title('Number of Articles for Each Topical Area')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Orlivskyi.ipynb",
        "question": "### Task 5 (1 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\n\n# the following function is not used\ndef remove_punctuation_(corpus): # corpus is a dataframe\n    # \\w - Matches any word character (alphanumeric & underscore). Equivalent to [A-Za-z0-9_]\n    corpus = re.sub(r'[^\\w\\s\\'-]', '', corpus)\n    return re.sub(r'(?<!\\w)-|-(?!\\w)', '', corpus)\n    # is dash in word long-awaited considered puctuation? I don't consider it a punctuation if it's inside of words, so I leave it\n    # is apostrophe in word \"don't\" considered puctuation? I don't consider it a punctuation, so I leave it\n\n\ndef remove_punctuation(corpus): # corpus is a dataframe\n    # in this fucntion I consider apostrophe and dashes as punctuation and remove them)\n    return re.sub(r'[^\\w\\s]', '', corpus)\n\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    return re.sub(r'\\d+', '', corpus)\n\nprint('before removal:', dataframe.loc[8, 'text'])\n\ndataframe['text'] = dataframe['text'].apply(remove_numbers)\ndataframe['text'] = dataframe['text'].apply(remove_punctuation)\n\nprint('after removal:', dataframe.loc[8, 'text'])\n\ndataframe.head(10)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Orlivskyi.ipynb",
        "question": "### Task 6 (2 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"parser\", \"ner\"])\n\nread_unitl = -1\narticles = dataframe['text'][:read_unitl]\nlabels = dataframe['category'][:read_unitl]\n\n# doc = np.array([article.vector for article in nlp.pipe(dataframe['text'][:10], n_process=4, batch_size=50,)])\ndoc = np.array([nlp(article).vector for article in articles])\nprint(doc)\n\nx = doc\ny = labels\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Orlivskyi.ipynb",
        "question": "### Task 7 (3 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.metrics import confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\nunique_categories = np.array(dataframe['category'].unique())\n\n\nclassifier = LinearSVC(random_state=101, max_iter=2000)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\naccuracy = classifier.score(x_test, y_test)\n\nprint(\"LinearSVC accuracy:\", accuracy)\n# print(\"Confusion matrix:\")\n# print(confusion_matrix(y_test, y_pred))\nfig, ax = plt.subplots(figsize=(10, 5))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\nax.xaxis.set_ticklabels(unique_categories)\nax.yaxis.set_ticklabels(unique_categories)\n_ = ax.set_title(\n    f\"Confusion Matrix for LinearSVC\"\n)\n\nclassifier = MLPClassifier(random_state=101, max_iter=2000)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\naccuracy = classifier.score(x_test, y_test)\n\nprint(\"MLPClassifier accuracy:\", accuracy)\n# print(\"Confusion matrix:\")\n# print(confusion_matrix(y_test, y_pred))\nfig, ax = plt.subplots(figsize=(10, 5))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\nax.xaxis.set_ticklabels(unique_categories)\nax.yaxis.set_ticklabels(unique_categories)\n_ = ax.set_title(\n    f\"Confusion Matrix for MLPClassifier\"\n)\n\nclassifier = RandomForestClassifier(random_state=101)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\naccuracy = classifier.score(x_test, y_test)\n\nprint(\"RandomForestClassifier accuracy:\", accuracy)\n# print(\"Confusion matrix:\")\n# print(confusion_matrix(y_test, y_pred))\nfig, ax = plt.subplots(figsize=(10, 5))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\nax.xaxis.set_ticklabels(unique_categories)\nax.yaxis.set_ticklabels(unique_categories)\n_ = ax.set_title(\n    f\"Confusion Matrix for RandomForestClassifier\"\n)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Orlivskyi.ipynb",
        "question": "### Bonus point (1 point)\nIgnore the task 3 (i.e., training without using the functions: remove_punctuation and remove_numbers) and see how your 3 models perform in terms of the accuracy.",
        "answer": "articles = dataframe_without_removals['text'][:read_unitl]\nlabels = dataframe_without_removals['category'][:read_unitl]\n\n# doc = np.array([article.vector for article in nlp.pipe(dataframe['text'][:10], n_process=4, batch_size=50,)])\ndoc = np.array([nlp(article).vector for article in articles])\n\nx = doc\ny = labels\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)\n\n\nclassifier = LinearSVC(random_state=101, max_iter=2000)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\naccuracy = classifier.score(x_test, y_test)\n\nprint(\"LinearSVC accuracy:\", accuracy)\n# print(\"Confusion matrix:\")\n# print(confusion_matrix(y_test, y_pred))\nfig, ax = plt.subplots(figsize=(10, 5))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\nax.xaxis.set_ticklabels(unique_categories)\nax.yaxis.set_ticklabels(unique_categories)\n_ = ax.set_title(\n    f\"Confusion Matrix for LinearSVC\"\n)\n\nclassifier = MLPClassifier(random_state=101, max_iter=2000) # hidden_layer_sizes=(7,)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\naccuracy = classifier.score(x_test, y_test)\n\nprint(\"MLPClassifier accuracy:\", accuracy)\n# print(\"Confusion matrix:\")\n# print(confusion_matrix(y_test, y_pred))\nfig, ax = plt.subplots(figsize=(10, 5))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\nax.xaxis.set_ticklabels(unique_categories)\nax.yaxis.set_ticklabels(unique_categories)\n_ = ax.set_title(\n    f\"Confusion Matrix for MLPClassifier\"\n)\n\nclassifier = RandomForestClassifier(random_state=101)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\naccuracy = classifier.score(x_test, y_test)\n\nprint(\"RandomForestClassifier accuracy:\", accuracy)\n# print(\"Confusion matrix:\")\n# print(confusion_matrix(y_test, y_pred))\nfig, ax = plt.subplots(figsize=(10, 5))\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\nax.xaxis.set_ticklabels(unique_categories)\nax.yaxis.set_ticklabels(unique_categories)\n_ = ax.set_title(\n    f\"Confusion Matrix for RandomForestClassifier\"\n)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jing_Wu.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\ndef extract_proper_nouns(my_file_name):\n    \n    # Load the English language model in spaCy\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the file\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n        \n    # Adjust maximum length\n    nlp.max_length = 5156090  # Set a higher value that fits your text\n    \n    # Process the text using spaCy\n    doc = nlp(text)\n    \n    proper_nouns = []\n\n    # Iterate through the entities in the text\n    for ent in doc.ents:\n        # Check if the entity is a proper noun with more than one token\n        if ent.label_ == 'GPE' and len(ent.text.split()) > 1:  # 'GPE' refers to geopolitical entities like cities, countries, etc.\n            proper_nouns.append(ent.text)\n\n    return list(set(proper_nouns))\n\n\nfile_name = 'bbc-news.csv'  \nresult = extract_proper_nouns(file_name)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jing_Wu.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    # Load the English language model in spaCy\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the file\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    # Adjust maximum length\n    nlp.max_length = 5156090  # Set a higher value that fits your text\n    \n    # Process the text using spaCy\n    doc = nlp(text)\n\n    lemma_dict = {}\n\n    # Iterate through the tokens in the text\n    for token in doc:\n        # Check if the token is a verb or noun\n        if token.pos_ in ['VERB', 'NOUN']:\n            # Get the lemma of the token\n            lemma = token.lemma_\n            # Check if the lemma is already in the dictionary\n            if lemma in lemma_dict:\n                # Append the token to the existing list for that lemma\n                if token.text not in lemma_dict[lemma]:\n                    lemma_dict[lemma].append(token.text)\n            else:\n                # Create a new list for the lemma and add the token\n                lemma_dict[lemma] = [token.text]\n\n    # Filter out lemmas with only one token\n    lemma_dict = {key: value for key, value in lemma_dict.items() if len(value) > 1}\n\n    return lemma_dict\n\n\nfile_name = 'bbc-news.csv'  \nresult = common_lemma(file_name)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jing_Wu.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\n# Load the data from bbc-text.csv\nfile_path = 'bbc-news.csv' \ndata = pd.read_csv(file_path)\n\n# Display basic information about the dataset\nprint(data.info())\n\n# Display the first few rows of the dataset\nprint(data.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jing_Wu.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\n# Count the number of articles for each class label\nclass_counts = data['category'].value_counts()\n\n# Create a bar plot\nplt.figure(figsize=(8, 6))\nclass_counts.plot(kind='bar', color='skyblue')\nplt.title('Number of Articles for Each Topical Area')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jing_Wu.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string \n\ndef remove_punctuation(corpus):\n    # Remove punctuation using string.punctuation\n    cleaned_corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n   \n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # Remove numbers using isdigit()\n    cleaned_corpus = ''.join(word for word in corpus if not word.isdigit())\n    \n    return(cleaned_corpus)\n\n# Load the large spaCy model\nnlp = spacy.load(\"en_core_web_lg\")\n\n# Load the data (it's loaded into a DataFrame named 'data')\ndata = pd.read_csv('bbc-news.csv')  \n\n# Apply the functions to the 'text' column\ndata['text'] = data['text'].apply(remove_punctuation)\ndata['text'] = data['text'].apply(remove_numbers)\n\nprint(data[:20])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jing_Wu.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n\n\n# Load the large spaCy model\nnlp = spacy.load(\"en_core_web_lg\")\n\n# Load the data (it's loaded into a DataFrame named 'data')\ndata = pd.read_csv('bbc-news.csv')  \n\n# Subsetting the data (using the first 1000 rows and first 150 tokens for each article)\nsubset_data = data.head(1000)\nsubset_data.loc[:, 'text'] = subset_data['text'].apply(lambda x: ' '.join(x.split()[:150]))\n\n# Split the data into training and test sets (70% and 30%) using scikit-learn\nX = subset_data['text']\ny = subset_data['category']  \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\n# Convert each article in the data splits to a vector representation using the pre-trained spaCy model\ndef text_to_vectors(text_data):\n    vectors = [nlp(text).vector for text in text_data]\n    return vectors\n\nX_train_vectors = text_to_vectors(X_train)\nX_test_vectors = text_to_vectors(X_test)\n\nprint(X_train_vectors[:5])\nprint(X_test_vectors[:5])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Jing_Wu.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n#  X_train_vectors and X_test_vectors are available from Task 6\n#  y_train and y_test are already defined\n\n# Initializing and training the classifiers\nsvm_classifier = SVC(kernel='linear')\nsvm_classifier.fit(X_train_vectors, y_train)\n\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=101)\nrf_classifier.fit(X_train_vectors, y_train)\n\nmlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=101)\nmlp_classifier.fit(X_train_vectors, y_train)\n\n# Making predictions\nsvm_predictions = svm_classifier.predict(X_test_vectors)\nrf_predictions = rf_classifier.predict(X_test_vectors)\nmlp_predictions = mlp_classifier.predict(X_test_vectors)\n\n# Calculating accuracies\nsvm_accuracy = accuracy_score(y_test, svm_predictions)\nrf_accuracy = accuracy_score(y_test, rf_predictions)\nmlp_accuracy = accuracy_score(y_test, mlp_predictions)\n\nprint(f\"SVM Accuracy: {svm_accuracy}\")\nprint(f\"Random Forest Accuracy: {rf_accuracy}\")\nprint(f\"MLP Classifier Accuracy: {mlp_accuracy}\")\n\n# Generating confusion matrices\nsvm_confusion_matrix = confusion_matrix(y_test, svm_predictions)\nrf_confusion_matrix = confusion_matrix(y_test, rf_predictions)\nmlp_confusion_matrix = confusion_matrix(y_test, mlp_predictions)\n\nprint(\"\\nSVM Confusion Matrix:\")\nprint(svm_confusion_matrix)\nprint(\"\\nRandom Forest Confusion Matrix:\")\nprint(rf_confusion_matrix)\nprint(\"\\nMLP Classifier Confusion Matrix:\")\nprint(mlp_confusion_matrix)\n"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "!pip install spacy\n!python -m spacy download en_core_web_sm\nimport spacy\n\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n\n    # Load the spaCy model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the file\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    # Process the text\n    doc = nlp(text)\n\n    # Extract proper nouns with more than one token\n    several_token_propn = [ent.text for ent in doc.ents if ' ' in ent.text]\n\n    return several_token_propn\n# Example usage\nproper_nouns = extract_proper_nouns(\"my_file.txt\")\nprint(proper_nouns)"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n\n    # Load English tokenizer, tagger, parser, NER and word vectors\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the file content\n    with open(my_file_name, \"r\") as file:\n        text = file.read()\n\n    # Process the text\n    doc = nlp(text)\n\n    # Dictionary to store lemmas and their corresponding words\n    lemma_dict = {}\n    # Dictionary to store the POS tags for each lemma\n    lemma_pos = {}\n\n    for token in doc:\n        # Only consider verbs and nouns\n        if token.pos_ in [\"VERB\", \"NOUN\"]:\n            lemma = token.lemma_\n            # Add the token text to the lemma's list\n            lemma_dict.setdefault(lemma, set()).add(token.text)\n            # Record the POS tag of this lemma\n            lemma_pos.setdefault(lemma, set()).add(token.pos_)\n\n    # Filter out lemmas that are not shared between verbs and nouns\n    tokens_with_common_lemma = {lemma: list(words) for lemma, words in lemma_dict.items() if \"VERB\" in lemma_pos[lemma] and \"NOUN\" in lemma_pos[lemma]}\n\n    return tokens_with_common_lemma\n# Example usage\nlemmas = common_lemma(\"my_file_2.txt\")\nprint(lemmas)"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\n# Load the dataset\nfile_path = 'bbc-news.csv'\nbbc_data = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset\nprint(bbc_data.head())"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\n# Counting the number of articles for each category\ncategory_counts = bbc_data['category'].value_counts()\n\n# Plotting the distribution of articles in each category\nplt.figure(figsize=(10, 6))\ncategory_counts.plot(kind='bar')\nplt.title('Number of Articles per Category in BBC News Dataset')\nplt.xlabel('Category')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.show()"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\n\ndef remove_punctuation(corpus):\n    # Create a translation table for removing punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    # Apply the translation table to remove punctuation\n    cleaned_corpus = corpus.translate(translator)\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    # Remove digits using a translation table\n    translator = str.maketrans('', '', string.digits)\n    # Apply the translation table to remove numbers\n    cleaned_corpus = corpus.translate(translator)\n    return cleaned_corpus\n\ndata = pd.read_csv('bbc-news.csv')\n\n# Apply the functions\ndata['text'] = data['text'].apply(remove_punctuation).apply(remove_numbers)\n\n# Optional: Export the cleaned data\ndata.to_csv('NEW-bbc-news.csv', index=False)  # Export to a new CSV file"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "!python -m spacy download en_core_web_lg\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport numpy as np\n\n#Load spaCy large model\nnlp = spacy.load('en_core_web_lg')\n\n#Load the dataset\ndata = pd.read_csv('NEW-bbc-news.csv')\n\n# Split the data into training and test set (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(data['text'], data['category'],\n                                                    test_size=0.3, random_state=101, shuffle=True)\n\n#Convert articles to vector representation\ndef text_to_vector(text):\n    doc = nlp(text)\n    return doc.vector\n\nX_train_vectors = np.array([text_to_vector(text) for text in X_train])\nX_test_vectors = np.array([text_to_vector(text) for text in X_test])\nX_train_vectors, X_test_vectors"
    },
    {
        "file_name": "Assignment_3_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Model 1: MLPClassifier\nmlp = MLPClassifier(random_state=1)\nmlp.fit(X_train_vectors, y_train)\n\n# Model 2: RandomForestClassifier\nrf = RandomForestClassifier(random_state=1)\nrf.fit(X_train_vectors, y_train)\n\n# Model 3: Support Vector Machine Classifier\nsvc = SVC(random_state=1)\nsvc.fit(X_train_vectors, y_train)\n\ndef evaluate_model(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    conf_matrix = confusion_matrix(y_test, predictions)\n    return accuracy, conf_matrix\n\nmlp_accuracy, mlp_conf_matrix = evaluate_model(mlp, X_test_vectors, y_test)\nrf_accuracy, rf_conf_matrix = evaluate_model(rf, X_test_vectors, y_test)\nsvc_accuracy, svc_conf_matrix = evaluate_model(svc, X_test_vectors, y_test)\n\n# Print the results\nprint(f\"MLPClassifier Accuracy: {mlp_accuracy}\\nConfusion Matrix:\\n{mlp_conf_matrix}\\n\")\nprint(f\"RandomForestClassifier Accuracy: {rf_accuracy}\\nConfusion Matrix:\\n{rf_conf_matrix}\\n\")\nprint(f\"SVC Accuracy: {svc_accuracy}\\nConfusion Matrix:\\n{svc_conf_matrix}\\n\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suyash_Thapa.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "!python -m spacy download en_core_web_sm\n!python -m spacy download en_core_web_lg\nimport spacy\nfrom collections import defaultdict\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string\nfrom sklearn.model_selection import train_test_split \nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\ndef extract_proper_nouns(my_file_name):\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = open(my_file_name, \"r\").read()\n    doc = nlp(doc)\n\n    N = len(doc)\n    i = 0\n\n    several_token_propn = []\n\n    while i < N:\n        # token at position i\n        tok = doc[i]\n        if tok.pos_ == \"PROPN\":\n            # a proper noun starting from token i\n            proper_noun = [tok.text, ]  # the holder\n            # check the following tokens and add to the holder\n            j = i + 1\n            while j < N and doc[j].pos_ == \"PROPN\":\n                proper_noun.append(doc[j].text)\n                j += 1\n\n            if len(proper_noun) > 1:\n                several_token_propn.append(\" \".join(proper_noun))\n\n            i = j - 1\n        i += 1\n\n    return several_token_propn\n\n# Example usage\ndummy_data = \"Hong Kong and Japan, and many other countries are copying New York style.\"\nwith open(\"dummy_data.txt\", 'w', encoding='utf-8') as dummy_file:\n    dummy_file.write(dummy_data)\n\nresult = extract_proper_nouns(\"dummy_data.txt\")\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suyash_Thapa.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    doc = nlp(text)\n\n    lemmas_with_both_types = defaultdict(list)\n\n    for token in doc:\n        if token.pos_ in [\"NOUN\", \"VERB\"]:\n            lemmas_with_both_types[token.lemma_].append(token.text)\n\n    # Filter out lemmas with only one type\n    common_lemmas = {lemma: tokens for lemma, tokens in lemmas_with_both_types.items() if len(set(token.pos_ for token in doc if token.text in tokens)) > 1}\n\n    return common_lemmas\n\n# Example usage\ntext1 = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nwith open(\"example1.txt\", 'w', encoding='utf-8') as file1:\n    file1.write(text1)\n\nresult1 = common_lemma(\"example1.txt\")\nprint(result1)\n\ntext2 = \"I really loved the movie and show, the movie was showing reality but it showed sometimes nonsense!\"\nwith open(\"example2.txt\", 'w', encoding='utf-8') as file2:\n    file2.write(text2)\n\nresult2 = common_lemma(\"example2.txt\")\nprint(result2)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suyash_Thapa.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "df_bbc = pd.read_csv(\"./bbc-news.csv\")  \ndf_bbc"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suyash_Thapa.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "cat = df_bbc[\"category\"].value_counts()\nplt.figure(figsize=(10, 6))\ncat.plot(kind='bar', color='skyblue')\nplt.title('Distribution of Articles by Topical Area')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suyash_Thapa.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    # Use the string.punctuation to get all punctuation characters\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n    return corpus.translate(translator)\n\ndef remove_numbers(corpus):\n    # Remove digits (numbers) from the text\n    cleaned_corpus = ''.join(char for char in corpus if not char.isdigit())\n    return(cleaned_corpus) \n\ndf_bbc['text'] = df_bbc['text'].apply(remove_punctuation) \ndf_bbc['text'] = df_bbc['text'].apply(remove_numbers) \ndf_bbc"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suyash_Thapa.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# Load the large spaCy model trained on web text\nnlp = spacy.load(\"en_core_web_lg\") \ntrain_df, test_df = train_test_split(df_bbc, test_size=0.3, random_state=101)\n\ndef text_to_vector(text):\n    # Use spaCy to get the document vector\n    doc = nlp(text)\n    # Return the vector representation as a NumPy array\n    return doc.vector\n\n# Apply the text_to_vector function to each article in the training and testing sets\ntrain_vectors = train_df['text'].apply(text_to_vector).to_numpy()\ntest_vectors = test_df['text'].apply(text_to_vector).to_numpy()\n\n# Now train_vectors and test_vectors contain the vector representations of your articles\n# You can use these arrays as features for machine learning models\n\n# Optionally, you can also store these vectors back in your DataFrames\ntrain_df['vectors'] = train_vectors.tolist()\ntest_df['vectors'] = test_vectors.tolist()\n\n# Display the first few rows of the training and testing DataFrames\nprint(\"Training Data:\")\nprint(train_df.head())"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suyash_Thapa.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Extract the vectors from the DataFrames\nX_train = np.array(train_df['vectors'].tolist())\nX_test = np.array(test_df['vectors'].tolist())\ny_train = train_df['category']\ny_test = test_df['category']\n\n# Standardize the feature vectors\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# MLPClassifier\nmlp_clf = MLPClassifier(max_iter=1000, random_state=42)\nmlp_clf.fit(X_train_scaled, y_train)\nmlp_predictions = mlp_clf.predict(X_test_scaled)\n\n# RandomForestClassifier\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train_scaled, y_train)\nrf_predictions = rf_clf.predict(X_test_scaled)\n\n# Support Vector Classifier (SVC)\nsvc_clf = SVC(random_state=42)\nsvc_clf.fit(X_train_scaled, y_train)\nsvc_predictions = svc_clf.predict(X_test_scaled)\n\n# Evaluate and report metrics\ndef evaluate_model(predictions, model_name):\n    accuracy = accuracy_score(y_test, predictions)\n    confusion_mat = confusion_matrix(y_test, predictions)\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(\"Confusion Matrix:\")\n    print(confusion_mat)\n\n# Evaluate MLPClassifier\nevaluate_model(mlp_predictions, \"MLPClassifier\")\n\n# Evaluate RandomForestClassifier\nevaluate_model(rf_predictions, \"RandomForestClassifier\")\n\n# Evaluate Support Vector Classifier\nevaluate_model(svc_predictions, \"SVC\")\n"
    },
    {
        "file_name": "Assignment3_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "pip install spacy\nimport spacy\n\ndef extract_proper_nouns(my_file_name):\n    # Load spaCy\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Process the text using spaCy\n    doc = nlp(text)\n\n    # Initialize a list to store proper nouns with more than one token\n    several_token_propn = []\n\n    # Iterate through named entities in the document\n    for ent in doc.ents:\n        # Check if the entity is a proper noun and has more than one token\n        if ent.label_ == \"GPE\" or ent.label_ == \"LOC\" or ent.label_ == \"FAC\":\n            if len(ent) > 1:\n                several_token_propn.append(ent.text)\n\n    return several_token_propn\n\n# Example\nfile_name = \"test.txt\"\nresult = extract_proper_nouns(file_name)\nprint(result)"
    },
    {
        "file_name": "Assignment3_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name):\n    # Load spaCy \n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Read the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Process the text using spaCy\n    doc = nlp(text)\n\n    # Initialize a dictionary to store lemmas and their associated words\n    tokens_with_common_lemma = {}\n\n    # Iterate through tokens in the document\n    for token in doc:\n        # Check if the token is a noun or a verb\n        if token.pos_ in ['NOUN', 'VERB']:\n            # Get the lemma of the token\n            lemma = token.lemma_\n\n            # If the lemma is not already in the dictionary, create an entry\n            if lemma not in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma] = [token.text]\n            else:\n                # If the lemma is already in the dictionary, add the token to the list\n                tokens_with_common_lemma[lemma].append(token.text)\n\n    # Filter out lemmas with only one associated word\n    tokens_with_common_lemma = {lemma: words for lemma, words in tokens_with_common_lemma.items() if len(words) > 1}\n\n    return tokens_with_common_lemma\n\n# Example\nfile_name = \"exercise2.txt\"\nresult = common_lemma(file_name)\nprint(result)"
    },
    {
        "file_name": "Assignment3_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\ndef load_bbc_data(file_path):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Display the first few rows of the DataFrame\n    print(df.head())\n\n    return df\n\nfile_path = \"bbc-news.csv\"\n\n# load the data\nbbc_data = load_bbc_data(file_path)"
    },
    {
        "file_name": "Assignment3_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef load_bbc_data(file_path):\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Display the first few rows of the DataFrame \n    print(df.head())\n\n    return df\n\ndef plot_article_counts(data_frame):\n    # Count the number of articles for each topical area\n    article_counts = data_frame['category'].value_counts()\n\n    # Plot the counts using a bar plot\n    article_counts.plot(kind='bar', figsize=(10, 6), color='skyblue')\n    \n    # Add labels and title\n    plt.xlabel('Topical Area')\n    plt.ylabel('Number of Articles')\n    plt.title('Number of Articles for Each Topical Area')\n    \n    # Show the plot\n    plt.show()\n\n\nfile_path = \"bbc-news.csv\"\n\n\nbbc_data = load_bbc_data(file_path)\n\n# Plot the article counts\nplot_article_counts(bbc_data)"
    },
    {
        "file_name": "Assignment3_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\nimport re\n\ndef remove_punctuation(corpus):\n    # Remove punctuation using the string library\n    translator = str.maketrans('', '', string.punctuation)\n    cleaned_corpus = [text.translate(translator) for text in corpus]\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    # Remove numbers using regular expressions\n    cleaned_corpus = [re.sub(r'\\d', '', text) for text in corpus]\n    return cleaned_corpus\n\n# Example \nbbc_data['text'] = remove_punctuation(bbc_data['text'])\nbbc_data['text'] = remove_numbers(bbc_data['text'])\n\nprint(bbc_data['text'])"
    },
    {
        "file_name": "Assignment3_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\n\n# Load spaCy\nnlp = spacy.load(\"en_core_web_lg\")\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(bbc_data['text'], test_size=0.3, random_state=101, shuffle=True)\n\ndef vectorize_text(text):\n    # Process the text using the spaCy model\n    doc = nlp(text)\n\n    # Return the vector representation of the document\n    return doc.vector\n\n# Apply the vectorization function to the training and test sets\nX_train_vectors = [vectorize_text(article) for article in X_train]\nX_test_vectors = [vectorize_text(article) for article in X_test]\n\n\n# Modify the text to use only the first 150 tokens\nX_train_subset = [article[:150] for article in X_train]\nX_test_subset = [article[:150] for article in X_test]\n\n# Apply the vectorization function to the modified sets\nX_train_vectors = [vectorize_text(article) for article in X_train_subset]\nX_test_vectors = [vectorize_text(article) for article in X_test_subset]"
    },
    {
        "file_name": "Assignment3_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(bbc_data['text'], bbc_data['category'], test_size=0.3, random_state=101, shuffle=True)\n\n\n# Convert class labels to numerical encoding\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\n# Train MLPClassifier\nmlp_classifier = MLPClassifier(random_state=101)\nmlp_classifier.fit(X_train_vectors, y_train_encoded)\n\n# Train RandomForestClassifier\nrf_classifier = RandomForestClassifier(random_state=101)\nrf_classifier.fit(X_train_vectors, y_train_encoded)\n\n# Train Support Vector Classifier\nsvc_classifier = SVC(random_state=101)\nsvc_classifier.fit(X_train_vectors, y_train_encoded)\n\n# Evaluate MLPClassifier\nmlp_predictions = mlp_classifier.predict(X_test_vectors)\nmlp_accuracy = accuracy_score(y_test_encoded, mlp_predictions)\nmlp_conf_matrix = confusion_matrix(y_test_encoded, mlp_predictions)\n\n# Evaluate RandomForestClassifier\nrf_predictions = rf_classifier.predict(X_test_vectors)\nrf_accuracy = accuracy_score(y_test_encoded, rf_predictions)\nrf_conf_matrix = confusion_matrix(y_test_encoded, rf_predictions)\n\n# Evaluate Support Vector Classifier\nsvc_predictions = svc_classifier.predict(X_test_vectors)\nsvc_accuracy = accuracy_score(y_test_encoded, svc_predictions)\nsvc_conf_matrix = confusion_matrix(y_test_encoded, svc_predictions)\n\n# Print results\nprint(\"MLP Classifier:\")\nprint(f\"Accuracy: {mlp_accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(mlp_conf_matrix)\nprint(\"\\n\")\n\nprint(\"Random Forest Classifier:\")\nprint(f\"Accuracy: {rf_accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\n\nprint(\"Support Vector Classifier:\")\nprint(f\"Accuracy: {svc_accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(svc_conf_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\n\ndef extract_proper_nouns(my_file_name):\n    # We will use the part of speech recognition attribute of en_core_web_sm model of the spacy module\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    # We will read the whole file as one document and input it into the nlp model\n    with open(my_file_name, 'r') as file:\n        inp = file.read()\n        \n    doc = nlp(inp)\n\n    several_token_propn = []\n    tmp = []\n    for tok in doc:\n        # Consecutive proper noun parts of speech are saved into a temporary list\n        if tok.pos_ == \"PROPN\":\n            tmp.append(tok.text)\n        else:\n            if len(tmp) > 1:\n                # If the chain of nouns has more than one token, it is saved in several_token_propn\n                several_token_propn.append(' '.join(tmp))\n                \n            # We discard the list if we encounter a non-proper noun\n            tmp.clear()\n            \n    return(several_token_propn)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    # We will use both part of speech and lemma attribute of en_core_web_sm model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # We will read the whole file as one document and input it into the nlp model\n    with open(my_file_name, 'r') as file:\n        inp = file.read()\n        \n    doc = nlp(inp)\n    \n    # We cache the lemma of every noun and verb in lemmas_pos dictionary\n    lemmas_pos = {}\n    for tok in doc:\n        if tok.pos_ in [\"NOUN\", \"VERB\"]:\n            # lemma will be the key and (token, pos) pair will be the value in this dictionary\n            if tok.lemma_ in lemmas_pos:\n                lemmas_pos[tok.lemma_].append((tok.text, tok.pos_))\n            else:\n                lemmas_pos[tok.lemma_] = [(tok.text, tok.pos_)]\n                \n    # We select only the tokens with common lemma that have both NOUN and VERB parts of speech\n    tokens_with_common_lemma = {}\n    for lemma in lemmas_pos:\n        # The token and pos pair is splitted\n        tokens = [pair[0] for pair in lemmas_pos[lemma]]\n        parts_of_speech = [pair[1] for pair in lemmas_pos[lemma]]\n                \n        if \"NOUN\" in parts_of_speech and \"VERB\" in parts_of_speech:\n            tokens_with_common_lemma[lemma] = tokens\n    \n    return(tokens_with_common_lemma)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\n# We use pandas to read and store the data\ndf = pd.read_csv(\"bbc-news.csv\")\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\n# We plot the counts of each category as a bar chart\ndf[\"category\"].value_counts().plot(kind='bar')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\nimport string\n\n\ndef remove_punctuation(corpus):\n    # A string with all punctuations is found in string.punctuation object\n    # We can easily substitute those characters with empty string using regex\n    cleaned_corpus = re.sub(f\"[{re.escape(string.punctuation)}]\", '', corpus)\n    \n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # numbers are matched via \\d+ pattern and substituted with empty string\n    cleaned_corpus = re.sub(\"\\d+\", '', corpus)\n    \n    return(cleaned_corpus)\n\n\n# The two preprocessing functions are applied consecutively to the text column\ndf[\"text\"] = df[\"text\"].apply(remove_punctuation).apply(remove_numbers)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\n\n\n# Loading the large language model\nnlp = spacy.load(\"en_core_web_lg\")\n\n# We split the dataset according to the instructions\nX = df[\"text\"]\ny = df[\"category\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\n# We get the vector representations of the articles using the large language model\nX_train = [nlp(article).vector for article in X_train]\nX_test = [nlp(article).vector for article in X_test]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# We have chosen to use Multi Layer Perceptron, Gaussian Naive Bayes and Linear Support Vector Machine classifier models to fit and test\n# on the data\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n# We fit a mlp classifier with default hyper parameters and evaluate its performance\nmlp_clf = MLPClassifier(hidden_layer_sizes=(100,))\nmlp_clf.fit(X_train, y_train)\nmlp_pred = mlp_clf.predict(X_test)\nprint(\"Metrics for MLP Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, mlp_pred))\nprint(\"Recall:\", recall_score(y_test, mlp_pred, average='micro'))\nprint(\"Precision:\", precision_score(y_test, mlp_pred, average='micro'))\nprint(\"F1_score:\", f1_score(y_test, mlp_pred, average='micro'))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, mlp_pred))\n# We fit a naive bayes classifier with default hyper parameters and evaluate its performance\nnb_clf = GaussianNB()\nnb_clf.fit(X_train, y_train)\nnb_pred = nb_clf.predict(X_test)\nprint(\"Metrics for Naive Bayes Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, nb_pred))\nprint(\"Recall:\", recall_score(y_test, nb_pred, average='micro'))\nprint(\"Precision:\", precision_score(y_test, nb_pred, average='micro'))\nprint(\"F1_score:\", f1_score(y_test, nb_pred, average='micro'))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_pred))\n# We fit a support vector machine classifier with default hyper parameters and evaluate its performance\nsvm_clf = LinearSVC()\nsvm_clf.fit(X_train, y_train)\nsvm_pred = svm_clf.predict(X_test)\nprint(\"Metrics for Support Vector Machine Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, svm_pred))\nprint(\"Recall:\", recall_score(y_test, svm_pred, average='micro'))\nprint(\"Precision:\", precision_score(y_test, svm_pred, average='micro'))\nprint(\"F1_score:\", f1_score(y_test, svm_pred, average='micro'))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, svm_pred))\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "def extract_proper_nouns(my_file_name = \"file_name.txt\"):\n    svp = []\n    doc = \"Honk Kong and Japan are two cities in Asia, $40.37 the New York City is a city in the United States of America. +9195408\"\n    with open(my_file_name, \"r\", encoding=\"utf-8-sig\") as file:\n        doc = nlp(doc)\n\n    matcher = Matcher(nlp.vocab)\n\n    pattern = [\n        [\n            {\"POS\":\"PROPN\",\"DEP\":\"compound\",\"OP\":\"+\"},\n            {\"POS\":\"PROPN\",\"DEP\":{\"NOT_IN\":[\"compound\"]}}\n        ]\n    ]\n    matcher.add(\"prop2\", pattern)\n    matches = matcher(doc, as_spans=True)\n    \n    for i in range(0, (len(matches) - 1)):\n        if(matches[i].text not in matches[i+1].text):\n            svp.append(matches[i].text)\n    \n    svp.append(matches[-1].text)\n    return(svp)\nextract_proper_nouns()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name = \"lemma_file.txt\"):\n    doc = \"\"\n    with open(my_file_name, \"r\", encoding=\"utf-8-sig\") as file:\n        doc = nlp(file.read())\n    \n    matcher = Matcher(nlp.vocab)\n    pattern = [\n        [\n            {\"POS\":\"NOUN\"}\n        ],\n        [\n            {\"POS\":\"VERB\"}\n        ]\n    ]\n    \n    matcher.add(\"prop2\", pattern)\n    matches = matcher(doc, as_spans=True)\n    \n    verb = set()\n    noun = set()\n    for span in matches:            \n        if span[0].pos_ == \"VERB\":\n            verb.add(span[0].lemma_)\n        else:\n            noun.add(span[0].lemma_)\n    \n    common = list(verb.intersection(noun))\n    v = []\n    for c in common:\n        d = []\n        for t in matches:\n            if t[0].lemma_ == c:\n                d.append(t[0].text)\n        v.append(d)\n        \n    res = {key: v[i] for i, key in enumerate(common)}\n    \n    \n    return(res)\ncommon_lemma()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "df = pd.read_csv(\"bbc-text.csv\")\nlabels = df.category.unique()\nprint(labels)\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "s = df.category.value_counts()\nx, y = s.to_list(), s.index.to_list()\n\nax = plt.subplot()\nax.set_ylabel(\"Frequency\")\nax.set_xlabel(\"Classes\")\nax = plt.bar(y, x, width=0.5)\n\nfor rect in ax:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.0f}', ha='center', va='bottom')\n\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation_and_num(corpus):\n    doc = nlp(corpus)\n    cleaned_corpus = \"\"\n    add_space = \"\"\n    for token in doc:\n        if(token.is_alpha):\n            cleaned_corpus += add_space + token.text_with_ws\n            add_space = \"\"\n        elif token.whitespace_:\n            add_space = \" \"\n    return(cleaned_corpus)\nv = \"Honk Kong and Japan are two cities in Asia, $40.37 the New York City is a city in the United States of America. 919.5408\"\nprint(remove_punctuation_and_num(v))\ndf.text = df.text.apply(remove_punctuation_and_num)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "nlp = spacy.load('en_core_web_lg')\nxtrain, xtest, ytrain, ytest = train_test_split(df[\"text\"], df[\"category\"], test_size=0.30, random_state=101, shuffle=True)\ndef to_vector(corpus):\n    return nlp(corpus).vector\n\ntrain_vector = np.array([to_vector(c) for c in xtrain])\ntest_vector = np.array([to_vector(c) for c in xtest])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "def get_predictions(model, xtest, ytest, log_string):\n    print(f\"Performance for {log_string}:\")\n    y_pred = model.predict(xtest)\n    \n    acc = accuracy(ytest, y_pred)\n    \n    print(f\"Accuracy: {acc}\")\n    \n    cm = confusion_matrix(ytest, y_pred, labels=labels)\n    plt.figure(figsize=(8, 6))\n    seaborn.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Reds')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix')\n    plt.show()\nscaler = StandardScaler()\n\nxtrain_s = scaler.fit_transform(train_vector)\nxtest_s = scaler.transform(test_vector)\n\n# label_test = [int_to_label(y) for y in ytest]\n# SVM Classifier\nsvm_classifier = svm.LinearSVC()\nsvm_classifier.fit(xtrain_s, ytrain)\n\nget_predictions(svm_classifier, xtest_s, ytest, \"SVM.LinearSVC\")\n# Random Forest\nrf_classifier = rfc(criterion=\"entropy\", random_state=101, n_estimators=200)\nrf_classifier.fit(xtrain_s, ytrain)\n\nget_predictions(rf_classifier, xtest_s, ytest, \"Random Forest Classifier\")\n# Gaussian Naive Bayes\nnbg = naive_bayes.GaussianNB()\nnbg.fit(xtrain_s, ytrain)\n\nget_predictions(nbg, xtest_s, ytest, \"Gaussian Naive Bayes Classifier\")\n# MLP Classifier\nmlp = MLP(random_state=101, activation='relu', hidden_layer_sizes=(40, 40), max_iter=1000)\nmlp.fit(xtrain_s, ytrain)\n\nget_predictions(mlp, xtest_s, ytest, \"MLP Classifier\")\n\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\ndef extract_proper_nouns(my_file_name):\n    \"\"\"\n    Extract proper nouns with multiple tokens from a text file. (e.g. New York, Hong Kong)\n    \"\"\"\n    \n    several_token_propn = []\n    # Load spacy model.\n    nlp = spacy.load('en_core_web_sm')\n    # Open file.\n    with open(my_file_name, 'r') as file:\n        # Read file.\n        doc = nlp(file.read())\n        # Extract proper nouns.\n        i = 0\n        while i < len(doc) - 1:\n            token = doc[i]\n            if token.pos_ == 'PROPN' and token.dep_ == 'compound':\n                compound = token.text\n                # Check if the next token is a proper noun.\n                # The last word of the compound has to have a different dep_ than 'compound'.\n                while i + 1 < len(doc) and doc[i + 1].dep_ == 'compound':\n                    compound += ' ' + doc[i + 1].text\n                    i += 1\n                # Add the last word.\n                compound += ' ' + doc[i + 1].text\n                # Add the compound to the list.\n                several_token_propn.append(compound)\n            i += 1\n\n\n    return several_token_propn"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    \"\"\"\n    Return dictionary of lemmata which are shared by a noun AND a verb at the same time.\n    \"\"\"\n    tokens_with_common_lemma = {}\n    # Read text from file.\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n    # Load spacy model.\n    nlp = spacy.load('en_core_web_sm')\n    # Create doc object.\n    doc = nlp(text)\n    # Iterate over the tokens.\n    for token in doc:\n        # Check if the token is a noun or a verb.\n        if token.pos_ == 'NOUN' or token.pos_ == 'VERB':\n            lemma = token.lemma_\n            text = token.text\n            # Check if the lemma is already in the dictionary.\n            if lemma in tokens_with_common_lemma:\n                # Add the token to the list of tokens with the same lemma.\n                tokens_with_common_lemma[lemma].append(token)\n            else:\n                # Add the lemma to the dictionary.\n                tokens_with_common_lemma[lemma] = [token]\n    # Lambda expression to check if a list contains at least two distinct elements.\n    distinct = lambda x: len(set(map(lambda y: y.pos_, x))) > 1\n\n    # Create new dict with only lemmata that have at least one noun AND at the same time at least one verb.\n    tokens_with_common_lemma = {lemma: tokens for lemma, tokens in tokens_with_common_lemma.items() if distinct(tokens)}\n\n\n\n\n\n    return(tokens_with_common_lemma)\n# Test the function.\nprint(common_lemma('test.txt'))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Load bbc-news.csv with pandas.\nimport pandas as pd\ncorpus = pd.read_csv('bbc-news.csv')\ncorpus.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Show amount of entries for each category in the dataset in graph.\nimport matplotlib.pyplot as plt\nprint(corpus['category'].value_counts())\ncorpus['category'].value_counts().plot(kind='bar')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus: pd.DataFrame) -> pd.DataFrame:\n    # Remove punctuation from column 'text'.\n    filtered = corpus['text'].str.replace('[^\\w\\s]','', regex=True)\n    # Remove 'text' column from corpus.\n    corpus = corpus.drop(columns=['text'])\n    # Add filtered text to corpus.\n    corpus['text'] = filtered\n    return(corpus)\n\ndef remove_numbers(corpus: pd.DataFrame) -> pd.DataFrame:\n    # Remove numbers from column 'text'.\n    filtered = corpus['text'].str.replace('\\d+', '', regex=True)\n    # Remove 'text' column from corpus.\n    corpus = corpus.drop(columns=['text'])\n    # Add filtered text to corpus.\n    corpus['text'] = filtered    \n\n    return(corpus)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\nimport tqdm\n\n# Load en_core_web_lg model.\nnlp = spacy.load('en_core_web_lg')\n\n# Remove punctuation and numbers from column 'text'.\ncorpus = remove_punctuation(corpus)\ncorpus = remove_numbers(corpus)\n\n# Split the data into training and test sets with random seed 101.\nX_train, X_test, y_train, y_test = train_test_split(corpus['text'], corpus['category'], test_size=0.3, random_state=101)\n\n# Use nlp to create turn each article to vector representation.\nX_train_vectors = []\nfor article in tqdm.tqdm(X_train):\n    X_train_vectors.append(nlp(article).vector)\n\n# Same for test set.\nX_test_vectors = []\nfor article in tqdm.tqdm(X_test):\n    X_test_vectors.append(nlp(article).vector)\n\n# Print first few vectors.\nprint(X_train_vectors[:3])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Ishfaq_Herbrik.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Create MLPClassifier.\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.neural_network import MLPClassifier\nimport seaborn as sn\nmlp = MLPClassifier(hidden_layer_sizes=(2048, 1024, 512), max_iter=500, learning_rate='adaptive', verbose=True)\nmlp.fit(X_train_vectors, y_train)\n# Display confusion matrix.\npredictions = mlp.predict(X_test_vectors)\ncm = confusion_matrix(y_test, predictions)\ndf_cm = pd.DataFrame(cm, index = corpus['category'].unique(),\n                  columns = corpus['category'].unique())\nplt.figure(figsize = (10,7))\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=sn.color_palette('crest'), fmt='g')\n# Add comment to bottom of the figure showing accuracy.\nplt.annotate('Accuracy: ' + str(round(mlp.score(X_test_vectors, y_test), 2)), xy=(0, -0.1), xycoords='axes fraction', fontsize=16)\n# Add title \nplt.title('MLP', fontsize=16)\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import LinearSVC\n\n# Create LinearSVC with crammer_singer as multi_class keyword\nsvc = LinearSVC(multi_class='crammer_singer', max_iter=50000000)\nsvc.fit(X_train_vectors, y_train)\n\n# Make predictions\npredictions = svc.predict(X_test_vectors)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, predictions)\n\n# Create DataFrame from confusion matrix\ndf_cm = pd.DataFrame(cm, index = corpus['category'].unique(),\n                  columns = corpus['category'].unique())\n\n# Plot confusion matrix\nplt.figure(figsize = (10,7))\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=sn.color_palette('crest'), fmt='g')\n\n# Add annotation showing accuracy\nplt.annotate('Accuracy: ' + str(round(svc.score(X_test_vectors, y_test), 2)), xy=(0, -0.1), xycoords='axes fraction', fontsize=16)\n\n# Add title\nplt.title('LinearSVC', fontsize=16)\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000, random_state=101, verbose=True)\nrf.fit(X_train_vectors, y_train)\n\n# Make predictions\npredictions = rf.predict(X_test_vectors)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, predictions)\n\n# Create DataFrame from confusion matrix\ndf_cm = pd.DataFrame(cm, index=corpus['category'].unique(),\n                     columns=corpus['category'].unique())\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 7))\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=sn.color_palette('crest'), fmt='g')\n\n# Add annotation showing accuracy\nplt.annotate('Accuracy: ' + str(round(rf.score(X_test_vectors, y_test), 2)), xy=(0, -0.1), xycoords='axes fraction', fontsize=16)\n\n# Add title\nplt.title('RandomForestClassifier', fontsize=16)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suraj_Giri_and_Duc_Manh_Vu.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "def extract_proper_nouns(my_file_name):\n  nlp = spacy.load(\"en_core_web_sm\")\n  doc = open(my_file_name, \"r\").read()\n  doc = nlp(doc)\n\n  N = len(doc)\n  i = 0\n\n  several_token_propn = []\n\n  while i < N:\n    # token at position i\n    tok = doc[i]\n    if tok.pos_ == \"PROPN\":\n      # a proper noun starting from token i\n      proper_noun = [tok.text, ] # the holder\n      # check the following tokens and add to the holder\n      j = i + 1\n      while j < N:\n        new_tok = doc[j]\n        if new_tok.pos_ == \"PROPN\":\n          proper_noun.append(new_tok.text)\n          j += 1\n        else:\n          break\n      if len(proper_noun) > 1:\n        several_token_propn.append(\" \".join(proper_noun))\n      i = j - 1\n    i += 1\n\n  # # recheck the condition\n  # for ent in doc.ents:\n  #   if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"] and \" \" in ent.text and ent.text not in several_token_propn:\n  #     several_token_propn.append(ent.text)\n\n  return(several_token_propn)\nextract_proper_nouns(\"./my_text.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suraj_Giri_and_Duc_Manh_Vu.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n  tokens_with_common_lemma = dict()\n  tokens_with_common_type = dict()\n  nlp = spacy.load(\"en_core_web_sm\")\n\n  doc = open(my_file_name, \"r\").read()\n  doc = nlp(doc)\n\n\n  for token in doc:\n    if token.pos_ in [\"NOUN\", \"VERB\"]: # we only consider noun and verb, not others\n      #\n      if token.lemma_ not in tokens_with_common_lemma:\n        tokens_with_common_lemma[token.lemma_] = list()\n      if token.lemma_ not in tokens_with_common_type:\n        tokens_with_common_type[token.lemma_] = set()\n\n      # add token lemma to the dict\n      tokens_with_common_lemma[token.lemma_].append(token.text)\n      tokens_with_common_type[token.lemma_].add(token.pos_)\n\n  remove = list()\n  for t in tokens_with_common_type:\n    # check if the lemma only represent verb or noun\n    if len(tokens_with_common_type[t]) < 2:\n      remove.append(t)\n\n  # remove it from the result\n  for t in remove:\n    tokens_with_common_lemma.pop(t)\n    tokens_with_common_type.pop(t)\n\n  return(tokens_with_common_lemma)\ncommon_lemma(\"./my_text2.txt\")\n\n# When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\n# I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense.\n# Three players who played last week play a play from the radio today.\n# They comb their hair with only one comb.\n# That was a class performance of him in the class today.\n# The ticket to the fair was quite fair."
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suraj_Giri_and_Duc_Manh_Vu.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "df = pd.read_csv(\"./bbc-news.csv\")\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suraj_Giri_and_Duc_Manh_Vu.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "counter = df['category'].value_counts()\nprint(counter)\ncounter.plot(kind='bar')"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suraj_Giri_and_Duc_Manh_Vu.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n  cleaned_corpus = corpus.copy(deep=True)\n  for index, row in corpus.iterrows():\n    text = row['text']\n\n    # Use maketrans to remove punctuations without replacing any text\n    cleaned_text = text.translate(str.maketrans('', '', string.punctuation))\n    cleaned_corpus.loc[index, 'text'] = cleaned_text\n  return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n  cleaned_corpus = corpus.copy(deep=True)\n  for index, row in corpus.iterrows():\n    text = row['text']\n    # Use regex to replace any number with a blank string\n    cleaned_text = re.sub('[0-9]+((st)|(nd)|(rd)|(th))', '', text) # remove ordinal numbers\n    cleaned_text = re.sub('[0-9]+([.,][0-9]+)*', '', cleaned_text) # remove cardinal numbers\n\n    cleaned_corpus.loc[index, 'text'] = cleaned_text\n  return(cleaned_corpus)\ncleaned_df = remove_punctuation(df)\ncleaned_df = remove_numbers(cleaned_df)\nprint(cleaned_df.loc[685, 'text'])\nprint(\"\\n\")\nprint(df.loc[685, 'text'])\nprint(cleaned_df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suraj_Giri_and_Duc_Manh_Vu.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# loading the large model trained on the web text provided by spacy. (en_core_web_lg))\nnlp = spacy.load(\"en_core_web_lg\")\nprint(nlp.pipe_names)\n# print(df)\n\n# split the df into train and test set 70/30 using sci-kit learn and seed 101\ntrain, test = train_test_split(cleaned_df, test_size=0.3, random_state=101)\nprint(\"|||||||||||||||||||||\")\nprint(cleaned_df.shape)\nprint(cleaned_df)\nprint(\"|||||||||||||||||||||\")\nprint(train.shape)\nprint(train)\nprint(\"|||||||||||||||||||||\")\nprint(test.shape)\nprint(test)\nprint(\"|||||||||||||||||||||\")\n# Function to convert each article in the data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\ndef vectorize_data(dataset):\n    vectors = []\n    # Loop through each article in the dataset\n    for article in dataset:\n        # Process the article text with spaCy\n        doc = nlp(article)\n        # Extract the vector representation (vector averaged over tokens)\n        vector = doc.vector\n        vectors.append(vector)\n    return vectors\n\n# converting each article to the vector representation\ntrain_vectors = vectorize_data(train['text'])\ntest_vectors = vectorize_data(test['text'])\nprint(\"Train Vector: \\n\", train_vectors)\nprint(\"Test Vector: \\n\", test_vectors)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Suraj_Giri_and_Duc_Manh_Vu.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Using the vectors from abovae to train 3 different models provided by sci-kit learn\n# 3 models: MLPClassifier, LogisticRegression, SVC (Support Vector Classifier)\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# MLPClassifier\nmlp = MLPClassifier()\nmlp.fit(train_vectors, train['category'])\nmlp_pred = mlp.predict(test_vectors)\nprint(\"MLP Classifier Prediction: \\n\", mlp_pred)\n# LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(train_vectors, train['category'])\nlog_reg_pred = log_reg.predict(test_vectors)\nprint(\"Logistic Regression Prediction: \\n\", log_reg_pred)\n# SVC (Support Vector Classifier)\nsvc = SVC()\nsvc.fit(train_vectors, train['category'])\nsvc_pred = svc.predict(test_vectors)\nprint(\"SVC Prediction: \\n\", svc_pred)\n# Evaluating the performance of the models on the test set and reporting accuracy and confusion matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# MLPClassifier\nmlp_acc = accuracy_score(test['category'], mlp_pred)\nmlp_cm = confusion_matrix(test['category'], mlp_pred)\nprint(\"MLP Classifier Accuracy: \\n\", mlp_acc)\nprint(\"MLP Classifier Confusion Matrix: \\n\", mlp_cm)\n\n# LogisticRegression\nlog_reg_acc = accuracy_score(test['category'], log_reg_pred)\nlog_reg_cm = confusion_matrix(test['category'], log_reg_pred)\nprint(\"Logistic Regression Accuracy: \\n\", log_reg_acc)\nprint(\"Logistic Regression Confusion Matrix: \\n\", log_reg_cm)\n\n# SVC\nsvc_acc = accuracy_score(test['category'], svc_pred)\nsvc_cm = confusion_matrix(test['category'], svc_pred)\nprint(\"SVC Accuracy: \\n\", svc_acc)\nprint(\"SVC Confusion Matrix: \\n\", svc_cm)\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\ndef extract_proper_nouns(my_file_name):\n    # Load the English NLP model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    try:\n        # Read the content of the file\n        with open(file_name, 'r', encoding='utf-8') as file:\n            content = file.read()\n\n        # Process the text with SpaCy\n        doc = nlp(content)\n\n        # Extract proper nouns with more than one token\n        proper_nouns = []\n        current_proper_noun = []\n\n        for token in doc:\n            if token.pos_ == 'PROPN':\n                current_proper_noun.append(token.text)\n            elif current_proper_noun:\n                if len(current_proper_noun) > 1:\n                    proper_nouns.append(' '.join(current_proper_noun))\n                current_proper_noun = []\n\n        # Check the last proper noun\n        if len(current_proper_noun) > 1:\n            proper_nouns.append(' '.join(current_proper_noun))\n\n        return proper_nouns\n\n    except FileNotFoundError:\n        print(f\"Error: File '{file_name}' not found.\")\n        return []\n\n# Example usage\nfile_name = \"sample.txt\"  # Replace with the actual file name\nresult = extract_proper_nouns(file_name)\nprint(\"Proper Nouns:\", result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name):\n    # Load the spaCy English model\n    nlp = spacy.load('en_core_web_lg')\n\n    # Read the content of the file\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Process the text using spaCy\n    doc = nlp(text)\n\n    # Initialize dictionaries to store verbs and nouns for each lemma\n    verb_dict = {}\n    noun_dict = {}\n\n    # Iterate through tokens in the processed document\n    for token in doc:\n        # Consider only verbs and nouns for lemmatization\n        if token.pos_ in {'VERB', 'NOUN'}:\n            # Lemmatize the token\n            lemma = token.lemma_\n\n            # Add the lemma to the corresponding dictionary\n            if token.pos_ == 'VERB':\n                verb_dict.setdefault(lemma, []).append(token.text.lower())\n            elif token.pos_ == 'NOUN':\n                noun_dict.setdefault(lemma, []).append(token.text.lower())\n\n    # Create a set of lemmas that have both verbs and nouns\n    common_lemmas = set(verb_dict.keys()) & set(noun_dict.keys())\n\n    # Create the final dictionary with lemmas and their shared words\n    lemma_dict = {lemma: verb_dict[lemma] + noun_dict[lemma] for lemma in common_lemmas}\n\n    return lemma_dict\n\n# Example usage\nfile_name = \"example.txt\"\nresult = common_lemma(file_name)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\ndf = pd.read_csv('bbc-news.csv')\ndf.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\nimport matplotlib.pyplot as plt\ncategory_counts = df['category'].value_counts()\n\n# Plot the data\nplt.figure(figsize=(10, 10))\ncategory_counts.plot(kind='bar', color='grey')\nplt.title('Number of Articles in Each Category')\nplt.xlabel('Category')\nplt.ylabel('Number of Articles')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\nimport re\n\ndef remove_punctuation(corpus):\n    # Here comes your code\n    translator = str.maketrans('', '', string.punctuation)\n    return corpus.translate(translator)\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    return re.sub(r'\\d+', '', corpus)\n\n#print(df)\ndf['text'] = df['text'].apply(remove_punctuation)\ndf['text'] = df['text'].apply(remove_numbers)\n#print(\"after pre processing\")\n#print(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "!python -m spacy download en_core_web_lg\n# Here comes your code\n\n# Loading the large spaCy English model\nnlp = spacy.load(\"en_core_web_lg\")\nfrom sklearn.model_selection import train_test_split\n\n# Assuming your DataFrame is named df\n# X is the feature (text data), y is the target variable (category)\nX = df['text']\ny = df['category']\n\n# Split the data into training and test sets (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101, shuffle=True)\nimport numpy as np\ndef doc_to_vector(doc):\n    \"\"\"\n    Convert a spaCy Doc object to a vector representation.\n    \"\"\"\n    return doc.vector\n\n# Apply spaCy to each article in the training set and test set\nX_train_vectors = [doc_to_vector(nlp(article)) for article in X_train]\nX_test_vectors = [doc_to_vector(nlp(article)) for article in X_test]\n\n# Convert the lists to NumPy arrays\nX_train_vectors = np.array(X_train_vectors)\nX_test_vectors = np.array(X_test_vectors)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the data before training Logistic Regression\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_vectors)\nX_test_scaled = scaler.transform(X_test_vectors)\n\n# Model 1: Logistic Regression\nlogreg_model = LogisticRegression(random_state=101,max_iter=1000)\nlogreg_model.fit(X_train_scaled, y_train)\n\n# Model 2: Random Forest\nrf_model = RandomForestClassifier(random_state=101,max_depth= None,min_samples_leaf= 1,min_samples_split= 2,n_estimators=200)\nrf_model.fit(X_train_scaled, y_train)\n\n# Model 3: MLPClassifier\nmlp_model = MLPClassifier(random_state=101,activation='logistic',alpha=0.0001,hidden_layer_sizes=(50, 50),max_iter=200)\nmlp_model.fit(X_train_scaled, y_train)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Function to evaluate and print results\ndef evaluate_model(model, X_test_vectors, y_test):\n    y_pred = model.predict(X_test_vectors)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy: {accuracy:.4f}\")\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred, labels=df['category'].unique())\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=df['category'].unique(), yticklabels=df['category'].unique())\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix - {type(model).__name__}')\n    plt.show()\n\n# Evaluate each model\nprint(\"Logistic Regression:\")\nevaluate_model(logreg_model, X_test_scaled, y_test)\n\nprint(\"\\nRandom Forest:\")\nevaluate_model(rf_model, X_test_scaled, y_test)\n\nprint(\"\\nMLPClassifier:\")\nevaluate_model(mlp_model, X_test_scaled, y_test)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    # here comes your code\n\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    doc = nlp(text)\n\n    # SpaCy's noun chunking capabilities (doc.noun_chunks) to extract noun phrases\n    for chunk in doc.noun_chunks:\n        if len(chunk) > 1 and all(token.pos_ == 'PROPN' for token in chunk):    #filter chunks acc to length (len(chunk) > 1) and\n                                                                                # all tokens in chunks are classified as proper nouns (PROPN)\n                                                                                # then append\n            several_token_propn.append(chunk.text)\n\n    return(several_token_propn)\n# A text file needs to be provided to test the function.\nextract_proper_nouns(\"test.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    # here comes your code\n\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n\n    doc = nlp(text)\n\n    # for token in doc:\n    #     if token.pos_ in ['VERB', 'NOUN']:\n    #         lemma = token.lemma_\n    #         if lemma not in tokens_with_common_lemma:\n    #             tokens_with_common_lemma[lemma] = []\n    #         tokens_with_common_lemma[lemma].append(token.text)\n\n    for token in doc:\n        if token.pos_ == 'VERB':\n            lemma = token.lemma_\n            if lemma not in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma] = []\n            tokens_with_common_lemma[lemma].append(token.text)\n\n            for other_token in token.children:\n                if other_token.pos_ in ['VERB', 'NOUN'] and other_token.lemma_ == lemma:\n                    tokens_with_common_lemma[lemma].append(other_token.text)\n\n    return(tokens_with_common_lemma)\n# A text file needs to be provided to test the function\ncommon_lemma(\"text.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'bbc-news.csv'\ndata = pd.read_csv(file_path)\n\nprint(data.head())  # Display the first few rows of the dataset"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\n# Count the occurrences of each category\ncategory_counts = data['category'].value_counts()\n\n# Plot the distribution of articles across different topical areas\nplt.figure(figsize=(10, 6))\ncategory_counts.plot(kind='bar', color='blue')\nplt.title('Distribution of Articles Across Topical Areas')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import string\nimport re\ndef remove_punctuation(corpus):\n    # Remove punctuation using the string library\n    translator = str.maketrans('', '', string.punctuation)\n    cleaned_corpus = [doc.translate(translator) for doc in corpus]\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    # Remove numbers using regular expressions\n    cleaned_corpus = [re.sub(r'\\d+', '', doc) for doc in corpus]\n    return cleaned_corpus\ndata['text'] = remove_punctuation(data['text'])\ndata['text'] = remove_numbers(data['text'])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# Function to convert text to vector using spaCy\ndef text_to_vector(text):\n    doc = nlp(text)\n    # Get the vector representation of the document\n    vector = doc.vector\n    return vector\n# Here comes your code\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_lg\")\n\n# Load your dataset (replace 'your_dataset.csv' with your actual dataset)\n# df = pd.read_csv('your_dataset.csv')\n\n# Optional: Work with a subset of the dataset (first 1000 rows)\n# Currently using the whole dataset\ndf_subset = data #.head(1000)\n\n# Optional: Use only the first 150 tokens for each article\ndf_subset['text'] = df_subset['text'] #.apply(lambda x: ' '.join(x.split()[:150]))\n\n# Split the data into training and test sets (70% train, 30% test)\nX_train, X_test = train_test_split(df_subset['text'], test_size=0.3, random_state=101)\n\n# Convert each article to a vector for training set\nX_train_vectors = np.array([text_to_vector(text) for text in X_train])\n\n# Convert each article to a vector for test set\nX_test_vectors = np.array([text_to_vector(text) for text in X_test])\ny_train = df_subset.loc[X_train.index]['category']\ny_test = df_subset.loc[X_test.index]['category']"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n# Train a Support Vector Machine (SVM) classifier\nsvm_classifier = SVC(random_state=101)\nsvm_classifier.fit(X_train_vectors, y_train)\n\n# Train a Random Forest Classifier\nrf_classifier = RandomForestClassifier(random_state=101)\nrf_classifier.fit(X_train_vectors, y_train)\n\n# Train an MLPClassifier\nmlp_classifier = MLPClassifier(random_state=101)\nmlp_classifier.fit(X_train_vectors, y_train)\n\n# Evaluate the classifiers on the test set\nsvm_predictions = svm_classifier.predict(X_test_vectors)\nrf_predictions = rf_classifier.predict(X_test_vectors)\nmlp_predictions = mlp_classifier.predict(X_test_vectors)\n\n# Calculate accuracy for each classifier\nsvm_accuracy = accuracy_score(y_test, svm_predictions)\nrf_accuracy = accuracy_score(y_test, rf_predictions)\nmlp_accuracy = accuracy_score(y_test, mlp_predictions)\n\n# Print accuracies\nprint(f\"SVM Accuracy: {svm_accuracy:.4f}\")\nprint(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\nprint(f\"MLP Classifier Accuracy: {mlp_accuracy:.4f}\")\n\n# Build and print confusion matrices for each classifier\nsvm_conf_matrix = confusion_matrix(y_test, svm_predictions)\nrf_conf_matrix = confusion_matrix(y_test, rf_predictions)\nmlp_conf_matrix = confusion_matrix(y_test, mlp_predictions)\n\nprint(\"\\nConfusion Matrix - SVM:\")\nprint(svm_conf_matrix)\n\nprint(\"\\nConfusion Matrix - Random Forest:\")\nprint(rf_conf_matrix)\n\nprint(\"\\nConfusion Matrix - MLP Classifier:\")\nprint(mlp_conf_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Confusion matrix with labels",
        "answer": "import itertools\n\ndef plot_confusion_matrix(conf_matrix, classes, title):\n    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.colorbar()\n\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd'\n    thresh = conf_matrix.max() / 2.\n    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n        plt.text(j, i, format(conf_matrix[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n# Assuming you have classes representing your categories\nclasses = ['business', 'entertainment', 'politics', 'sport', 'tech']\n\n# Plot confusion matrices for each classifier\nplot_confusion_matrix(svm_conf_matrix, classes, \"Confusion Matrix - SVM\")\nplot_confusion_matrix(rf_conf_matrix, classes, \"Confusion Matrix - Random Forest\")\nplot_confusion_matrix(mlp_conf_matrix, classes, \"Confusion Matrix - MLP Classifier\")\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_SimonWelz_JonBreid.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "!pip install spacy\n!python -m spacy download en_core_web_sm\nimport re\ndef extract_proper_nouns(my_file_name):\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Define a regular expression pattern to match proper nouns\n    pattern = re.compile(r'\\b[A-Z][a-z]+\\b(?:\\s+[A-Z][a-z]+)*\\b')\n\n    # Find all matches in the text\n    matches = re.findall(pattern, text)\n\n    # Filter matches with more than one token\n    several_token_propn = [match for match in matches if len(match.split()) > 1]\n    return(several_token_propn)\nfile_name = \"test.txt\"\nprint(extract_proper_nouns(file_name))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_SimonWelz_JonBreid.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\ndef common_lemma(my_file_name):\n    # Load the English language model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # Process the text using spaCy\n    doc = nlp(text)\n\n    # Dictionary to store lemmas and their associated words\n    lemma_dict = {}\n\n    # Iterate through each token in the document\n    for token in doc:\n        # Check if the token is a noun or a verb\n        if token.pos_ in ['NOUN', 'VERB']:\n            # Get the lemma of the token\n            lemma = token.lemma_\n\n            # Add the word to the list associated with the lemma in the dictionary\n            if lemma in lemma_dict:\n                lemma_dict[lemma].append(token.text)\n            else:\n                lemma_dict[lemma] = [token.text]\n\n    # Filter out lemmas with only one associated word\n    lemma_dict = {lemma: words for lemma, words in lemma_dict.items() if len(words) > 1}\n\n    # Filter out lemmas where both nouns and verbs are present\n    filtered_dict = {lemma: words for lemma, words in lemma_dict.items() if any(t.pos_ == 'NOUN' for t in nlp(\" \".join(words))) and any(t.pos_ == 'VERB' for t in nlp(\" \".join(words)))}\n\n    return filtered_dict\nfile_name = \"test.txt\"\nprint(common_lemma(file_name))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_SimonWelz_JonBreid.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\nfile_path = \"bbc-news.csv\"\ndf = pd.read_csv(file_path)\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_SimonWelz_JonBreid.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n# count the number of occurences of each label\nlabel_counts = df['category'].value_counts()\n\nplt.figure(figsize=(10,6))\nlabel_counts.plot(kind='bar', color='skyblue')\nplt.title(\"Number of Articls for Each Topical Area\")\nplt.xlabel(\"Topical Area\")\nplt.ylabel(\"Number of Articles\")\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_SimonWelz_JonBreid.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    # only keep spaces and alphanumeric characters\n    cleaned_corpus = ''.join(char for char in corpus if char.isalnum() or char.isspace())\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # keep all characters that are not digits\n    cleaned_corpus = ''.join(char for char in corpus if not char.isdigit())\n    return(cleaned_corpus)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_SimonWelz_JonBreid.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX = df['text']\ny = df['category']\nnlp = spacy.load(\"en_core_web_lg\")\n# create train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101, shuffle=True)\n\ndef get_text_vectors(texts):\n    # list to save all calculated vectors\n    vectors = []\n    # process each text \n    for text in texts:\n        processed_text = remove_numbers(remove_punctuation(text))\n        # apply the pre trained model\n        doc_vector = nlp(processed_text).vector\n        # save the results\n        vectors.append(doc_vector)\n    # create one big array from all stored vectors\n    return np.array(vectors)\n\nX_train_vectors = get_text_vectors(X_train)\nX_test_vectors = get_text_vectors(X_test)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_SimonWelz_JonBreid.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "!pip install seaborn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\nseed = 101\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_vectors)\nX_test_scaled = scaler.transform(X_test_vectors)\n\n# Train Support Vector Machine (SVM) model\nsvm_model = SVC(random_state=seed)\nsvm_model.fit(X_train_vectors, y_train)\n\n# Train Logistic Regression model on scaled data\nlogistic_model = LogisticRegression(random_state=seed, max_iter=1000)\nlogistic_model.fit(X_train_scaled, y_train)\n\n\n# Train MLPClassifier model\nmlp_model = MLPClassifier(random_state=seed)\nmlp_model.fit(X_train_vectors, y_train)\n# Make predictions on the test set\nlogistic_preds = logistic_model.predict(X_test_vectors)\nsvm_preds = svm_model.predict(X_test_vectors)\nmlp_preds = mlp_model.predict(X_test_vectors)\n# Evaluate model performance\nlogistic_accuracy = accuracy_score(y_test, logistic_preds)\nsvm_accuracy = accuracy_score(y_test, svm_preds)\nmlp_accuracy = accuracy_score(y_test, mlp_preds)\n# Print accuracies\nprint(f\"Logistic Regression Accuracy: {logistic_accuracy}\")\nprint(f\"SVM Accuracy: {svm_accuracy}\")\nprint(f\"MLPClassifier Accuracy: {mlp_accuracy}\")\n# Convert class labels to integers for confusion matrix\nlabel_encoder = LabelEncoder()\ny_test_encoded = label_encoder.fit_transform(y_test)\nlogistic_preds_encoded = label_encoder.transform(logistic_preds)\nsvm_preds_encoded = label_encoder.transform(svm_preds)\nmlp_preds_encoded = label_encoder.transform(mlp_preds)\n# Build confusion matrices\nlogistic_cm = confusion_matrix(y_test, logistic_preds, labels=label_encoder.classes_)\nsvm_cm = confusion_matrix(y_test, svm_preds,  labels=label_encoder.classes_)\nmlp_cm = confusion_matrix(y_test, mlp_preds, labels=label_encoder.classes_)\n# Plot confusion matrices\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nsns.heatmap(logistic_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, ax=axes[0])\naxes[0].set_title(\"Logistic Regression Confusion Matrix\")\n\nsns.heatmap(svm_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, ax=axes[1])\naxes[1].set_title(\"SVM Confusion Matrix\")\n\nsns.heatmap(mlp_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, ax=axes[2])\naxes[2].set_title(\"MLPClassifier Confusion Matrix\")\n\nplt.tight_layout()\nplt.show()"
    },
    {
        "file_name": "Assignment_3_Ilhom_Khalimov.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_proper_nouns(my_file_name: str):\n    several_token_propn: list[str] = []\n    with open(my_file_name, 'r') as file:\n        cur: list[str] = []\n        for line in file.readlines():\n            parsed = nlp(line)\n            for token in parsed:\n                # print(token, token.pos_)\n                if token.pos_ == \"PROPN\":\n                    cur.append(token.text)\n                elif len(cur) != 0:\n                        if len(cur) >= 2:\n                            several_token_propn.append(' '.join(cur))\n                        cur.clear()\n\n\n    return(several_token_propn)\n\nprint(extract_proper_nouns(\"task1.txt\"))"
    },
    {
        "file_name": "Assignment_3_Ilhom_Khalimov.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "from collections import defaultdict\nimport tempfile\n\ndef common_lemma(my_file_name: str):\n    tokens_with_common_lemma: dict[str, set[str]] = {}\n    with open(my_file_name, 'r') as file:\n        lemma_nouns: set[str] = set()\n        lemma_verbs: set[str] = set()\n        set_map: dict[str, set[str]] = {\"NOUN\": lemma_nouns, \"VERB\": lemma_verbs}\n        lemmas_should_add: set[str] = set()\n        common_lemma_all: dict[str, set[str]] = defaultdict(set)\n        for line in file.readlines():\n            parsed = nlp(line)\n            for token in parsed:\n                lemma = token.lemma_\n                pos = token.pos_\n                # print(token, pos, lemma)\n                if pos in set_map:\n                    set_map.get(pos).add(lemma)\n                    common_lemma_all[lemma].add(token.text)\n                if lemma in lemma_nouns and lemma in lemma_verbs:\n                    lemmas_should_add.add(lemma)\n        # converting set to list because the examples seem to explicitly expect list\n        tokens_with_common_lemma = {lemma: list(common_lemma_all.get(lemma)) for lemma in lemmas_should_add}\n\n    return tokens_with_common_lemma\n\nfrom typing import TextIO\ndef test() -> None:\n    ex1 = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\n    ex2 =\"I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\"\n    ex3 = ex1 + \"\\n\" + ex2\n    examples = [ex1, ex2, ex3]\n    \n    def generate_file(content: str) -> TextIO:\n        file = tempfile.NamedTemporaryFile(\"w+t\", encoding=\"utf-8\")\n        file.write(content)\n        file.seek(0)\n        return file\n    files = [generate_file(example) for example in examples]\n\n    for file in files:\n        print(common_lemma(file.name))\n\ntest()"
    },
    {
        "file_name": "Assignment_3_Ilhom_Khalimov.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\nbbc = pd.read_csv(\"bbc-news.csv\")\nprint(bbc.head())\nprint(bbc.shape)"
    },
    {
        "file_name": "Assignment_3_Ilhom_Khalimov.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib\n\ncount = bbc.groupby(\"category\").count().rename(columns={\"text\": \"count\"})\nprint(count)\ncount.plot.bar(rot=0)"
    },
    {
        "file_name": "Assignment_3_Ilhom_Khalimov.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus: pd.DataFrame):\n    filter = lambda doc: ' '.join(token.text for token in nlp(doc) if not token.is_punct)\n    corpus[\"text\"] = corpus[\"text\"].apply(filter)\n    # flt = lambda row: ' '.join(token.text for token in nlp(row[\"text\"]) if not token.is_punct)\n    # corpus[\"text\"] = corpus.apply(flt, axis=1)\n    return corpus\n\ndef remove_numbers(corpus: pd.DataFrame):\n    # Here comes your code\n    filter = lambda doc: ' '.join(token.text for token in nlp(doc) if token.pos_ != \"NUM\")\n    corpus[\"text\"] = corpus[\"text\"].apply(filter)\n    return corpus\n\n\nbbc = remove_numbers(remove_punctuation(bbc))"
    },
    {
        "file_name": "Assignment_3_Ilhom_Khalimov.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "from sklearn.model_selection import train_test_split\nimport numpy as np\n\nnlp = spacy.load(\"en_core_web_sm\")\ntext, category = bbc[\"text\"], bbc[\"category\"]\n\ntext_train, text_test, category_train, category_test = train_test_split(text, category, shuffle=True, train_size=0.7, random_state=101)\n# text_train2 = pd.Series([nlp(doc).vector for doc in text_train.head()])\n# display(nlp(text_train).vector)\ntext_train_vec = [nlp(text).vector.reshape(1,-1) for text in text_train]\ntext_train_vec = np.concatenate(text_train_vec)\ntext_test_vec = [nlp(text).vector.reshape(1,-1) for text in text_test]\ntext_test_vec = np.concatenate(text_test_vec)\ndisplay(text_train_vec)\ndisplay(text_test_vec)"
    },
    {
        "file_name": "Assignment_3_Ilhom_Khalimov.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn import metrics\nfrom sklearn.calibration import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\ngauss = GaussianNB()\ngauss.fit(text_train_vec, category_train)\ngauss_pred = gauss.predict(text_test_vec)\n\ndecision = DecisionTreeClassifier(max_depth=10, random_state=101)\ndecision.fit(text_train_vec, category_train)\ndecision_pred = decision.predict(text_test_vec)\n\nneighbor = KNeighborsClassifier(n_neighbors=3)\nneighbor.fit(text_train_vec, category_train)\nneighbor_pred = neighbor.predict(text_test_vec)\n\nsvc = LinearSVC(dual=True, random_state=101)\nsvc.fit(text_train_vec, category_train)\nsvc_pred = svc.predict(text_test_vec)\n\nmlp = MLPClassifier(alpha=1e-5, max_iter=1000, random_state=101)\nmlp.fit(text_train_vec, category_train)\nmlp_pred = mlp.predict(text_test_vec)\n\ndef print_scores(name: str, classifier, label) -> None:\n    print(f\"Classifier: {name}\")\n    print(f'Accuracy: {metrics.accuracy_score(label, classifier)}')\n    print('Confusion:')\n    print(metrics.confusion_matrix(label, classifier))\n\nprint_scores(\"MLP\", mlp_pred, category_test)\nprint()\nprint_scores(\"SVC\", svc_pred, category_test)\nprint()\nprint_scores(\"K Neighbors\", neighbor_pred, category_test)\nprint()\nprint_scores(\"Decision Tree\", decision_pred, category_test)\nprint()\nprint_scores(\"Gaussian\", gauss_pred, category_test)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "# Import all required libraries\nimport spacy\nfrom spacy.lang.en import English\nfrom spacy.matcher import Matcher\nnlp = spacy.load('en_core_web_sm')\nimport pandas as pd\nfrom spacy import displacy\nimport matplotlib.pyplot as plt\nimport string\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\ndef extract_proper_nouns(my_file_name):\n\n    several_token_propn = []\n# open the file and read it line by line\n    with open(my_file_name, 'r', encoding='utf-8') as my_file:\n        for line in my_file:\n# process the text using the SpaCy NLP pipeline\n            doc = nlp(line)\n# Loop through each token except the last one \n            for x in range(len(doc)-1):\n# check if the token is a proper nour\n                if (doc[x].pos_ == \"PROPN\"):\n# if the token is a proper noun check if the one next to it is also a proper noun\n                    if (doc[x+1].pos_ == \"PROPN\"):\n# if yes, then append both tokens to the array\n                        several_token_propn.append(doc[x].text + \" \" + doc[x+1].text)\n#return the array \n    return(several_token_propn)\nprint(extract_proper_nouns(\"task1.txt\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "my_file_name = \"task2.txt\"\ndef common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    # traversing through the contents of the file line by line\n    with open(my_file_name, 'r', encoding='utf-8') as my_file:\n        for line in my_file:\n            doc = nlp(line)\n            # traversing through every token of a line\n            for x in range(len(doc)-1):\n                # print(doc[x].text, doc[x].pos_, doc[x].lemma_) \n                # checking if the token is either a verb or a noun   \n                if (doc[x].pos_ == \"VERB\" or doc[x].pos_ == \"NOUN\"):\n                    # if the key is already in the dictionary, the text is appended to the list\n                    # if the key is not present, a new key is made which is the lemma of the word\n                    try:\n                        tokens_with_common_lemma[doc[x].lemma_].append(doc[x])\n                    except Exception:\n                        tokens_with_common_lemma[doc[x].lemma_] = [doc[x]]\n            # print(tokens_with_common_lemma)\n\n    # filtering only those lemmas which share both a noun and a verb in the provided text\n    # creating a copy of the keys to avoid RuntimeError\n    keys_to_remove = list(tokens_with_common_lemma.keys())\n\n    for lemma in keys_to_remove:\n        # if the length == 1, the word will be either a noun or a verb but not both \n        # such keys should be deleted from the dictionary\n        if len(tokens_with_common_lemma) == 1:\n            del tokens_with_common_lemma[lemma]\n        else:\n            is_verb = False\n            is_noun = False\n\n            for word in tokens_with_common_lemma[lemma]:\n                if word.pos_ == \"VERB\":\n                    is_verb = True\n                elif word.pos_ == \"NOUN\":\n                    is_noun = True\n            # checking if the lemma contains both a noun and a verb\n            if not (is_verb and is_noun):\n                del tokens_with_common_lemma[lemma]\n    \n    return(tokens_with_common_lemma)\ncommon_lemma(my_file_name)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# define the file path \nfile_path = 'bbc-news.csv'\n\n# Load the CSV file into a DataFrame\ndata = pd.read_csv(file_path)\n\n# Display the first few rows of the DataFrame\ndata.head()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "#count the number of articles per category\narticle_counts = data['category'].value_counts()\n# plot the number of articles against the topics\nplt.figure(figsize=(6, 3))\narticle_counts.plot(kind='bar', color='lightseagreen')\nplt.title('Number of Articles for Each Topic')\nplt.xlabel('Topic')\nplt.ylabel('Number of Articles')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    # remove punctuations from the corpus\n    translator = str.maketrans('', '', string.punctuation)\n    cleaned_corpus = list(map(lambda text: text.translate(translator), corpus))\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    # remove numbers from the corpus\n    cleaned_corpus = list(map(lambda text: ''.join([char for char in text if not char.isdigit()]), corpus))\n    return cleaned_corpus\ndata['preprocessed_data'] = remove_numbers(remove_punctuation(data['text']))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "nlp = spacy.load('en_core_web_lg')\n# Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 \ntrain_data, test_data = train_test_split(data['preprocessed_data'], test_size=0.3, random_state=101)\n\n# Use SpaCy's NLP pipeline to generate word vectors for the first 150 tokens\ntrain_vectors = [nlp(text).vector for text in train_data]\ntest_vectors = [nlp(text).vector for text in test_data]\n\n# Convert to arrays\ntrain_vectors = np.array(train_vectors)\ntest_vectors = np.array(test_vectors)\n\n# Print the shape of both vectors\nprint(\"Training vectors shape:\", train_vectors.shape)\nprint(\"Testing vectors shape:\", test_vectors.shape)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Define the labels (assuming 'category' is the column containing class labels)\nlabels = data['category']\n\n# Split the labels into training and testing sets\ntrain_labels, test_labels = train_test_split(labels, test_size=0.3, random_state=101)\n\n# 1. Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier(random_state=101)\ndt_classifier.fit(train_vectors, train_labels)\ndt_predictions = dt_classifier.predict(test_vectors)\n\n# 2. Random Forest Classifier\nrf_classifier = RandomForestClassifier(random_state=101)\nrf_classifier.fit(train_vectors, train_labels)\nrf_predictions = rf_classifier.predict(test_vectors)\n\n# 3. MLP Classifier\nmlp_classifier = MLPClassifier(random_state=101)\nmlp_classifier.fit(train_vectors, train_labels)\nmlp_predictions = mlp_classifier.predict(test_vectors)\n\n# Evaluate and report the results\nclassifiers = ['Decision Tree', 'Random Forest', 'MLP Classifier']\nfor i, predictions in enumerate([dt_predictions, rf_predictions, mlp_predictions]):\n    accuracy = accuracy_score(test_labels, predictions)*100\n    conf_matrix = confusion_matrix(test_labels, predictions)\n    \n    print(f\"\\nResults for {classifiers[i]}:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Confusion Matrix:\\n{conf_matrix}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muhamamd_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n\n    with open(my_file_name, \"r\") as f:\n        text = f.read()\n\n    ## Tokenize the text\n    tokens = nltk.word_tokenize(text)\n\n    ## Tag the tokens with their parts of speech\n    pos_tags = nltk.pos_tag(tokens)\n\n    ## Iterate over parts of speech and get words with more than on\n    for i, (token, tag) in enumerate(pos_tags):\n      if tag == \"NNP\" or tag == \"NNPS\":\n        if i > 0 and (pos_tags[i-1][1] == 'NNP' or pos_tags[i-1][1] == 'NNPS'):\n          full_token = pos_tags[i-1][0] + ' ' + pos_tags[i][0]\n          several_token_propn.append(full_token)\n\n    return(several_token_propn)\nproper_nouns = extract_proper_nouns(\"my_file_name.txt\")\nprint(proper_nouns)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muhamamd_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "nltk.download('wordnet')\ndef common_lemma(my_file_name):\n    ## Read the file contents\n    with open(my_file_name, \"r\") as file:\n        text = file.read()\n\n    ## Tokenize the text into sentences\n    sentences = nltk.sent_tokenize(text)\n    common_lemmas = {}\n\n    ## Extract common lemmas from each sentence\n    for sentence in sentences:\n        ## Tag the words with their parts of speech\n        pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n\n        ## Extract verbs and nouns\n        verbs_nouns = []\n        for word, tag in pos_tags:\n            if tag.startswith(\"VB\") or tag.startswith(\"NN\"):\n                verbs_nouns.append(word)\n\n        ## Lemmatize each verb and noun\n        lemmas = [nltk.WordNetLemmatizer().lemmatize(word) for word in verbs_nouns]\n\n        ## Count occurrences of each lemma\n        lemma_counts = nltk.FreqDist(lemmas)\n\n        ## Identify common lemmas (appear in both verbs and nouns)\n        for lemma, count in lemma_counts.items():\n            if count > 1:\n                if lemma not in common_lemmas:\n                    common_lemmas[lemma] = []\n                common_lemmas[lemma].extend([word for word in verbs_nouns if nltk.WordNetLemmatizer().lemmatize(word) == lemma])\n\n    return common_lemmas\ncommon_lemmas = common_lemma(\"my_file_name.txt\")\nprint(common_lemmas)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muhamamd_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\nbbc_news = pd.read_csv('bbc-news.csv')\nbbc_news\nprint(bbc_news['text'][2221])"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muhamamd_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\n## Get the count of each category\nclass_counts = bbc_news['category'].value_counts()\n\n## Create a bar graph for the count\nplt.barh(class_counts.index, class_counts.values)\nplt.ylabel(\"Topic\")\nplt.xlabel(\"Number of Articles\")\nplt.title(\"Number of Articles per Category\")\n\n## Annonate bars with frequencies\nfor i, (freq, label) in enumerate(zip(class_counts.values, class_counts.index)):\n  plt.annotate(str(freq), (freq , i), va='center', fontsize=10)\n\n## Show plot\nplt.grid(axis='x', linestyle='--', alpha=0.6)\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muhamamd_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import pandas as pd\nimport string\n\ndef remove_punctuation(corpus):\n    # Here comes your code\n    corpus['text'] = corpus['text'].str.replace('[^\\w\\s]', '')\n    return corpus\n\ndef remove_numbers(corpus):\n    corpus['text'] = corpus['text'].str.replace('\\d+', '')\n    return corpus\nremove_punctuation(bbc_news)\nremove_numbers(bbc_news)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muhamamd_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nspacy.cli.download(\"en_core_web_lg\")\n### Part a)\nnlp = spacy.load(\"en_core_web_lg\")\n### Part b)\nfrom sklearn.model_selection import train_test_split\n\nX = bbc_news['text']\ny = bbc_news['category']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n### Part c)\nimport numpy as np\n\ndef vectorization(corpus):\n    vector_representations = []\n\n    for text in corpus:\n        doc = nlp(text)\n        vector_representations.append(doc.vector)\n\n    vector_representations = np.array(vector_representations)\n    return vector_representations\n\ntraining_vectors = vectorization(X_train)\ntest_vectors = vectorization(X_test)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Muhamamd_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "### Part a)\nfrom sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(hidden_layer_sizes=(32, 16), activation='relu', solver='adam', max_iter=200)\nmlp.fit(training_vectors, y_train)\nfrom sklearn.svm import SVC\n\nsvc = SVC(kernel='rbf', C=1.0, gamma=0.1)\nsvc.fit(training_vectors, y_train)\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB(var_smoothing=0.01)\ngnb.fit(training_vectors, y_train)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nmlp_predictions = mlp.predict(test_vectors)\nmlp_accuracy = accuracy_score(y_test, mlp_predictions)\nmlp_confusion_matrix = confusion_matrix(y_test, mlp_predictions)\n\nsvc_predictions = svc.predict(test_vectors)\nsvc_accuracy = accuracy_score(y_test, svc_predictions)\nsvc_confusion_matrix = confusion_matrix(y_test, svc_predictions)\n\n\ngnb_predictions = gnb.predict(test_vectors)\ngnb_accuracy = accuracy_score(y_test, gnb_predictions)\ngnb_confusion_matrix = confusion_matrix(y_test, gnb_predictions)\n\n# Compare and analyze the results\nprint(\"MLP:\", mlp_accuracy)\nprint(\"MLPConfusion Matrix:\")\nprint(mlp_confusion_matrix)\n\nprint(\"\\nSupport Vector Machine Accuracy:\", svc_accuracy)\nprint(\"Support Vector Machine Confusion Matrix:\")\nprint(svc_confusion_matrix)\n\nprint(\"\\nGaussian Naive Bayes Accuracy:\", gnb_accuracy)\nprint(\"Gaussian Naive Bayes Confusion Matrix:\")\nprint(gnb_confusion_matrix)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Yagmur_Caglar.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "def extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    # here comes your code\n\n    #read file\n    with open(file_path + my_file_name, 'r') as file:\n        text = file.read()\n\n    tokens = word_tokenize(text)\n    tagged_tokens = pos_tag(tokens)\n\n    current_proper_noun = \"\"\n\n    for token, pos in tagged_tokens:\n        if pos == 'NNP':  #NNP for proper noun\n            current_proper_noun += \" \" + token\n        else:\n            if current_proper_noun != \"\" and len(current_proper_noun.split()) > 1:\n                several_token_propn.append(current_proper_noun.strip())\n            current_proper_noun = \"\"\n\n    #checking last word in case it's a proper noun\n    if current_proper_noun != \"\" and len(current_proper_noun.split()) > 1:\n        several_token_propn.append(current_proper_noun.strip())\n\n\n    return(several_token_propn)\nextract_proper_nouns(\"mytext.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Yagmur_Caglar.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    # here comes your code\n\n    with open(file_path + my_file_name, 'r') as file:\n        text = file.read()\n\n    #tokenize the text and perform part-of-speech tagging\n    tokens = word_tokenize(text)\n    tagged_tokens = pos_tag(tokens)\n\n    lemmatizer = WordNetLemmatizer()\n\n    temp_lemma = {}\n    for token, pos in tagged_tokens:\n        #use lemmatization on verbs and nouns only\n        if pos.startswith('VB') or pos.startswith('NN'):\n            lemma = lemmatizer.lemmatize(token, pos=pos[0].lower())\n            if lemma in temp_lemma:\n                temp_lemma[lemma].append((token, pos))\n            else:\n                temp_lemma[lemma] = [(token, pos)]\n\n    #check if there is at least one verb token, discard if all noun\n    for lemma, value in temp_lemma.items():\n      flag = 0\n      for (token, pos) in value:\n        if pos.startswith('VB'):\n          flag=1\n          break\n\n      if flag and len(value)>1:\n        for (token, pos) in value:\n          if lemma in tokens_with_common_lemma:\n            tokens_with_common_lemma[lemma].append(token)\n          else:\n            tokens_with_common_lemma[lemma] = [token]\n\n    return(tokens_with_common_lemma)\ncommon_lemma(\"mytext.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Yagmur_Caglar.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\ndf = pd.read_csv(file_path + 'bbc-news.csv')\nprint(df)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Yagmur_Caglar.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\n\nclass_counts = df['category'].value_counts()\nclass_counts.plot(kind='bar')\nplt.title('Number of Articles in Each Topical Area')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Yagmur_Caglar.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    # Here comes your code\n    cleaned_corpus = corpus.replace('[{}]'.format(string.punctuation), '')\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    cleaned_corpus = corpus.replace('\\d+', '')\n    return(cleaned_corpus)\ndf['text'].apply(remove_punctuation)\ndf['text'].apply(remove_numbers)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Yagmur_Caglar.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "#i could not successfully run the code for big dataset.\n#i used these parameters\ntoken_length = 150\ndata_length = 1000\ndef vectorize(text, num_token=token_length):\n  doc = nlp(text)\n  #limit to the first token_length number of tokens\n  tokens = [token.vector for token in doc[:num_token]]\n  return np.array(tokens)\n# Here comes your code\nnlp = en_core_web_sm.load()\ndata_subset = df.head(data_length)\n#split the dataset\nX_train, X_test, y_train, y_test = train_test_split(data_subset['text'],data_subset['category'], test_size=0.3, random_state=101, shuffle=True)\n\n#vectorization, takes only specified number of tokens\ntrain_vectors = np.array(list(X_train.apply(vectorize)))\ntest_vectors = np.array(list(X_test.apply(vectorize)))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Yagmur_Caglar.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\n# Here comes your code\n\ndef evaluate_classifier(classifier, test_vectors, test_labels):\n    predictions = classifier.predict(test_vectors)\n    accuracy = accuracy_score(test_labels, predictions)\n    confusion_mat = confusion_matrix(test_labels, predictions)\n    return accuracy, confusion_mat\n#MLP Classifier\nmlp_classifier = MLPClassifier(random_state=42)\nmlp_classifier.fit(train_vectors, y_train.values)\n\naccuracy_mlp, confusion_mat_mlp = evaluate_classifier(mlp_classifier, test_vectors, y_test.values)\nprint(\"\\nMLP Classifier Accuracy:\", accuracy_mlp)\nprint(\"Confusion Matrix (MLP Classifier):\\n\", confusion_mat_mlp)\n#logistic regression\nlogistic_regression = LogisticRegression(random_state=42)\nlogistic_regression.fit(train_vectors, y_train.values)\n\naccuracy_lr, confusion_mat_lr = evaluate_classifier(logistic_regression, test_vectors, y_test.values)\nprint(\"Logistic Regression Accuracy:\", accuracy_lr)\nprint(\"Confusion Matrix (Logistic Regression):\\n\", confusion_mat_lr)\n#random forest\nrandom_forest = RandomForestClassifier(random_state=42)\nrandom_forest.fit(train_vectors, y_train.values)\n\naccuracy_rf, confusion_mat_rf = evaluate_classifier(random_forest, test_vectors, y_test.values)\nprint(\"\\nRandom Forest Accuracy:\", accuracy_rf)\nprint(\"Confusion Matrix (Random Forest):\\n\", confusion_mat_rf)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "!python -m spacy download en_core_web_sm\nimport spacy\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    #Load the spaCy small English model\n    nlp = spacy.load('en_core_web_sm')\n    #Read contents of file\n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n\n    # Process the text with spaCy\n    doc = nlp(text)\n\n    #Extract proper nouns with more than one token \n    for chunk in doc.noun_chunks:\n        if len(chunk) > 1 and all(token.pos_ == 'PROPN' for token in chunk):\n            several_token_propn.append(chunk.text)\n    \n    return(several_token_propn)\n#Example\nmy_file_name = \"my_file_name.txt\"\nresult = extract_proper_nouns(my_file_name)\nprint (result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "from collections import defaultdict\ndef common_lemma(my_file_name):\n    tokens_with_common_lemma = defaultdict(list)\n    #Load the spaCy small English model\n    nlp = spacy.load('en_core_web_sm')\n    #Read contents of file\n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n    doc = nlp(text)\n\n    # Iterate through the tokens in the processed text\n    for token in doc:\n        # Check if the token is a verb or a noun and not already present in the dictionary, if already present then dont add \n        if token.pos_ == 'VERB' and (not token.lemma_ in tokens_with_common_lemma or not token.text in tokens_with_common_lemma):\n            tokens_with_common_lemma[token.lemma_].append(token.text)\n        elif token.pos_ == 'NOUN' and (not token.lemma_ in tokens_with_common_lemma or not token.text in tokens_with_common_lemma):\n            tokens_with_common_lemma[token.lemma_].append(token.text)\n    \n    updated_dict = {}\n    # Remove all values from dictioanry that dont have both noun and verb with the same lemma\n    for lemma, tokens in tokens_with_common_lemma.items():\n        if len(tokens) > 1:\n            updated_dict[lemma] = tokens\n    tokens_with_common_lemma = updated_dict   \n    \n    return tokens_with_common_lemma\n#Example\nmy_file_name = \"my_file_name.txt\"\nresult = common_lemma(my_file_name)\nprint (result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\ndata = pd.read_csv(\"bbc-news.csv\")\ndata"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\nimport matplotlib.pyplot as plt\n\n#Get all the unique cateogires from the data\ncategories = data['category'].unique()\n#Get pandas series with categories and their count\ncount_per_value = data['category'].value_counts()\n#Get list of counts of each category\ncounts_list = count_per_value.tolist()\n\n# Create a bar chart\nplt.figure(figsize=(8, 6))\nplt.bar(categories, counts_list, color=['blue', 'green', 'red', 'purple', 'orange'])\nplt.xlabel(\"Category\")\nplt.ylabel(\"Number of Articles\")\nplt.title(\"Number of Articles per Category\")\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Show the plot\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\nimport string\ndef remove_punctuation(corpus):\n    # Remove punctuation from the text\n    cleaned_corpus = ''.join([char for char in corpus if char not in string.punctuation])\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # Remove numbers from the text using regular expression\n    cleaned_corpus = re.sub(r'\\d+', '', corpus)\n    return(cleaned_corpus)\ndata['text'] = data['text'].apply(remove_punctuation)\ndata['text'] = data['text'].apply(remove_numbers)\n\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_lg\")\n\n# Load dataset \ndf = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\Masters in Computer Science\\\\Semester 4\\\\bbcnews.csv', encoding='utf-8')\n\n\n# Taking only 1000 dataset\ndf = df.head(1000)\n\n# using 150 tokens of each article\ndf['text'] = df['text'].apply(lambda x: ' '.join(x.split()[:150]))\n\n# Split the data into training and test sets into 70% and 30% using the scikit-learn\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=101, shuffle=True)\n\n# Convert each article to a vector representation\ndef text_to_vector(text):\n    # Process the text using spaCy\n    doc = nlp(text)\n    # Return the vector representation of the entire document\n    return doc.vector\n\n# Apply the text_to_vector function to the training and test sets\ntrain_vectors = train_df['text'].apply(text_to_vector).to_list()\ntest_vectors = test_df['text'].apply(text_to_vector).to_list()\n\n# Convert vectors to arrays\ntrain_vectors = pd.DataFrame(train_vectors).to_numpy()\ntest_vectors = pd.DataFrame(test_vectors).to_numpy()\n\n# shapes of the resulting arrays\nprint(\"Shape of the training vectors:\", train_vectors.shape)\nprint(\"Shape of the test vectors:\", test_vectors.shape)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Abhishek_Murtuza.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "import spacy\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\n\n#the first 1000 rows)\ndf = df.head(1000)\n\n#150 tokens of each article\ndf['text'] = df['text'].apply(lambda x: ' '.join(x.split()[:150]))\n\n# Split the data into training and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=101, shuffle=True)\n\n# Vectorize text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=1000)\ntrain_vectors = tfidf_vectorizer.fit_transform(train_df['text']).toarray()\ntest_vectors = tfidf_vectorizer.transform(test_df['text']).toarray()\n\n\n\n\n# Task a) Now lets Train 3 different models\n\n# Model 1: MLPClassifier\nmlp_model = MLPClassifier(random_state=101)\nmlp_model.fit(train_vectors, train_df['category'])\n\n# Model 2: RandomForestClassifier\nrf_model = RandomForestClassifier(random_state=101)\nrf_model.fit(train_vectors, train_df['category'])\n\n# Model 3: Support Vector Classifier (SVC)\nsvc_model = SVC(random_state=101)\nsvc_model.fit(train_vectors, train_df['category'])\n\n# Task b) Evaluate the classifiers\n\n# Predictions on the test set\nmlp_predictions = mlp_model.predict(test_vectors)\nrf_predictions = rf_model.predict(test_vectors)\nsvc_predictions = svc_model.predict(test_vectors)\n\n\n# Accuracy and Confusion Matrix\nmlp_accuracy = accuracy_score(test_df['category'], mlp_predictions)\nrf_accuracy = accuracy_score(test_df['category'], rf_predictions)\nsvc_accuracy = accuracy_score(test_df['category'], svc_predictions)\n\n\n\nmlp_confusion_matrix = confusion_matrix(test_df['category'], mlp_predictions)\nrf_confusion_matrix = confusion_matrix(test_df['category'], rf_predictions)\nsvc_confusion_matrix = confusion_matrix(test_df['category'], svc_predictions)\n\n# Display results\nprint(\"MLP Classifier Accuracy:\", mlp_accuracy)\nprint(\"Random Forest Classifier Accuracy:\", rf_accuracy)\nprint(\"Support Vector Classifier Accuracy:\", svc_accuracy)\n\nprint(\"\\nMLP Classifier Confusion Matrix:\")\nprint(mlp_confusion_matrix)\n\nprint(\"\\nRandom Forest Classifier Confusion Matrix:\")\nprint(rf_confusion_matrix)\n\nprint(\"\\nSupport Vector Classifier Confusion Matrix:\")\nprint(svc_confusion_matrix)\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Maria_Artemyeva.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\nnlp = spacy.load('en_core_web_sm')\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    with open(my_file_name) as f:\n        text = f.read()\n    doc = nlp(text)\n    # Extract proper nouns by getting chunks longer than 1 tagged as \n    several_token_propn = [c.text for c in doc.noun_chunks if c.root.pos_ == 'PROPN' and len(c) > 1]\n    return(several_token_propn)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Maria_Artemyeva.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import re\n\ndef common_lemma(my_file_name):\n    with open(my_file_name, 'r') as f: #open file and extract words\n        t = f.read()\n        t = t.lower() \n        words = re.findall(r'\\b\\w+\\b', t)\n    tokens_with_common_lemma = {}\n    for w in words:\n        doc = nlp(w)\n        #forming dictionary with nouns and verbs\n        for t in doc:\n            if t.pos_ in ['NOUN', 'VERB']:\n                l = t.lemma_\n                if l not in tokens_with_common_lemma:\n                    tokens_with_common_lemma[l] = []\n                tokens_with_common_lemma[l].append(w)\n    #filtering dictionary and removing everything except entries containing both verbs and nouns\n    tokens_with_common_lemma = {l: wl for l, wl in tokens_with_common_lemma.items() \n                                if any(nlp(w)[0].tag_.startswith('VB') for w in wl) and len(wl) > 1}     \n    return(tokens_with_common_lemma)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Maria_Artemyeva.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\ndata = pd.read_csv('bbc-news.csv', usecols=['category', 'text'])\nprint(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Maria_Artemyeva.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "topics = data['category'].value_counts()\nax = topics.plot(kind='barh')\n#add exact number\nfor i, c in enumerate(topics):\n    ax.text(c, i, str(c))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Maria_Artemyeva.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    cleaned_corpus =corpus.copy()\n    cleaned_corpus['text'] = cleaned_corpus['text'].str.replace('[^\\w\\s]', '', regex=True)\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    cleaned_corpus = corpus.copy()\n    cleaned_corpus['text']= cleaned_corpus['text'].str.replace('\\d+','', regex=True)\n    return(cleaned_corpus)\n\ndata= remove_punctuation(data)\ndata= remove_numbers(data)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Maria_Artemyeva.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import spacy\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\nnlp = spacy.load('en_core_web_lg')\n\nX = data['text']\ny = data['category']  \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101, shuffle=True)\n\n\nX_train_vectors = [nlp(text).vector for text in X_train]\nX_test_vectors = [nlp(text).vector for text in X_test]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Maria_Artemyeva.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n#MLP classifier\nmlp_cl = MLPClassifier(max_iter=1000)\nmlp_cl.fit(X_train_vectors, y_train)\n\nmlp_pr = mlp_cl.predict(X_test_vectors)\n\nprint(\"Accuracy for mlp classifier is:\", accuracy_score(y_test, mlp_pr))\nprint(\"Confusion matrix for mlp classifier is:\")\nprint(confusion_matrix(y_test, mlp_pr))\n\n\n#SVM classifier\nsvc_cl = SVC()\nsvc_cl.fit(X_train_vectors, y_train)\nsvc_pr = svc_cl.predict(X_test_vectors)\n\nprint(\"Accuracy for svm classifier is:\", accuracy_score(y_test, svc_pr))\nprint(\"Confusion matrix for svm classifier is:\")\nprint(confusion_matrix(y_test, svc_pr))\n\n#Random forest classifier\n\nrf_cl = RandomForestClassifier()\nrf_cl.fit(X_train_vectors, y_train)\n\nrf_pr = rf_cl.predict(X_test_vectors)\n\nprint(\"Accuracy for random forest classifier is:\", accuracy_score(y_test, rf_pr))\nprint(\"Confusion matrix for random forest classifier is:\")\nprint(confusion_matrix(y_test, rf_pr))\n"
    },
    {
        "file_name": "Assignment_3_Bondarenko_Nikita.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\ndef extract_proper_nouns(my_file_name):\n    #loading small english model \n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n    \n    doc = nlp(text)\n    \n    # filter entities that already have proper nouns that have more than 1 token\n    several_token_propn = [ent.text for ent in doc.ents if len(ent.text.split()) > 1]\n\n    return several_token_propn"
    },
    {
        "file_name": "Assignment_3_Bondarenko_Nikita.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    # load english small model\n    nlp = spacy.load(\"en_core_web_sm\")\n    tokens_with_common_lemma = {}\n    \n    # read text file\n    with open(my_file_name, 'r') as file:\n        text = file.read()\n    \n    # process text\n    doc = nlp(text)\n\n    for token in doc:\n        # Check if the token is a verb or a noun\n        if token.pos_ in [\"VERB\", \"NOUN\"]:\n            # If the lemma is not in the dictionary, add it\n            if token.lemma_ not in tokens_with_common_lemma:\n                tokens_with_common_lemma[token.lemma_] = []\n\n            # Add the word form to the list associated with the lemma\n            if token.text not in tokens_with_common_lemma[token.lemma_]:\n                tokens_with_common_lemma[token.lemma_].append(token.text)\n\n    return(tokens_with_common_lemma)"
    },
    {
        "file_name": "Assignment_3_Bondarenko_Nikita.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\ndataframe = pd.read_csv('bbc-news.csv')\ndataframe.head(10)"
    },
    {
        "file_name": "Assignment_3_Bondarenko_Nikita.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\n\n# Count num of articles for each label 'category' \narticle_counts = dataframe['category'].value_counts()\n\n# show number of articles \nprint(article_counts) \n\n# plot the histogram\narticle_counts.plot(kind='bar')\nplt.title('Num of articles for each topical area (class label)')\nplt.show()"
    },
    {
        "file_name": "Assignment_3_Bondarenko_Nikita.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\n\ndef remove_punctuation(corpus):\n    # Apostrophe and dashes are considered punctuation in this case\n    cleaned_corpus = re.sub(r'[^\\w\\s]', '', corpus)\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    cleaned_corpus = re.sub(r'\\d+', '', corpus)\n    return(cleaned_corpus)"
    },
    {
        "file_name": "Assignment_3_Bondarenko_Nikita.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"parser\", \"ner\"])\n\n\nlabels = dataframe['category']\narticles = dataframe['text']\n\ndoc = np.array([nlp(article).vector for article in articles])\n\nx = doc\ny = labels\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101, shuffle=True)"
    },
    {
        "file_name": "Assignment_3_Bondarenko_Nikita.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\n# MLP classifier\nmlp = MLPClassifier(random_state=101)\nmlp.fit(X_train, y_train)\n\n# Random Forest classifier\nrf = RandomForestClassifier(random_state=101)\nrf.fit(X_train, y_train)\n\n# Support Vector Machine classifier\nsvc = SVC(random_state=101)\nsvc.fit(X_train, y_train)\n\ndef evaluate_model(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    conf_matrix = confusion_matrix(y_test, predictions)\n    return accuracy, conf_matrix\n\n# evaluate MLP classifier\naccuracy_mlp, conf_matrix_mlp = evaluate_model(mlp, X_test, y_test)\n\n# evaluate Random Forest classifier\naccuracy_rf, conf_matrix_rf = evaluate_model(rf, X_test, y_test)\n\n# evaluate SVM classifier\naccuracy_svc, conf_matrix_svc = evaluate_model(svc, X_test, y_test)\n\n# results\nprint(\"MLP Classifier - Accuracy:\", accuracy_mlp, \"Confusion Matrix:\\n\", conf_matrix_mlp)\nprint(\"Random Forest - Accuracy:\", accuracy_rf, \"Confusion Matrix:\\n\", conf_matrix_rf)\nprint(\"SVM Classifier - Accuracy:\", accuracy_svc, \"Confusion Matrix:\\n\", conf_matrix_svc)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Niklas_Dobberstein.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_proper_nouns(my_file_name):\n\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    doc = nlp(text)\n    proper_nouns = []\n    current_proper_noun = []\n\n    for tok in doc:\n        if tok.pos_ == \"PROPN\":\n            current_proper_noun.append(tok.text)\n        else:\n            if len(current_proper_noun) > 1:\n                proper_nouns.append(\" \".join(current_proper_noun))\n            current_proper_noun = []\n\n    if len(current_proper_noun) > 1:\n        proper_nouns.append(\" \".join(current_proper_noun))\n\n    return proper_nouns\n\nextract_proper_nouns(\"test_file.txt\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Niklas_Dobberstein.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    with open(my_file_name, 'r', encoding='utf-8') as f:\n        text = f.read()\n\n    doc = nlp(text)\n\n    lemma_dict = {}\n\n    for token in doc:\n        if token.pos_ in [\"VERB\", \"NOUN\"]:\n            lemma = token.lemma_\n            if lemma in lemma_dict:\n                lemma_dict[lemma].append(token.text)\n            else:\n                lemma_dict[lemma] = [token.text]\n\n    result_dict = {lemma: words for lemma, words in lemma_dict.items() if len(set([tok.pos_ for tok in nlp(\" \".join(words))])) > 1}\n\n    return(result_dict)\n\nresult1 = common_lemma(\"test_file2.txt\")\nprint(result1)\n\nresult2 = common_lemma(\"test_file3.txt\")\nprint(result2)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Niklas_Dobberstein.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "import pandas as pd\n\ndf = pd.read_csv(\"bbc-news.csv\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Niklas_Dobberstein.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "import matplotlib.pyplot as plt\nunique_cat = df[\"category\"].unique()\n\ncounts = []\nfor cat in unique_cat:\n    count = df[df[\"category\"] == cat][\"text\"].count()\n    counts.append(count)\n\nplt.figure(figsize=(10, 6))\nplt.bar(unique_cat, counts, color='skyblue')\nplt.xlabel('Topical Area (Class Label)')\nplt.ylabel('Number of Articles')\nplt.title('Number of Articles for Each Topical Area')\nplt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Niklas_Dobberstein.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "import re\ndef remove_punctuation(corpus):\n    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    cleaned_corpus = [c.translate(str.maketrans('', '', punc)) for c in corpus]\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    cleaned_corpus = [re.sub(r'[0-9]', '', s) for s in corpus]\n    return cleaned_corpus\n\nremove_numbers(remove_punctuation([\"test is 21!.a test.\"]))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Niklas_Dobberstein.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom concurrent.futures import ProcessPoolExecutor\n\nnlp = spacy.load(\"en_core_web_lg\")\n\narticles_train, articles_test = train_test_split(df, test_size=0.3, random_state=101, shuffle=True)\n\ndef tokenize_article(article):\n    return nlp(article).vector\n\nwith ProcessPoolExecutor() as executor:\n    articles_train_vectors = list(executor.map(tokenize_article, articles_train[\"text\"]))\n    articles_test_vectors = list(executor.map(tokenize_article, articles_test[\"text\"]))\n\narticles_train_vectors = np.array(articles_train_vectors)\ny_train = articles_train[\"category\"]\narticles_test_vectors = np.array(articles_test_vectors)\ny_test = articles_test[\"category\"]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Niklas_Dobberstein.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 2, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, multilabel_confusion_matrix\n\nrf_model = RandomForestClassifier()\nrf_model.fit(articles_train_vectors, y_train)\n\nsvc_model = SVC(kernel='rbf')\nsvc_model.fit(articles_train_vectors, y_train)\n\nmlp_model = MLPClassifier(hidden_layer_sizes=(256, 128), activation='relu', solver='adam')\nmlp_model.fit(articles_train_vectors, y_train)\n\nrf_predictions = rf_model.predict(articles_test_vectors)\nrf_accuracy = accuracy_score(y_test, rf_predictions)\nprint(\"Rf Accuracy:\", rf_accuracy)\nrf_confusion_matrix = multilabel_confusion_matrix(y_test, rf_predictions)\n# print(\"Rf Confusion Matrix:\")\n# print(rf_confusion_matrix.shape)\n\nsvc_predictions = svc_model.predict(articles_test_vectors)\nsvc_accuracy = accuracy_score(y_test, svc_predictions)\nprint(\"SVC Accuracy:\", svc_accuracy)\nsvc_confusion_matrix = multilabel_confusion_matrix(y_test, svc_predictions)\n# print(\"SVC Confusion Matrix:\")\n# print(svc_confusion_matrix)\n\nmlp_predictions = mlp_model.predict(articles_test_vectors)\nmlp_accuracy = accuracy_score(y_test, mlp_predictions)\nprint(\"MLP Accuracy:\", mlp_accuracy)\nmlp_confusion_matrix = multilabel_confusion_matrix(y_test, mlp_predictions)\n# print(\"MLP Confusion Matrix:\")\n# print(mlp_confusion_matrix)\nfrom sklearn.metrics import  ConfusionMatrixDisplay\n\nfor i in range(len(unique_cat)):\n    rf_disp = ConfusionMatrixDisplay(rf_confusion_matrix[i])\n    rf_disp.plot(cmap='coolwarm')\n    plt.title(f'Random Forest Confusion Matrix {unique_cat[i]}')\n    plt.ylabel(f'True {unique_cat[i]}')\n    plt.xlabel(f'Predicted {unique_cat[i]}')\n    plt.show()\n\n# svc_disp = ConfusionMatrixDisplay(svc_confusion_matrix)\n# svc_disp.plot(cmap='coolwarm')\n# plt.title('SVC Confusion Matrix')\n# plt.ylabel('True Label')\n# plt.xlabel('Predicted Label')\n# plt.show()\n\n# mlp_disp = ConfusionMatrixDisplay(mlp_confusion_matrix)\n# mlp_disp.plot(cmap='coolwarm')\n# plt.title('MLP Confusion Matrix')\n# plt.ylabel('True Label')\n# plt.xlabel('Predicted Label')\n# plt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Carl_Jacobsen.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "import re\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n#Takes an entity and returns wheter it is a proper noun with more then one token.\ndef is_a_long_proper_noun(ent):\n    for i in range(len(ent)):\n        if ent[i].pos_ != \"PROPN\":\n            return False\n    return len(ent) > 1\n\ndef extract_proper_nouns(my_file_name):\n    several_token_propn = []\n    # here comes your code\n    #Load language\n    nlp = spacy.load(\"en_core_web_sm\")\n    #Open file\n    with open(my_file_name) as file:\n        document = nlp(file.read())\n        \n    #Adds any entity which is a proper noun with more then one token.\n    for ent in document.ents:\n        if is_a_long_proper_noun(ent):\n            several_token_propn.append(str(ent))\n    return several_token_propn"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Carl_Jacobsen.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "def common_lemma(my_file_name):\n    tokens_with_common_lemma = {}\n    # here comes your code\n    nlp = spacy.load(\"en_core_web_sm\")\n    with open(my_file_name) as file:\n        document = nlp(file.read())\n    \n    #A dictionary with the lemmata of the verbs as keys and the corresponding verbs as values.\n    lemmata_verbs = {}\n    #Same for nouns\n    lemmata_nouns = {}\n    \n    #Lets fill these two dictionaries\n    for token in document:\n        #If necessary add the lemma as a key and then add the verb as value.\n        if token.pos_ == \"VERB\":\n            if lemmata_verbs.get(token.lemma_) == None:\n                lemmata_verbs[token.lemma_] = []\n            lemmata_verbs[token.lemma_].append(token.text)\n        #Same for nouns.\n        if token.pos_ == \"NOUN\":\n            if lemmata_nouns.get(token.lemma_) == None:\n                lemmata_nouns[token.lemma_] = []\n            lemmata_nouns[token.lemma_].append(token.text)\n    \n    #Combine the to dictionaries to get the desired one.\n    for lemma in lemmata_verbs.keys():\n        if lemma in lemmata_nouns.keys():\n            tokens_with_common_lemma[lemma] = list(\n                set().union(lemmata_verbs[lemma], lemmata_nouns[lemma])\n            )\n    \n    return(tokens_with_common_lemma)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Carl_Jacobsen.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\nimport pandas as pd\n\ndocument = pd.read_csv(\"bbc-news.csv\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Carl_Jacobsen.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\n#Lets create a dictionary with the areas as keys and their number of articles as values.\nfrom collections import defaultdict\narticle_count = defaultdict(lambda : 0) #This is like an dictionary with the value 0 for any key.\nfor article_index in range(document.shape[0]):\n    article_count[\n        document.loc[article_index].at[\"category\"]\n    ] += 1\n\n#Now we plot it.\ndf = pd.DataFrame.from_dict(dict(article_count), orient = \"index\")\nplot = df.plot(xlabel = \"Article areas\", ylabel = \"Articles in that ares\", kind = \"bar\")\nprint(plot)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Carl_Jacobsen.ipynb",
        "question": "### Task 5 (2 point)\na) Preprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    # Here comes your code\n    cleaned_corpus = \"\"\n    for letter in corpus:\n        if letter not in \", . ! ? : ; - \".split(\" \"):\n            cleaned_corpus += letter\n    return(cleaned_corpus)\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    cleaned_corpus = \"\"\n    for letter in corpus:\n        if letter not in \"1 2 3 4 5 6 7 8 9 0\".split(\" \"):\n            cleaned_corpus += letter\n    return(cleaned_corpus)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Carl_Jacobsen.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "# Here comes your code\n#b)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX_train_pre, X_test_pre, y_train, y_test = train_test_split(\n    np.array(document[[\"text\"]]),\n    np.array(document[[\"category\"]]),\n    test_size = 0.3,\n    random_state = 101\n)\n#c) Vectorize the data.\nnlp = spacy.load(\"en_core_web_sm\")\nX_train = np.array([\n    nlp(X_train_pre[index,0]).vector for index in range(X_train_pre.shape[0])]\n)\nX_test = [nlp(X_test_pre[index,0]).vector for index in range(X_test_pre.shape[0])]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Carl_Jacobsen.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# Here comes your code\n#a) and b) for each classifier\nfrom sklearn.metrics import confusion_matrix\n#MLPClassifier\nfrom sklearn.neural_network import MLPClassifier\nMLP = MLPClassifier(max_iter=1000).fit(\n    X_train, y_train.reshape((y_train.shape[0],))\n)\nprint(\"The accuracy of the MLP is {}.\".format(MLP.score(X_test, y_test)))\nprint(\"The confusion matrix of the MLP is:\\n\", confusion_matrix(y_test, MLP.predict(X_test)))\n#SVM\nfrom sklearn.svm import SVC\nsvm = SVC().fit(\n    X_train, y_train.reshape((y_train.shape[0],))\n)\nprint(\"The accuracy of the SVM is {}.\".format(svm.score(X_test, y_test)))\nprint(\"The confusion matrix of the SVM is:\\n\", confusion_matrix(y_test, svm.predict(X_test)))\n#KNeighborsClassifier for K = 5\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors = 5).fit(\n    X_train, y_train.reshape((y_train.shape[0],))\n)\nprint(\"The accuracy of the KNeighborsClassifier is {}.\".format(neigh.score(X_test, y_test)))\nprint(\"The confusion matrix of the KNeighborsClassifier is:\\n\", confusion_matrix(y_test, neigh.predict(X_test)))"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 1 (2 points)\nWrite a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n**Example:**\ntext = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\nreturn = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**",
        "answer": "def extract_proper_nouns(my_file_name):\n\n    # spaCy english model\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, 'r', encoding='utf-8') as file:\n        text = file.read()\n\n    # process the text\n    doc = nlp(text)\n\n\n    several_token_propn = []\n    current_proper_noun = \"\"\n\n    # get nouns with more than one token\n    for token in doc:\n        # check if a given token in the processed text is a proper noun\n        if token.pos_ == 'PROPN':\n            current_proper_noun += token.text + \" \"\n        else:\n            # check if there is more than one token noun\n            if len(current_proper_noun.split()) > 1:\n                several_token_propn.append(current_proper_noun.strip())\n            current_proper_noun = \"\"\n\n    # check the last potential proper noun\n    if len(current_proper_noun.split()) > 1:\n        several_token_propn.append(current_proper_noun.strip())\n\n    return several_token_propn\n\n# file_name = \"test.txt\"\n# result = extract_proper_nouns(file_name)\n# print(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 2 (4 points)\nWrite a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n**Examples:**\n1.\ntext = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\nreturn = `{\"query\": [\"query\", \"querying\"]}`\n2.\ntext = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\nreturn = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**",
        "answer": "import spacy\n\ndef common_lemma(my_file_name):\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    with open(my_file_name, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n\n    doc = nlp(text)\n\n    tokens_with_common_lemma = {}\n\n    for token in doc:\n        # only nouns and verbs\n        if token.pos_ in [\"NOUN\", \"VERB\"]:\n            lemma = token.lemma_\n\n            if lemma in tokens_with_common_lemma:\n                tokens_with_common_lemma[lemma].append(token.text)\n            else:\n                tokens_with_common_lemma[lemma] = [token.text]\n\n    # filter out lemmas\n    # make a set of the values for each key to make sure it contains both verb and noun\n    tokens_with_common_lemma = {lemma: words for lemma, words in tokens_with_common_lemma.items()\n     if len(set(words).intersection(set(tokens_with_common_lemma.get(lemma, [])))) > 1}\n\n    return tokens_with_common_lemma\n\n# file_name = \"test2.txt\"\n# result = common_lemma(file_name)\n# print(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 3 (1 point)\na) Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005.",
        "answer": "# Here comes your code\ndf = pd.read_csv(\"bbc-news.csv\")\ndf"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 4 (1 point)\na) Show how many articles we have for each topical area (class label) in the dataset using a plot.",
        "answer": "# Here comes your code\narticle_counts = df['category'].value_counts()\nprint(article_counts)\n\nax = article_counts.plot(kind='bar', color='blue')\nplt.title('Number of Articles for Each Topical Area')\nplt.xlabel('Topical Area')\nplt.ylabel('Number of Articles')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n\nplt.show()\nplt.figure(figsize=(8, 8))\nplt.pie(article_counts, labels=article_counts.index, autopct='%1.1f%%', startangle=90, colors=plt.cm.Paired.colors)\nplt.title('Distribution of Articles Across Topical Areas')\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 5 (2 point)\nPreprocessing: Define two following functions and apply them to the dataset:\n1. Remove punctuation\n2. Remove any numbers",
        "answer": "def remove_punctuation(corpus):\n    # Here comes your code\n    cleaned_corpus = re.sub(r'[^\\w\\s]', '', corpus)\n    return cleaned_corpus\n\ndef remove_numbers(corpus):\n    # Here comes your code\n    cleaned_corpus = re.sub(r'\\d+', '', corpus)\n    return cleaned_corpus\n# sanity check\ndf.iloc[0][\"text\"]\ndf['text'] = df['text'].apply(remove_punctuation)\n# sanity check\ndf.iloc[0][\"text\"]\ndf['text'] = df['text'].apply(remove_numbers)\n# sanity check\ndf.iloc[0][\"text\"]"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 6 (4 points)\na) Load the **large model trained on the web text** provided by spaCy. (`en_core_web_lg`)\nb) Split the data into trainng and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\nc) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code.",
        "answer": "def spacy_vectorize(text):\n    doc = nlp(text)\n    return doc.vector\n# Here comes your code\nnlp = spacy.load(\"en_core_web_lg\")\n\ntrain_data, test_data = train_test_split(df, test_size=0.3, random_state=101, shuffle=True)\n\n# apply vectorization\ntrain_vectors = train_data['text'].apply(spacy_vectorize).tolist()\ntest_vectors = test_data['text'].apply(spacy_vectorize).tolist()\n\n# convert vectors to arrays\nX_train = pd.DataFrame(train_vectors).to_numpy()\nX_test = pd.DataFrame(test_vectors).to_numpy()\n\ny_train = train_data['category'].to_numpy()\ny_test = test_data['category'].to_numpy()"
    },
    {
        "file_name": "Intro2NLP_Assignment_3_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 7 (6 points)\na) Using the vectors from Task 6, train 3 different models provided by scikit-learn. (**Note:** One of the models must be `MLPClassifier`)\nb) Evaluate the classifiers on the test set and report the accuracy and confusion matrix. (**Hint:** You should build a confusion matrix for multi-class classification)",
        "answer": "# MLPClassifier\nmlp_model = MLPClassifier(random_state=101)\nmlp_model.fit(X_train, y_train)\n\nmlp_pred = mlp_model.predict(X_test)\nmlp_accuracy = accuracy_score(y_test, mlp_pred)\nmlp_conf_matrix = confusion_matrix(y_test, mlp_pred)\nprint(\"MLPClassifier Accuracy:\", mlp_accuracy)\n# Random Forest\nrf_model = RandomForestClassifier(random_state=101)\nrf_model.fit(X_train, y_train)\n\nrf_pred = rf_model.predict(X_test)\nrf_accuracy = accuracy_score(y_test, rf_pred)\nrf_conf_matrix = confusion_matrix(y_test, rf_pred)\nprint(\"Random Forest Accuracy:\", rf_accuracy)\n# Support Vector Machine (SVM)\nsvm_model = SVC(random_state=101)\nsvm_model.fit(X_train, y_train)\n\nsvm_pred = svm_model.predict(X_test)\nsvm_accuracy = accuracy_score(y_test, svm_pred)\nsvm_conf_matrix = confusion_matrix(y_test, svm_pred)\n\nprint(\"SVM Accuracy:\", svm_accuracy)\nplt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nsns.heatmap(mlp_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['category'].unique(), yticklabels=df['category'].unique())\nplt.title('MLPClassifier Confusion Matrix')\n\nplt.subplot(132)\nsns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['category'].unique(), yticklabels=df['category'].unique())\nplt.title('Random Forest Confusion Matrix')\n\nplt.subplot(133)\nsns.heatmap(svm_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['category'].unique(), yticklabels=df['category'].unique())\nplt.title('SVM Confusion Matrix')\n\n\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aylin_Gheisar.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "import datasets\n\n# We load the english subtask of the sem_eval dataset\ntweets = datasets.load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\nprint(tweets[\"validation\"][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aylin_Gheisar.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "import torch as pt\nfrom transformers import BertTokenizer\n\n# The tokenizer is loaded from huggingface\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# This function will convert the boolean labels to a float tensor\ndef labels_as_float(dataset):\n    label_names = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n    \n    # each attribute is converted as a float tensor and is assigned as a column to labels\n    labels = pt.zeros((len(dataset[\"Tweet\"]), len(label_names)), dtype=pt.float)\n    for i, label_name in enumerate(label_names):\n        labels[:, i] = pt.tensor(dataset[label_name], dtype=pt.float)\n        \n    return labels\n\ndef tokenize_data(dataset):\n    # We avoid loading the tokenizer multiple times by using the global variable\n    global tokenizer\n    \n    # Tweets are tokenized as batch; padding and truncation attributes ensure that all resulting tensors are the same size\n    text = dataset[\"Tweet\"]\n    encoded_dataset = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    \n    # converted labels are added as attribute to the mail dataset\n    encoded_dataset[\"labels\"] = labels_as_float(dataset)\n    \n    return(encoded_dataset)\n# Tokenization is batched as row by row processing is very slow; Original attributes are dropped in the process\ntokenized_tweets = tweets.map(tokenize_data, batched=True, batch_size=1000, remove_columns=['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'])\n# We can see that the tokenized data has the right attribute\nprint(tokenized_tweets[\"validation\"][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aylin_Gheisar.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoModelForSequenceClassification\n\n# cuda device is preferred if present, else cpu is used for training\ndevice = \"cuda:0\" if pt.cuda.is_available() else \"cpu\"\n\n# We load the bert model from huggingface and move it to the preferred device\nbert_clf = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=11,\n    problem_type=\"multi_label_classification\"\n).to(device)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# This function calculates the predicted labels of the model from output logits\ndef label_from_pred(pred):\n    # We first turn the logits into probability using the sigmoid function\n    sigmoid = pt.nn.Sigmoid()\n    probs = sigmoid(pt.Tensor(pred))\n\n    # The probabilities are quantized with a threshold of 0.5\n    y_pred = pt.zeros(probs.shape)\n    y_pred[pt.where(probs >= 0.5)] = 1\n    \n    return y_pred\n\ndef compute_metrics(predictions):\n    # the expected values can be found at label_ids attribute of the prediction output\n    # and predicted labels is calculated via label_from_pred function\n    actual_labels = predictions.label_ids\n    pred_labels = label_from_pred(predictions.predictions)\n\n    # Accuracy, F1-score, precision, recall metrics are calculated using micro averaging and are returned\n    metrics = {\n        \"accuracy\": accuracy_score(actual_labels, pred_labels),\n        \"precision\": precision_score(actual_labels, pred_labels, average=\"micro\"),\n        \"recall\": recall_score(actual_labels, pred_labels, average=\"micro\"),\n        \"f1_score\": f1_score(actual_labels, pred_labels, average=\"micro\")\n    }\n    \n    return metrics\nfrom transformers import Trainer, TrainingArguments\n\n# We will want to train the model for 5 epochs and save the chechpoint at every step\ntraining_args = TrainingArguments(\n    \"bert_model\",\n    evaluation_strategy = \"steps\",\n    save_strategy = \"steps\",\n    num_train_epochs=5,\n    load_best_model_at_end=True\n)\n\n# The trainer is constructed with the required parameters\ntrainer = Trainer(\n    bert_clf,\n    training_args,\n    train_dataset=tokenized_tweets[\"train\"],\n    eval_dataset=tokenized_tweets[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n# Training is achieved by simply running the train function\ntrainer.train()\n# We can evaluate the trained model using the metrics on the validation set\npred_bert = trainer.predict(tokenized_tweets[\"validation\"])\n\ncompute_metrics(pred_bert)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# We can vectorize the text data using Tfidf\nvectorizer = TfidfVectorizer()\n\nX_train = vectorizer.fit_transform(tweets[\"train\"][\"Tweet\"]).toarray()\n\n# The labels also must be converted to numerical values\ny_train = labels_as_float(tweets[\"train\"])\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# OneVsRest strategy is used to turn the svm into a multilabel classifier\nsvm_clf = OneVsRestClassifier(LinearSVC())\nsvm_clf.fit(X_train, y_train)\n# The performance of the svm model is evaluated on the validation set using the same metrics as the bert model\nX_valid = vectorizer.transform(tweets[\"validation\"][\"Tweet\"]).toarray()\ny_valid = labels_as_float(tweets[\"validation\"])\n\nsvm_pred = svm_clf.predict(X_valid)\nprint(\"Metrics for SVM Model:\")\nprint(\"Accuracy:\", accuracy_score(y_valid, svm_pred))\nprint(\"Recall:\", recall_score(y_valid, svm_pred, average=\"micro\"))\nprint(\"Precision:\", precision_score(y_valid, svm_pred, average=\"micro\"))\nprint(\"F1_score:\", f1_score(y_valid, svm_pred, average=\"micro\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aylin_Gheisar.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aksa_Aksa.ipynb",
        "question": "### **Note following cells download some packages or updates them to be able the run the code properly for Google Colab** (can ignore if not running it on google colab)",
        "answer": "# install required version on google colab\n!pip install tensorflow==2.14.0\n# install datasets on google colab\n!pip install datasets\n# install transformers for TrainingArguments and Trainer for Google Colab\n\n# To resolve error in Google Colab Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n!pip install -U transformers\n!pip install accelerate -U\n# check version of transformes\nimport transformers\ntransformers.__version__"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aksa_Aksa.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# Here comes your code\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n# overview of the dataset\ndataset\nlast_validation_data = dataset['validation'][-1]\nlast_validation_data\ndataset['validation'].features\n# get the labels\nlabels = list(dataset['train'].features.keys())[2:]\nlabels"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aksa_Aksa.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport numpy as np\ntokenizer_model = 'bert-base-uncased'\nbert_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\ndef tokenize_data(dataset):\n    # here comes your code\n\n    # get the input tweets\n    tweet = dataset[\"Tweet\"]\n\n    # encode the tweets\n    encoded_dataset = bert_tokenizer(tweet, padding=\"max_length\", truncation=True)\n\n    # get the labels\n    labels_batch = {key: dataset[key] for key in dataset.keys() if key in labels}\n\n    # initialize numpy array of shape (batch_size, num_labels) of type float\n    labels_matrix = np.zeros((len(tweet), len(labels)), dtype=np.float32)\n\n    for index, label in enumerate(labels):\n      labels_matrix[:, index] = labels_batch[label]\n\n    encoded_dataset[\"labels\"] = labels_matrix.tolist()\n\n    return(encoded_dataset)\nencoded_dataset = dataset.map(tokenize_data, batched=True, remove_columns=dataset['train'].column_names)\nencoded_dataset.set_format(\"torch\") # set the format of the data to PyTorch tensors\nencoded_dataset[\"train\"][\"labels\"]\n# print the keys() of the the last data point in the validation set in the encoded dataset\nlast_validation_example_keys = encoded_dataset[\"validation\"][-1].keys()\nlast_validation_example_keys"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aksa_Aksa.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# creating dictionaries for number value to integers and vice versa\n\nid2label = {idx:label for idx, label in enumerate(labels)}\nlabel2id = {label:idx for idx, label in enumerate(labels)}\nid2label, label2id\nfrom transformers import AutoModelForSequenceClassification\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                           problem_type=\"multi_label_classification\",\n                                                           num_labels=len(labels),\n                                                           id2label=id2label,\n                                                           label2id=label2id)\nfrom transformers import TrainingArguments, Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"my_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=8,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n    metric_for_best_model=\"f1\"\n)\nfrom transformers import EvalPrediction\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n\ndef multi_label_metrics(predictions, labels, threshold=0.5):\n\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = torch.nn.Sigmoid()\n    probabilities = sigmoid(torch.Tensor(predictions))\n\n\n    # applying thresholding\n    y_pred = np.zeros(probabilities.shape)\n    y_pred[np.where(probabilities >= threshold)] = 1\n\n    # Compute metrics\n    y_true = labels\n    f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average = 'micro')\n    recall = recall_score(y_true, y_pred, average = 'micro')\n\n\n    # return metrics dictionary\n    metrics = {'f1': f1,\n               'roc_auc': roc_auc,\n               'accuracy': accuracy,\n               'precision': precision,\n               'recall': recall,\n               }\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions,\n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds,\n        labels=p.label_ids)\n    return result\n# Train the BERT-based model using the Trainer API\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n    tokenizer=bert_tokenizer,\n)\ntrainer.train()\n# Evaluate the model on the validation\nresult = trainer.evaluate()\nmetrics_bert = {\n     'f1': result['eval_f1'],\n     'roc_auc': result['eval_roc_auc'],\n     'accuracy': result['eval_accuracy'],\n     'precision': result['eval_precision'],\n     'recall': result['eval_recall'],\n      }\nresult\npt_save_directory = \"pt_save_pretrained\"\nbert_tokenizer.save_pretrained(pt_save_directory)\nmodel.save_pretrained(pt_save_directory)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Aksa_Aksa.ipynb",
        "question": "### Inference",
        "answer": "pt_model = AutoModelForSequenceClassification.from_pretrained(\"pt_save_pretrained\")\ndef filter_scores(lst):\n    filtered_dicts = []\n    for dictionary_list in lst:\n        for item in dictionary_list:\n            if item['score'] > 0.5:\n                filtered_dicts.append(item)\n    return filtered_dicts\n\ndef filter_true_labels(data_dict):\n    true_labels = [label for label, value in data_dict.items() if value is True and label != 'ID' and label != 'Tweet']\n    return true_labels\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"./pt_save_pretrained\", top_k=None)\nresults = classifier(\"I am very angry at the team for losing the match and very sad for the players\")\nfilter_scores(results)\nresults = classifier(\"I am very angry at the team for losing the match and very sad for the players\")\nfilter_scores(results)\ndata_point = dataset[\"test\"][2]\ntweet = data_point[\"Tweet\"]\nprint(\"Test data point Tweet\", tweet)\nprint(\"Actual Labels\", filter_true_labels(data_point))\nresults = classifier(tweet)\nprint(\"Predicted Labels\", filter_scores(results))\nimport numpy as np\nimport pandas as pd\nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n# Get the train, test, and validation dataframe\ntrain_df = pd.DataFrame(dataset['train'])\nvalidation_df = pd.DataFrame(dataset['validation'])\ntest_df =  pd.DataFrame(dataset['test'])\n\n# drop the ID column\ntrain_df = train_df.drop('ID', axis=1)\ntest_df = test_df.drop('ID', axis=1)\nvalidation_df = validation_df.drop('ID', axis=1)\n# split input and labels\nX_train = train_df['Tweet']\nX_test = test_df['Tweet']\nX_validation = validation_df['Tweet']\n\ny_train = train_df.drop('Tweet', axis=1)\ny_test = test_df.drop('Tweet', axis=1)\ny_validation = validation_df.drop('Tweet', axis=1)\nX_train.iloc[0], y_train.iloc[0], X_validation.iloc[0], y_validation.iloc[0], X_test.iloc[0], y_test.iloc[0]\n# Apply TF-IDF\ntf_idf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\nX_idf = tf_idf_vectorizer.fit_transform(X_train)\n\n# create df containing X_idf corresponding each token\ntfidf_tokens = tf_idf_vectorizer.get_feature_names_out()\ndf_tfidfvect = pd.DataFrame(data = X_idf.toarray(),columns = tfidf_tokens)\ndf_tfidfvect\n# Define SVM Model\nsvm_classifier = svm.SVC(kernel='linear')\nmulti_label_svm = MultiOutputClassifier(svm_classifier)\n# SVM model training\nmulti_label_svm.fit(X_idf, y_train);\n# SVM predict output on validation set\nX_validation_idf = tf_idf_vectorizer.transform(X_validation)\npredictions_svm = multi_label_svm.predict(X_validation_idf)\nf1 = f1_score(y_validation, predictions_svm, average='weighted')\nroc_auc = roc_auc_score(y_validation, predictions_svm, average = 'weighted')\nprecision = precision_score(y_validation, predictions_svm, average = 'weighted')\naccuracy = accuracy_score(y_validation, predictions_svm)\nrecall = recall_score(y_validation, predictions_svm, average = 'weighted')\n\nmetrics_svm = {'f1': f1,\n               'roc_auc': roc_auc,\n               'accuracy': accuracy,\n               'precision': precision,\n               'recall': recall,\n               }\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1 Score: {f1}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nimport matplotlib.pyplot as plt\n\nmetrics = ['f1', 'roc_auc', 'accuracy', 'precision', 'recall']\nvalues_model1 = [metrics_bert[metric] for metric in metrics]\nvalues_model2 = [metrics_svm[metric] for metric in metrics]\n\nbar_width = 0.35\nindex = range(len(metrics))\n\nfig, ax = plt.subplots()\nbar1 = ax.bar(index, values_model1, bar_width, label='Bert')\nbar2 = ax.bar([i + bar_width for i in index], values_model2, bar_width, label='SVM (Alternative Approach)')\n\nax.set_xlabel('Metrics')\nax.set_ylabel('Values')\nax.set_title('Comparison of Performance Metrics')\nax.set_xticks([i + bar_width / 2 for i in index])\nax.set_xticklabels(metrics)\nax.legend()\n\nplt.tight_layout()\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Valdrin_Smakaj.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n\n# load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\n# access and print the last data point from validation set\nlast_data_point = dataset['validation'][-1]\nprint(last_data_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Valdrin_Smakaj.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nfrom datasets import load_dataset\nimport torch\n\ndef tokenize_data(dataset):\n    # Tokenize the text\n    encodings = tokenizer(dataset['Tweet'], truncation=True, padding='max_length', return_tensors=\"pt\")\n\n    # Prepare labels as floats\n    labels = []\n    for label in ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']:\n        labels.append([float(val) for val in dataset[label]])\n\n    # Convert the labels list to a tensor\n    labels_tensor = torch.tensor(labels).T.float()  # Transpose to match batch size\n\n    # Combine tokenized inputs with labels\n    encoded_dataset = {**encodings, 'labels': labels_tensor}\n\n    return encoded_dataset\n\n# Load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Apply the function to tokenize and format the dataset\nencoded_dataset = dataset.map(tokenize_data, batched=True)\n\n# Set the format to PyTorch tensors\nencoded_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n# Print the keys of the last data point in the validation set\nprint(encoded_dataset['validation'][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Valdrin_Smakaj.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "import torch\nfrom transformers import BertTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n\n# Function to tokenize the data\ndef tokenize_data(examples):\n    encodings = tokenizer(examples['Tweet'], truncation=True, padding='max_length', max_length=128)\n    labels = {label: examples[label] for label in label_list}\n    labels = np.array([labels[label] for label in label_list]).astype(float).T\n    return {**encodings, 'labels': labels}\n\n# Function to compute metrics for evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = (logits > 0).astype(int)\n    f1 = f1_score(labels, predictions, average='micro')\n    roc_auc = roc_auc_score(labels, logits, average='micro')\n    accuracy = accuracy_score(labels, predictions)\n    precision = precision_score(labels, predictions, average='micro')\n    recall = recall_score(labels, predictions, average='micro')\n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'roc_auc': roc_auc\n    }\n\n# Load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\n# Define the list of labels\nlabel_list = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the dataset\nencoded_dataset = dataset.map(tokenize_data, batched=True)\nencoded_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n# Load the pre-trained BERT model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=len(label_list),\n    problem_type=\"multi_label_classification\"\n)\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset['validation'],\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\nevaluation_results = trainer.evaluate()\n\n# Print evaluation results\nprint(evaluation_results)\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\n# Define the list of labels\nlabel_list = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n\n# Prepare the dataset\ndef extract_labels(examples):\n    return {'labels': [int(examples[label]) for label in label_list]}\n\n# Process the labels in the dataset\ndataset = dataset.map(extract_labels)\n\n# Convert dataset to pandas dataframe \ntrain_df = dataset['train'].to_pandas()\nvalid_df = dataset['validation'].to_pandas()\n\n# Define X and Y\nX_train = train_df['Tweet']\nY_train = np.array(train_df[label_list])\nX_valid = valid_df['Tweet']\nY_valid = np.array(valid_df[label_list])\n\n# Create a TF-IDF Vectorizer and Logistic Regression model within a Pipeline\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n    ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear'))),\n])\n\n# Train the model\npipeline.fit(X_train, Y_train)\n\n# Predict on the validation set\nY_pred = pipeline.predict(X_valid)\n\n# Evaluate the model\naccuracy = accuracy_score(Y_valid, Y_pred)\nf1 = f1_score(Y_valid, Y_pred, average='micro')\nprecision = precision_score(Y_valid, Y_pred, average='micro')\nrecall = recall_score(Y_valid, Y_pred, average='micro')\n\n# Print evaluation metrics\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1-score: {f1}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Valdrin_Smakaj.ipynb",
        "question": "#### Here comes your discussion",
        "answer": "# BERT Approach:\n\n# Accuracy: 28.67%\n# F1-score: 71.14%\n# Precision: 77.09%\n# Recall: 66.03%\n# TF-IDF + SVM Approach:\n\n# Accuracy: 13.99%\n# F1-score: 44.67%\n# Precision: 80.48%\n# Recall: 30.91%\n\n# Conclusion:\n# The BERT approach outperforms the TF-IDF + SVM approach across all metrics. The most notable difference is in the F1-score, which is a measure of a test's accuracy and considers both the precision and the recall. The BERT model's F1-score is substantially higher, indicating a better balance between precision and recall.\n\n# BERT's precision is slightly lower than the TF-IDF + SVM's, but its recall is significantly higher, suggesting that BERT is better at identifying relevant instances. This could be crucial in applications where missing out on relevant predictions is costly.\n\n# The accuracy of BERT is more than double that of TF-IDF + SVM, confirming the overall superiority of the BERT model in classifying the labels correctly."
    },
    {
        "file_name": "Intro2NLP_Assignment_4_RaoRohilPrakash.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "pip install datasets\n# Here comes your code\n\nfrom datasets import load_dataset\n\n# Specify the dataset and download it\ndataset = load_dataset(\"imdb\")\n\n# Print information about the dataset\nprint(dataset)\n# Access the validation split of the dataset\nvalidation_dataset = dataset[\"test\"]\n\n# Get the number of examples in the validation set\nnum_examples = len(validation_dataset)\n\n# Print the last data point in the validation set\nlast_data_point = validation_dataset[num_examples - 1]\nprint(last_data_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_RaoRohilPrakash.ipynb",
        "question": "### Subset",
        "answer": "from datasets import load_dataset\n\n# Specify the dataset and download it\n# dataset = load_dataset(\"imdb\")\n\n# Create a subset with 10% of the data\nsubset_size = int(0.1 * len(dataset[\"train\"]))\nsubset = {\n    \"train\": dataset[\"train\"].shuffle(seed=42).select(list(range(subset_size))),\n    \"validation\": dataset[\"test\"].shuffle(seed=42).select(list(range(subset_size))),\n    \"test\": dataset[\"unsupervised\"].shuffle(seed=42).select(list(range(subset_size))),\n}\n\n# Print information about the new subset\nprint(subset)\nsubset = DatasetDict(subset)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_RaoRohilPrakash.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nfrom datasets import load_dataset, DatasetDict\n\n# Load BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize the text and add labels as numbers\ndef tokenize_function(examples):\n    tokenized_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n    tokenized_inputs[\"labels\"] = examples[\"label\"]\n    return tokenized_inputs\n\ndef tokenize_data(dataset):\n    # Tokenize each split of the dataset\n    tokenized_dataset = {split: dataset[split].map(tokenize_function, batched=True) for split in dataset.keys()}\n\n    # Set the format of the data to PyTorch tensors\n    for split in tokenized_dataset.keys():\n        tokenized_dataset[split].set_format(\"torch\")\n\n    return tokenized_dataset\n\n# Convert the subset into a DatasetDict\nsubset_dataset_dict = subset\n\n# Apply the tokenize_data function to the subset dataset\nencoded_subset_dataset = tokenize_data(subset_dataset_dict)\n\n# Print the keys of the last data point in each split\nfor split, data in encoded_subset_dataset.items():\n    last_data_point_keys = list(data.features.keys())\n    print(f\"Keys of the last data point in the {split} set:\", last_data_point_keys)\nencoded_subset_dataset"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_RaoRohilPrakash.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import BertForSequenceClassification, BertTokenizer, TrainingArguments, Trainer\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nimport torch\n\n\n# Load BERT model and tokenizer\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,\n    problem_type=\"single_label_classification\",  # Use \"single_label_classification\" for binary classification\n)\n\n\n# Define the compute_metrics function\ndef compute_metrics(p):\n    predictions, labels = p.predictions, p.label_ids\n    predictions = (predictions > 0.5).float()\n\n    # Calculate metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n    acc = accuracy_score(labels, predictions)\n\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_model\",\n    per_device_train_batch_size=4,\n    num_train_epochs=3,\n    save_total_limit=1,\n    save_steps=1000,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\n# Train the BERT-based model using Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=encoded_subset_dataset[\"train\"],\n    eval_dataset=encoded_subset_dataset[\"validation\"],\n)\n\ntrainer.train()\npip install accelerate -U\n# Here comes your code for the alternative approach\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Assuming `subset` is your original dataset\ntrain_texts = subset[\"train\"][\"text\"]\ntrain_labels = subset[\"train\"][\"label\"]\n\nval_texts = subset[\"validation\"][\"text\"]\nval_labels = subset[\"validation\"][\"label\"]\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\nX_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\nX_val_tfidf = tfidf_vectorizer.transform(val_texts)\n\n# Train an SVM classifier\nsvm_classifier = SVC(kernel='linear', probability=True)\nsvm_classifier.fit(X_train_tfidf, train_labels)\n\n# Make predictions on the validation set\nval_predictions = svm_classifier.predict(X_val_tfidf)\n\n# Evaluate the SVM classifier\naccuracy = accuracy_score(val_labels, val_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(val_labels, val_predictions, average=\"binary\")\n\nprint(\"SVM Classifier Metrics:\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1 Score: {f1}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_RaoRohilPrakash.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# !pip install datasets \nfrom datasets import load_dataset\n\n# Load SemEval 2018 Task 1 dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\",\"subtask5.english\")\n\n# Access the validation, train, test  split\nvalidation_set = dataset[\"validation\"]\ntrain_set = dataset[\"train\"]\ntest_set = dataset[\"test\"]\n\n# Print the last data point in the validation set\nlast_data_point = validation_set[-1]\nprint(last_data_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport numpy as np\n\n# Load the BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Get the list of labels\nlabels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n\n# Define a function to tokenize the dataset\ndef tokenize_data(dataset):\n  # tokenize the text\n  text = dataset[\"Tweet\"]\n\n  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n\n  # tokenize the labels\n  labels_batch = {k: dataset[k] for k in dataset.keys() if k in labels}\n  \n  # create numpy array of shape (batch_size, num_labels)\n  labels_matrix = np.zeros((len(text), len(labels)))\n  \n  # fill numpy array\n  for idx, label in enumerate(labels):\n    labels_matrix[:, idx] = labels_batch[label]\n\n  encoding[\"labels\"] = labels_matrix.tolist()\n  \n  return encoding\n     \n\nencoded_dataset = dataset.map(tokenize_data, batched=True, remove_columns=dataset['train'].column_names)\nencoded_dataset['validation'][-1].keys()\n#`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\n\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score\nimport torch\n\n# Create a dictionary to map the label names to label ids\nid2label = {idx:label for idx, label in enumerate(labels)}\nlabel2id = {label:idx for idx, label in enumerate(labels)}\n\n# Define the model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n                                                           problem_type=\"multi_label_classification\", \n                                                           num_labels=len(labels),\n                                                           id2label=id2label,\n                                                           label2id=label2id)\n\n\n# Define the training arguments\nargs = TrainingArguments(\n    \"bert-finetuned\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    report_to=None\n)\n\n\n    \n# Define a function to compute the metrics\ndef multi_label_metrics(predictions, labels, threshold=0.5):\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = torch.nn.Sigmoid()\n    probs = sigmoid(torch.Tensor(predictions))\n    # next, use threshold to turn them into integer predictions\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs >= threshold)] = 1\n    # finally, compute metrics\n    y_true = labels\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    recall_micro_average = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n\n    accuracy = accuracy_score(y_true, y_pred)\n    # return as dictionary\n    metrics = {'f1': f1_micro_average,\n            'recall': recall_micro_average,\n               'accuracy': accuracy}\n    return metrics\n\n# Define a function to compute the metrics\ndef compute_metrics(p):\n    preds = p.predictions[0] if isinstance(p.predictions, \n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds, \n        labels=p.label_ids)\n    return result\n\n# Define the trainer\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n# Here comes your code for the alternative approach"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muskaan_Chopra.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install -q transformers datasets\npip install accelerate -U\n# Import necessary libraries\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\ndataset\n\n# downsampled_dataset = dataset['train'].shuffle(seed=42).select(range(1000))  # Adjust the range as needed"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muskaan_Chopra.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "print(dataset['train'].features)\nlabels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\nid2label = {idx:label for idx, label in enumerate(labels)}\nlabel2id = {label:idx for idx, label in enumerate(labels)}\nlabels\nfrom transformers import AutoTokenizer\nimport numpy as np\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_data(examples):\n  # take a batch of texts\n  text = examples[\"Tweet\"]\n  # encode them\n  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n  # add labels\n  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n  # create numpy array of shape (batch_size, num_labels)\n  labels_matrix = np.zeros((len(text), len(labels)))\n  # fill numpy array\n  for idx, label in enumerate(labels):\n    labels_matrix[:, idx] = labels_batch[label]\n\n  encoding[\"labels\"] = labels_matrix.tolist()\n\n  return encoding\nencoded_dataset = dataset.map(tokenize_data, batched=True, remove_columns=dataset['train'].column_names)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muskaan_Chopra.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoModelForSequenceClassification\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nfrom transformers import EvalPrediction\nimport torch\nfrom transformers import TrainingArguments, Trainer\n\nbatch_size = 8\nmetric_name = \"f1\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                           problem_type=\"multi_label_classification\",\n                                                           num_labels=len(labels),\n                                                           id2label=id2label,\n                                                           label2id=label2id)\nargs = TrainingArguments(\n    f\"bert-finetuned-sem_eval-english\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=metric_name,\n    #push_to_hub=True,\n)\n\n\n# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\ndef multi_label_metrics(predictions, labels, threshold=0.5):\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = torch.nn.Sigmoid()\n    probs = sigmoid(torch.Tensor(predictions))\n    # next, use threshold to turn them into integer predictions\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs >= threshold)] = 1\n    # finally, compute metrics\n    y_true = labels\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n    accuracy = accuracy_score(y_true, y_pred)\n    # return as dictionary\n    metrics = {'f1': f1_micro_average,\n               'roc_auc': roc_auc,\n               'accuracy': accuracy}\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions,\n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds,\n        labels=p.label_ids)\n    return result\n\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\ntrainer.train()\ntrainer.evaluate()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\ndf = pd.DataFrame(dataset['train'])\n\n# Extract tweets and labels from your DataFrame\ntweets = df['Tweet']\nlabels = df.drop(['ID', 'Tweet'], axis=1).idxmax(axis=1)  # Get the column name of the max value in each row\n\n# Encode labels\nle = LabelEncoder()\nencoded_labels = le.fit_transform(labels)\n\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(tweets, encoded_labels, test_size=0.2)\n\n# Tokenize using TF-IDF\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_val_tfidf = vectorizer.transform(X_val)\n\n# Train logistic regression model\nclassifier = LogisticRegression()\nclassifier.fit(X_train_tfidf, y_train)\n\n# Predict on validation set\ny_pred = classifier.predict(X_val_tfidf)\n\n# Evaluate performance\naccuracy = accuracy_score(y_val,y_pred)\nf1score=f1_score(y_val,y_pred, average='weighted')  # Use weighted average for multiclass problem\nprecision=precision_score(y_val,y_pred, average='weighted')\nrecall=recall_score(y_val,y_pred, average='weighted')\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1-score: {f1score}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muskaan_Chopra.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muskaan_Chopra.ipynb",
        "question": "c) **Discussion:**\n### BERT Approach:\n**Strengths:**\n1. **Contextual Understanding:** BERT, being a pre-trained transformer model, captures contextual information and dependencies between words in a sentence. This is particularly beneficial for tasks like text classification where the meaning of a word often depends on its context.\n2. **Transfer Learning:** BERT is pre-trained on a large corpus of text, enabling it to learn general language representations. Fine-tuning on a specific task requires less labeled data compared to training a model from scratch, making it effective for tasks with limited labeled data.\n3. **Multi-label Classification:** BERT can handle multi-label classification tasks effectively, as it can predict multiple labels for a given text.\n**Weaknesses:**\n1. **Computational Intensity:** BERT models are computationally expensive, both in terms of training and inference. Fine-tuning on large datasets can be time-consuming and resource-intensive.\n2. **Large Model Size:** BERT models are large in size, which can be a limitation for deployment in resource-constrained environments, such as mobile devices.\n### Alternative Approach (TF-IDF + SVM or BOW + LR):\n**Strengths:**\n1. **Interpretability:** TF-IDF and Bag of Words representations are more interpretable compared to the complex embeddings used in BERT. This makes it easier to understand the features contributing to the model's decision.\n2. **Computational Efficiency:** Traditional methods like TF-IDF and Bag of Words are computationally less intensive compared to deep learning models like BERT. They are faster to train and require less computational resources.\n3. **Smaller Model Size:** Models based on TF-IDF or Bag of Words are typically smaller in size compared to BERT, making them more suitable for deployment in resource-constrained environments.\n**Weaknesses:**\n1. **Lack of Contextual Information:** TF-IDF and Bag of Words representations do not capture the contextual information and semantic relationships between words. This limitation can impact the model's performance, especially in tasks where context is crucial.\n2. **Limited Generalization:** Traditional methods may not generalize well to complex tasks or datasets with diverse and nuanced language. They may struggle with tasks that require a deeper understanding of context.\n3. **Handling Out-of-Vocabulary Words:** TF-IDF and Bag of Words methods may struggle with out-of-vocabulary words, as they rely on a fixed vocabulary. BERT, on the other hand, can handle unseen words through subword tokenization.\n### Overall Comparison:\n- **Data Availability:** If labeled data is abundant, BERT may outperform traditional methods due to its ability to leverage large pre-trained language models. However, if labeled data is limited, traditional methods may be more practical.\n- **Resource Constraints:** In scenarios with limited computational resources or deployment on edge devices, the alternative approach may be preferred due to its smaller model size and lower computational requirements.\n- **Interpretability vs. Performance Trade-off:** The choice between approaches may also depend on the importance of model interpretability. If understanding the model's decision-making process is crucial, traditional methods might be preferred.\n- **Task Complexity:** For simple tasks where contextual information is less critical, traditional methods may perform adequately. For complex tasks that demand a deep understanding of context, BERT may provide better results.\nIn conclusion, the choice between BERT and traditional methods depends on factors such as the task at hand, the availability of labeled data, computational resources, and the desired level of interpretability. It's often beneficial to experiment with both approaches and choose the one that best suits the specific requirements of the text classification task.",
        "answer": ""
    },
    {
        "file_name": "Assignment_4_AmirhosseinBarari.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n\ndataset = load_dataset(\"rotten_tomatoes\")\nlast_data_point = dataset[\"validation\"][-1]\n\nprint(f\"Last dp in the validation set:\\n{last_data_point}\")"
    },
    {
        "file_name": "Assignment_4_AmirhosseinBarari.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "def tokenize_data(dataset):\n    # here comes your code\n    return(encoded_dataset)"
    },
    {
        "file_name": "Assignment_4_AmirhosseinBarari.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\n# Here comes your code for the alternative approach"
    },
    {
        "file_name": "Assignment_4_AmirhosseinBarari.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Elwakeel_Wedermann.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# Here comes your code\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"webis/Touche23-ValueEval\")\nvalidation_set = dataset['validation']\nprint(dataset)\nprint(validation_set[-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Elwakeel_Wedermann.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\nimport torch\n\ndef tokenize_data(dataset):\n    # Load BERT tokenizer\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n    # Tokenize the text and convert labels to tensors\n    def tokenize_function(ex):\n        # correct keys for the given dataset\n        argument_id_key = 'Argument ID'\n        conclusion_key = 'Conclusion'\n        stance_key = 'Stance'\n        premise_key = 'Premise'\n        labels_key = 'Labels'\n\n        # Concatenate the relevant features\n        text = ex[argument_id_key] + \" \" + ex[conclusion_key] + \" \" + ex[stance_key] + \" \" + ex[premise_key]\n\n        # Tokenize the text\n        inputs = tokenizer(text)\n\n        # Convert labels to tensors\n        labels = ex[labels_key]\n        \n        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n\n        return {\n            'input_ids': inputs['input_ids'],\n            'token_type_ids': inputs['token_type_ids'],\n            'attention_mask': inputs['attention_mask'],\n            'labels': labels_tensor\n        }\n\n    # Tokenize the dataset using map()\n    encoded_dataset = dataset.map(tokenize_function)\n\n    # Set the format to PyTorch tensors\n    encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n    return encoded_dataset\n\nencoded_dataset = tokenize_data(dataset)\n\nlast_data_point = encoded_dataset['validation'][-1]\nkeys = last_data_point.keys()\nprint(\"dict_keys(\" + str(keys) + \")\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Elwakeel_Wedermann.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\n# Here comes your code for the alternative approach"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Elwakeel_Wedermann.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Abhishek_Murtuza.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install -q transformers datasets\n!pip install transformers[torch]\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport sys\ntorch.cuda.is_available()\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\",'subtask5.english',)\n\n# Access the last data point\nlast_data_point = dataset['validation'][-1]\n\n# Print the last data point\nprint(last_data_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Abhishek_Murtuza.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer, BertModel\nMAX_LEN = 256\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 8\nEPOCHS = 1\nLEARNING_RATE = 1e-05\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nlabels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\ndef tokenize_data(dataset):\n    tokens = tokenizer(dataset['Tweet'],add_special_tokens =True, truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_attention_mask = True, return_tensors=\"pt\")\n    # add labels\n    labels_batch = {k: dataset[k] for k in dataset.keys() if k in labels}\n    # create numpy array of shape (batch_size, num_labels)\n    labels_matrix = np.zeros((len(dataset['Tweet']), len(labels)))\n    # fill numpy array\n    for idx, label in enumerate(labels):\n        labels_matrix[:, idx] = labels_batch[label]\n\n    encoded_dataset =  {\"input_ids\": tokens[\"input_ids\"],\"token_type_ids\": tokens[\"token_type_ids\"], \"attention_mask\": tokens[\"attention_mask\"], \"labels\": labels_matrix.tolist()}\n\n    return(encoded_dataset)\n# Apply the function to the dataset using map\ntokenized_dataset = dataset.map(tokenize_data, batched=True, remove_columns=dataset['train'].column_names)\n# Set the format of the data to PyTorch tensors\ntokenized_dataset.set_format(\"torch\")\n\n# Print the last tokenized data point\nprint(tokenized_dataset['validation'][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Abhishek_Murtuza.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\nid2label = {idx:label for idx, label in enumerate(labels)}\nlabel2id = {label:idx for idx, label in enumerate(labels)}\n\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n                                                           problem_type=\"multi_label_classification\", \n                                                           num_labels=len(labels),\n                                                           id2label=id2label,\n                                                           label2id=label2id)\n!pip install transformers[torch]\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    f\"bert-finetuned-sem_eval-english\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=LEARNING_RATE,\n    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=VALID_BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n)\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nfrom transformers import EvalPrediction\nimport torch\n    \ndef multi_label_metrics(predictions, labels, threshold=0.5):\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = torch.nn.Sigmoid()\n    probs = sigmoid(torch.Tensor(predictions))\n    # next, use threshold to turn them into integer predictions\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs >= threshold)] = 1\n    # finally, compute metrics\n    y_true = labels\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n    accuracy = accuracy_score(y_true, y_pred)\n    # return as dictionary\n    metrics = {'f1': f1_micro_average,\n               'roc_auc': roc_auc,\n               'accuracy': accuracy}\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, \n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds, \n        labels=p.label_ids)\n    return result\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\ntrainer.train()\ntrainer.evaluate()\ntext = \"I'm very very sad the accuracy of the model is only 25%, but also happy that I was able to train the model\"\n\nencoding = tokenizer(text, return_tensors=\"pt\")\nencoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n\noutputs = trainer.model(**encoding)\nlogits = outputs.logits\nlogits.shape\n# apply sigmoid + threshold\nsigmoid = torch.nn.Sigmoid()\nprobs = sigmoid(logits.squeeze().cpu())\npredictions = np.zeros(probs.shape)\npredictions[np.where(probs >= 0.5)] = 1\n# turn predicted id's into actual label names\npredicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\nprint(predicted_labels)\n# b) Alternative Approach:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndataset = load_dataset(\"sem_eval_2018_task_1\",'subtask5.english')\n\ndf_train = dataset['train'].to_pandas()\ndf_test = dataset['validation'].to_pandas()\n\n# Extract features (text) and labels from the dataset\nX_train = df_train['Tweet']\ny_train = df_train.drop(labels = ['ID','Tweet'], axis=1).astype(int)\n\nX_test = df_test['Tweet']\ny_test = df_test.drop(labels = ['ID','Tweet'], axis=1).astype(int)\n\n\n# Tokenization using Bag of Words (BoW)\nvectorizer = CountVectorizer()\nX_train_bow = vectorizer.fit_transform(X_train)\nX_test_bow = vectorizer.transform(X_test)\n\n# using classifier chains\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.linear_model import LogisticRegression\n# initialize classifier chains multi-label classifier\nclassifier = ClassifierChain(LogisticRegression())\n# Training logistic regression model on train data\nclassifier.fit(X_train_bow, y_train)\n# predict\npredictions = classifier.predict(X_test_bow)\n\n# Evaluate the performance using metrics\naccuracy = accuracy_score(y_test, predictions)\nprecision = precision_score(y_test, predictions, average='weighted')\nrecall = recall_score(y_test, predictions, average='weighted')\nf1 = f1_score(y_test, predictions, average='weighted')\n\n# Print the metrics\nprint(\"LogisticRegression\")\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1-score: {f1:.2f}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Abhishek_Murtuza.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mauel_Maximilian.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install datasets evaluate\n!pip install transformers[torch]\n!pip install accelerate -U\nimport transformers\nfrom transformers import pipeline\nfrom datasets import load_dataset, DatasetDict\n\nprint(transformers.__version__)\n\ndataset = load_dataset('xed_en_fi', 'en_annotated')\ndataset\ndataset['train'][\"labels\"][2]\ntrain_testvalid_dataset = dataset['train'].train_test_split(train_size=0.800, test_size=0.200, seed=11)\ntest_valid_dataset = train_testvalid_dataset['test'].train_test_split(test_size=0.5)\n\ntrain_test_valid_dataset = DatasetDict({\n    'train': train_testvalid_dataset['train'],\n    'test': test_valid_dataset['test'],\n    'valid': test_valid_dataset['train']})\ntrain_test_valid_dataset[\"valid\"][-1]\nlabels_column = dataset['train']['labels']\nall_labels = [label for labels_list in labels_column for label in labels_list]\nemotion_labels = set(all_labels)\nassert len(emotion_labels) == 8\nemotion_labels\nemotion_labels = {\"anger\":1, \"anticipation\":2, \"disgust\":3, \"fear\":4, \"joy\":5, \"sadness\":6, \"surprise\":7, \"trust\":8 }"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mauel_Maximilian.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, BertTokenizer\nimport torch\nimport numpy as np\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_data(dataset):\n    text = dataset['sentence']\n    labels = dataset['labels']\n\n    tokenized_input = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n\n    enc_label = torch.zeros(len(labels), 8, dtype=torch.float32)\n    for i, elem in enumerate(labels):\n      elem = [x - 1 for x in elem]\n      enc_label[i, elem] = 1.0\n\n    return {\n        'input_ids': tokenized_input['input_ids'].squeeze(0),\n        'token_type_ids': tokenized_input['token_type_ids'].squeeze(0),\n        'attention_mask': tokenized_input['attention_mask'].squeeze(0),\n        'labels_one_hot': enc_label\n    }\n\ndef encode(dataset):\n  encoded_dataset = dataset.map(tokenize_data, batched=True, remove_columns=[\"labels\", \"sentence\"])\n  encoded_dataset = encoded_dataset.rename_column(\"labels_one_hot\", \"labels\")\n  encoded_dataset.set_format(\"torch\")\n\n  return encoded_dataset\nenc_train = encode(train_test_valid_dataset[\"train\"])\nenc_test = encode(train_test_valid_dataset[\"test\"])\nenc_valid = encode(train_test_valid_dataset[\"valid\"])\nenc_valid[-1].keys()"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mauel_Maximilian.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "import numpy as np\nfrom transformers import TrainingArguments, Trainer\nimport evaluate\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nlabel2id = {label: i for i, label in enumerate(emotion_labels)}\nid2label = {i: label for label, i in label2id.items()}\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    print(eval_pred)\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\n# 2.\n\nclf_metrics = evaluate.combine([\"f1\", \"precision\", \"recall\"]) #\"accuracy\",\n#https://github.com/huggingface/evaluate/issues/423\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    #predictions = np.argmax(logits, axis=1)\n    #ref = np.argmax(labels, axis=1)\n    predictions = (logits > 0.5).astype(np.int32)\n    ref = (labels > 0.5).astype(np.int32)\n\n    pred = predictions\n\n    print(ref.shape)\n    print(pred.shape)\n    print(labels.shape)\n\n    accuracy = accuracy_score(pred, ref)\n    f1 = f1_score(pred, ref, average='micro')\n    precision = precision_score(pred, ref, average='micro')\n    recall = recall_score(pred, ref, average='micro')\n\n    #met = clf_metrics.compute(predictions=pred, references=ref, average='micro')\n    #acc = accuracy.compute(predictions=pred, references=ref)\n    #met[\"accuracy\"] = acc[\"accuracy\"]\n\n    met = {\n        \"accuracy\": accuracy,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall\n    }\n\n    return met\nmodel_name = \"bert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(emotion_labels),\n    id2label=id2label,\n    label2id=label2id,\n    problem_type=\"multi_label_classification\"\n    #problem_type=\"single_label_classification\"\n)\n#tokenizer = AutoTokenizer.from_pretrained(model_name)\ntraining_args = TrainingArguments(\n    output_dir=\"my_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=enc_train,\n    eval_dataset=enc_test,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n# seems to start to overfit after one epoch already -> according to traning / validation loss at least\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.multioutput import MultiOutputClassifier\n\ndataset = load_dataset('xed_en_fi', 'en_annotated')\n\n\"\"\"\ntrain_testvalid_dataset = dataset['train'].train_test_split(train_size=0.800, test_size=0.200, seed=11)\ntest_valid_dataset = train_testvalid_dataset['test'].train_test_split(test_size=0.5)\n\ntrain_test_valid_dataset = DatasetDict({\n    'train': train_testvalid_dataset['train'],\n    'test': test_valid_dataset['test'],\n    'valid': test_valid_dataset['train']})\n\n\"\"\"\ndataset\nsentences = dataset['train']['sentence']\nlabels = dataset['train']['labels']\n\ntfidf_vectorizer = TfidfVectorizer()\nX = tfidf_vectorizer.fit_transform(sentences)\n\nmlb = MultiLabelBinarizer()\ny = mlb.fit_transform(labels)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsvm_classifier = SVC(kernel='rbf')\n\nmulti_output_classifier = MultiOutputClassifier(svm_classifier)\nmulti_output_classifier.fit(X_train, y_train)\npredictions = multi_output_classifier.predict(X_test)\n\naccuracy = accuracy_score(y_test, predictions)\nf1 = f1_score(y_test, predictions, average='micro')\nprecision = precision_score(y_test, predictions, average='micro')\nrecall = recall_score(y_test, predictions, average='micro')\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1-score: {f1}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mauel_Maximilian.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Assignment_4_Naman_Jain.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\nfrom datasets import DatasetDict\n\n# load dataset\ndataset = load_dataset('joelniklaus/online_terms_of_service')\n\n# print last data point in the validation set\nprint(dataset['validation'][-1])"
    },
    {
        "file_name": "Assignment_4_Naman_Jain.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nimport torch\n\nfrom transformers import BertTokenizer\nimport torch\n\ndef tokenize_data(dataset):\n\n    # BERT tokenizer\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def encode(examples):\n        # tokenize the texts\n        tokenized_inputs = tokenizer(examples['sentence'], padding='max_length', truncation=True)\n\n        # prepare labels as a tensor of floats\n        label_fields = ['a', 'ch', 'cr', 'j', 'law', 'ltd', 'ter', 'use', 'pinc']\n        labels = [[float(examples[field][i]) for field in label_fields] for i in range(len(examples['sentence']))]\n\n        tokenized_inputs['labels'] = torch.tensor(labels, dtype=torch.float)\n        return tokenized_inputs\n\n    # apply the encoding function to the entire dataset\n    return dataset.map(encode, batched=True)\n\nencoded_dataset = tokenize_data(dataset)\n\n# set the format\nencoded_dataset.set_format(\"torch\")\n\n# last data point in the validation set\nlast_data_point = encoded_dataset['validation'][-1]\n\n# print\nprint(last_data_point.keys())"
    },
    {
        "file_name": "Assignment_4_Naman_Jain.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoModelForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# load the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# define label mappings\nlabel_fields = ['a', 'ch', 'cr', 'j', 'law', 'ltd', 'ter', 'use', 'pinc']\nlabel2id = {label: i for i, label in enumerate(label_fields)}\nid2label = {i: label for i, label in enumerate(label_fields)}\n\n# load the BERT model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=len(label2id),\n    problem_type=\"multi_label_classification\",\n    id2label=id2label,\n    label2id=label2id\n)\n\n# define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\n# compute_metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    threshold = 0.5\n    predictions = np.where(logits > threshold, 1, 0)\n\n    accuracy = accuracy_score(labels.flatten(), predictions.flatten())\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='micro')\n\n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset['validation'],\n    compute_metrics=compute_metrics\n)\n\n# train\ntrainer.train()\n\n# evaluate the model on the validation set\nevaluation_results = trainer.evaluate(encoded_dataset['validation'])\n\n# print\nprint(f\"BERT Model Metrics:\\n\")\nfor key, value in evaluation_results.items():\n    print(f\"{key}: {value}\")\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# TF-IDF vectorizer and SVM classifier pipeline\npipeline = make_pipeline(\n    TfidfVectorizer(),\n    OneVsRestClassifier(SVC(probability=True))\n)\n\n# prepare data\nX_train = [example['sentence'] for example in encoded_dataset['train']]\ny_train = [[example[label] for label in label_fields] for example in encoded_dataset['train']]\nX_val = [example['sentence'] for example in encoded_dataset['validation']]\ny_val = [[example[label] for label in label_fields] for example in encoded_dataset['validation']]\n\n# train\npipeline.fit(X_train, y_train)\n\n# predict on validation set\ny_pred = pipeline.predict(X_val)\n\n# compute metrics\naccuracy = accuracy_score(y_val, y_pred)\nprecision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='micro')\n\n# print metrics\nprint(f\"TF-IDF + SVM:\\nAccuracy: {accuracy}\\nF1-Score: {f1}\\nPrecision: {precision}\\nRecall: {recall}\")"
    },
    {
        "file_name": "Assignment_4_Naman_Jain.ipynb",
        "question": "#### Here comes your discussion",
        "answer": "In comparing the BERT model with the TF-IDF + SVM method for classifying texts with multiple labels, each has its own pros and cons.\n\nBERT is great at understanding complex language but struggles with class imbalance, leading to more misses of true positives (low recall), despite being accurate most times it predicts a label (high precision).\n\nTF-IDF + SVM, while easier to use and less demanding on computer resources, lacks the contextual depth of BERT. It faces similar challenges in dealing with class imbalance, as seen from its high precision but low recall.\n\nChoosing between these models depends on what's more important: deep language understanding or computational simplicity."
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### **Note following cells download some packages or updates them to be able the run the code properly for Google Colab** (can ignore if not running it on google colab)",
        "answer": "# install required libraries\n!pip install tensorflow==2.14.0\n!pip install datasets\n!pip install -U transformers\n!pip install accelerate -U"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# Here comes your code\nfrom datasets import load_dataset\n\n# a) Select and load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\n# b) Print the last data point in the validation set\nvalidation_last_point = dataset[\"validation\"][-1]\nvalidation_last_point\n# get the labels\nlabels = list(dataset['train'].features.keys())[2:]\nlabels"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport numpy as np\n\ntokenizer_model = 'bert-base-uncased'\nbert_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n\ndef tokenize_data(dataset):\n\n    # get input tweets\n    tweet = dataset[\"Tweet\"]\n\n    # encode tweets\n    encoded_dataset = bert_tokenizer(tweet, padding=\"max_length\", truncation=True)\n\n    # get labels\n    label_values = {key: dataset[key] for key in labels if key in dataset}\n\n    # initialize numpy array\n    l_matrix = np.zeros((len(tweet), len(labels)), dtype=np.float32)\n\n    for index, label in enumerate(labels):\n        l_matrix[:, index] = label_values[label]\n\n    encoded_dataset[\"labels\"] = l_matrix.tolist()\n\n    return encoded_dataset\nencoded_dataset = dataset.map(tokenize_data, batched=True, remove_columns=dataset['train'].column_names)\nencoded_dataset.set_format(\"torch\")\n# print the keys() of the the last data point in the validation set in the encoded dataset\nlast_validation_example_keys = encoded_dataset[\"validation\"][-1].keys()\nlast_validation_example_keys"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Sadia_Naseer_Ali_Ather.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# creating dictionaries for number value to integers and vice versa\n\nid2label = {idx:label for idx, label in enumerate(labels)}\nlabel2id = {label:idx for idx, label in enumerate(labels)}\nid2label, label2id\nfrom transformers import AutoModelForSequenceClassification\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                           problem_type=\"multi_label_classification\",\n                                                           num_labels=len(labels),\n                                                           id2label=id2label,\n                                                           label2id=label2id)\nfrom transformers import TrainingArguments, Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"my_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n    metric_for_best_model=\"f1\"\n)\nfrom transformers import EvalPrediction\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n\ndef multi_label_metrics(predictions, labels, threshold=0.5):\n\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = torch.nn.Sigmoid()\n    probabilities = sigmoid(torch.Tensor(predictions))\n\n\n    # applying thresholding\n    y_pred = np.zeros(probabilities.shape)\n    y_pred[np.where(probabilities >= threshold)] = 1\n\n    # Compute metrics\n    y_true = labels\n    f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average = 'micro')\n    recall = recall_score(y_true, y_pred, average = 'micro')\n\n\n    # return metrics dictionary\n    metrics = {'f1': f1,\n               'roc_auc': roc_auc,\n               'accuracy': accuracy,\n               'precision': precision,\n               'recall': recall,\n               }\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions,\n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds,\n        labels=p.label_ids)\n    return result\n# Train the BERT-based model using the Trainer API\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n    tokenizer=bert_tokenizer,\n)\ntrainer.train()\n# Evaluate the model on the validation\nresult = trainer.evaluate()\nmetrics_bert = {\n     'f1': result['eval_f1'],\n     'roc_auc': result['eval_roc_auc'],\n     'accuracy': result['eval_accuracy'],\n     'precision': result['eval_precision'],\n     'recall': result['eval_recall'],\n      }\nresult\nimport numpy as np\nimport pandas as pd\nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n# Get the train, test, and validation dataframe\ntrain_df = pd.DataFrame(dataset['train'])\nvalidation_df = pd.DataFrame(dataset['validation'])\ntest_df =  pd.DataFrame(dataset['test'])\n\n# drop the ID column\ntrain_df = train_df.drop('ID', axis=1)\ntest_df = test_df.drop('ID', axis=1)\nvalidation_df = validation_df.drop('ID', axis=1)\n# split input and labels\nX_train = train_df['Tweet']\nX_test = test_df['Tweet']\nX_validation = validation_df['Tweet']\n\ny_train = train_df.drop('Tweet', axis=1)\ny_test = test_df.drop('Tweet', axis=1)\ny_validation = validation_df.drop('Tweet', axis=1)\nX_train.iloc[0], y_train.iloc[0], X_validation.iloc[0], y_validation.iloc[0], X_test.iloc[0], y_test.iloc[0]\n# Apply TF-IDF\ntf_idf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\nX_idf = tf_idf_vectorizer.fit_transform(X_train)\n\n# create df containing X_idf corresponding each token\ntfidf_tokens = tf_idf_vectorizer.get_feature_names_out()\ndf_tfidfvect = pd.DataFrame(data = X_idf.toarray(),columns = tfidf_tokens)\ndf_tfidfvect\n# Define SVM Model\nsvm_classifier = svm.SVC(kernel='linear')\nmulti_label_svm = MultiOutputClassifier(svm_classifier)\n# SVM model training\nmulti_label_svm.fit(X_idf, y_train);\n# SVM predict output on validation set\nX_validation_idf = tf_idf_vectorizer.transform(X_validation)\npredictions_svm = multi_label_svm.predict(X_validation_idf)\nf1 = f1_score(y_validation, predictions_svm, average='weighted')\nroc_auc = roc_auc_score(y_validation, predictions_svm, average = 'weighted')\nprecision = precision_score(y_validation, predictions_svm, average = 'weighted')\naccuracy = accuracy_score(y_validation, predictions_svm)\nrecall = recall_score(y_validation, predictions_svm, average = 'weighted')\n\nmetrics_svm = {'f1': f1,\n               'roc_auc': roc_auc,\n               'accuracy': accuracy,\n               'precision': precision,\n               'recall': recall,\n               }\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1 Score: {f1}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Sinem_Dnmez.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "pip install datasets\n# Here comes your code\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\nsizes = {split: len(dataset[split]) for split in dataset.keys()}\nprint(\"Sizes of each dataset split:\", sizes)\n\nif len(dataset['train']) > 1000:\n    dataset['train'] = dataset['train'].shuffle(seed=42).select(range(1000))\n\n# Downsampling the validation dataset if it has more than 500 samples\nif len(dataset['validation']) > 500:\n    dataset['validation'] = dataset['validation'].shuffle(seed=42).select(range(500))\n\n# Downsampling the test dataset if it has more than 500 samples\nif len(dataset['test']) > 500:\n    dataset['test'] = dataset['test'].shuffle(seed=42).select(range(500))\n\n# Display the last data point of the downsampled validation set\nlast_validation_data_point = dataset['validation'][-1]\nprint(\"Last data point in downsampled validation set:\", last_validation_data_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Sinem_Dnmez.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nfrom torch import tensor\nimport numpy as np\nimport torch\ndef tokenize_data(dataset):\n\n    # here comes your code\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def encode_example(examples):\n        # Tokenize the text\n        encoded = tokenizer(examples['Tweet'], truncation=True, padding='max_length', return_tensors='pt')\n\n        # Prepare the labels for each example in the batch\n        labels = []\n        label_keys = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n        for key in label_keys:\n            labels.append(examples[key])\n        labels_tensor = torch.tensor(labels, dtype=torch.float).T  # Transpose to get the correct shape\n\n        encoded['labels'] = labels_tensor\n\n        return encoded\n\n    # Apply the function to the entire dataset\n    encoded_dataset = dataset.map(encode_example, batched=True)\n\n    # Set the format to PyTorch tensors\n    encoded_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n    return encoded_dataset\n\n\nencoded_dataset = tokenize_data(dataset)\n\nencoded_dataset = tokenize_data(dataset)\n\nprint(encoded_dataset['validation'][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Sinem_Dnmez.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "!pip install accelerate -U\n!pip install transformers[torch] -U\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import f1_score, hamming_loss\n# Here comes your code for BERT Approach\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\n# List of label fields in your dataset\nlabel_fields = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n\n# Create label2id and id2label mappings\nlabel2id = {label: idx for idx, label in enumerate(label_fields)}\nid2label = {idx: label for label, idx in label2id.items()}\n\n# Set num_labels\nnum_labels = len(label_fields)\n\nprint(\"Number of labels:\", num_labels)\nprint(\"Label to ID:\", label2id)\nprint(\"ID to label:\", id2label)\n\n\n# Load pre-trained BERT model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=num_labels,\n    problem_type=\"multi_label_classification\",\n    id2label=id2label,\n    label2id=label2id\n)\n\ndef compute_metrics(pred):\n    # Note: For multi-label classification, 'pred.predictions' will be a 2D array with shape [batch_size, num_labels]\n    # Each element is the output of a sigmoid on the logits, indicating the probability of the corresponding label being true.\n    predictions = pred.predictions >= 0.5  # Apply threshold to get binary label predictions\n    labels = pred.label_ids\n\n    f1 = f1_score(labels, predictions, average='micro')  # 'micro' to aggregate the contributions of all classes.\n    hamming = hamming_loss(labels, predictions)  # The lower the Hamming loss, the better the performance.\n\n    return {\n        'f1_micro': f1,\n        'hamming_loss': hamming\n    }\n\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_strategy=\"epoch\",  # Save at the end of each epoch\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    load_best_model_at_end=True,  # Load the best model at the end of training\n)\n\n\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset['validation'],\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n# Here comes your code for the alternative approach\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\n\nX_train = [example['Tweet'] for example in dataset['train']]\nX_val = [example['Tweet'] for example in dataset['validation']]\n\nlabel_fields = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\ny_train = [[example[field] for field in label_fields] for example in dataset['train']]\ny_val = [[example[field] for field in label_fields] for example in dataset['validation']]\n\n# TF-IDF and Logistic Regression pipeline\nmodel = make_pipeline(\n    TfidfVectorizer(),\n    OneVsRestClassifier(LogisticRegression())\n)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Evaluate on the validation set\ny_pred = model.predict(X_val)\nprint(classification_report(y_val, y_pred))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Sinem_Dnmez.ipynb",
        "question": "#### Here comes your discussion",
        "answer": "\"\"\"\nBERT Approach Results\nTraining Loss: Decreased from 0.5877 to 0.4312 over 3 epochs, indicating that the model was learning effectively.\nValidation Loss: Showed a decreasing trend, suggesting good generalization.\nF1 Micro Score: 0.217765 in the last epoch, which is a decent score for multi-label classification but indicates room for improvement.\nHamming Loss: 0.198545 in the last epoch, lower is better for this metric.\n\nAlternative Approach (TF-IDF + Logistic Regression) Results\nPrecision and Recall: Varied significantly across labels, with some labels showing high precision (e.g., 0.83 for label 0) but low recall.\nF1-Score: Generally low for individual labels, indicating that the model struggled to balance precision and recall.\nOverall Performance: The micro and macro average scores show that the model's overall performance was quite limited, particularly in recall.\n\nBERT demonstrated an ability to learn and generalize from the training data, as evidenced by the decreasing loss and decent F1 score\nbut the relatively low F1 score and Hamming loss suggest that the model might require further tuning, more data, or additional epochs of training for optimal performance.\n\nTF-IDF + Logistic Regression Approach showed simplicity and faster training compared to BERT. Does not require extensive computational resources.\nBut it has lower performance metrics across the board. Struggles particularly with recall, suggesting that it fails to identify many relevant instances of each label.\n\"\"\""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Jan_Rogge.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# Here comes your code\nfrom datasets import load_dataset\ndataset = load_dataset(\"kan_hope\")#load a dataset given the specifications above, I chose: https://huggingface.co/datasets/kan_hope/\ndataset[\"train\"][-1]#labels are 0, not hope and 1 hope, output last datapoint"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Jan_Rogge.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")#set model\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")#set the tokenizer\n\ndef tokenize_data(dataset):\n    encoded_dataset = tokenizer(dataset[\"text\"])#encode the text part of the dataset using berts tokenizer\n    return(encoded_dataset)\n\ndataset = dataset.map(tokenize_data, remove_columns=[\"text\"])#remove the text column when done\ndataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True, remove_columns=[\"label\"])#set label to labels column\ndataset[\"train\"].features.keys()#features are saved in the .features, so to .features.keys gives the keys\nimport torch\n\n#set the format of the dataset to torch:\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=32)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Jan_Rogge.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n\n#id to label dict:\nid2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\nlabel2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n\n#set the model:, setting problem type to multi label classification gives an error here, maybe needs more preprocessing?\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)#define the datacollator\n\n#compute the desired metrics using the sklearn classes\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n\n    predictions = np.argmax(predictions, axis=1)#get best prediction value\n\n    #calculate metrics:\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n    prec = precision_score(labels, predictions)\n    rec = recall_score(labels, predictions)\n\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": prec, \"recall\": rec}\n\ntraining_args = TrainingArguments(\n    output_dir=\"my_model\",#save the model\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,#set to 4 cause I dont have a lot of VRAM\n    per_device_eval_batch_size=4,\n    num_train_epochs=5,#train for 5 epochs, could potentially be higher for better results\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\n#trainer class, sets the model, training args, dataset and metrics\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n# Here comes your code for the alternative approach\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n\ndataset = load_dataset(\"kan_hope\")#reload the dataset\n\n#set the splits:\nX_train = dataset[\"train\"][\"text\"]\ny_train = dataset[\"train\"][\"label\"]\n\nX_test = dataset[\"test\"][\"text\"]\ny_test = dataset[\"test\"][\"label\"]\n\n#vectorize the text data:\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train)\nX_test = vectorizer.transform(X_test)\n\n#fit the svm model\nsvm_model = SVC(kernel='linear')\nsvm_model.fit(X_train, y_train)\n\n#predict:\ny_pred = svm_model.predict(X_test)\n\n#and evaluate:\nprint(\"accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"f1-score:\", f1_score(y_test, y_pred))\nprint(\"precision:\", precision_score(y_test, y_pred))\nprint(\"recall:\", recall_score(y_test, y_pred))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Jan_Rogge.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Assignment_4_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n\n# a) Load a multi-label text classification dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n# Other multi-label text classification dataset\n# load_dataset(\"joelniklaus/online_terms_of_service\")\n\n\n# b) Print the last data point in the validation set\nvalidation_last_point = dataset['validation'][-1]\nprint(validation_last_point)"
    },
    {
        "file_name": "Assignment_4_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nfrom datasets import load_dataset\nimport torch\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# a) Write a function to tokenize the data\ndef tokenize_data(dataset):\n    def tokenize_batch(batch):\n        # Tokenize the text\n        inputs = tokenizer(batch['Tweet'], truncation=True, padding=True, return_tensors=\"pt\")\n\n        # Tokenize and convert labels to a tensor of floats\n        label_columns = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n        labels = torch.tensor([list(map(float, [batch[column] for column in label_columns]))])\n\n        # Add labels to the dictionary\n        inputs['labels'] = labels\n\n        return inputs\n\n    # Tokenize the training, validation, and test sets\n    encoded_dataset = dataset.map(tokenize_batch)\n\n    # Remove unnecessary keys\n    encoded_dataset = encoded_dataset.remove_columns(['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'])\n\n\n    return encoded_dataset\n\n# Load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\n# Tokenize the dataset\ntokenized_dataset = tokenize_data(dataset)\n\n# Set the format to PyTorch tensors\ntokenized_dataset.set_format(\"torch\")\n\n# Print the keys of the last data point in the validation set\nlast_data_point_keys = tokenized_dataset['validation'][-1].keys()\nprint(last_data_point_keys)"
    },
    {
        "file_name": "Assignment_4_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for the BERT approach\n# Here comes your code for the alternative approach"
    },
    {
        "file_name": "Assignment_4_Karsten_Weber__Leo_Schmidt.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Gjergj_Plepi.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# Here comes your code\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\", 'subtask5.english')\ndataset\nprint(dataset['validation'][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Gjergj_Plepi.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport numpy as np\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nprint(dataset['train'][3]['Tweet'])\ntokenizer(dataset['train'][3]['Tweet'])\nlabels = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\ndef tokenize_data(dataset):\n    # here comes your code\n\n    encoded_dataset = tokenizer(dataset[\"Tweet\"], padding=\"max_length\", truncation=True) # map the tweet to {'input_ids':[...], 'token_type_ids':[...], 'attention_mask':[...]}\n\n    encoded_labels = np.zeros( (len(dataset[\"Tweet\"]), len(labels)) ) # batch_size x number_of_labels\n\n    #group the values (True or False) of each label in all the Tweets in the batch\n    grouped_labels = { label: dataset[label] for label in dataset.keys() if label in labels } # number_of_labels keys, each with batch_size values\n\n    #map the labels to IDs\n    for i, label in enumerate(labels):\n        encoded_labels[:, i] = grouped_labels[label] #True will be stored as 1.0 , False will be stored as 0.0  (both as float values)\n\n    \n    encoded_dataset['labels'] = encoded_labels.tolist() # new feature representing the labels of a Tweet\n    \n    return encoded_dataset\ntokenized_dataset = dataset.map(tokenize_data, batched=True)\ntokenized_dataset = tokenized_dataset.select_columns(['input_ids', 'token_type_ids', 'attention_mask', 'labels']) #filter the columns\ntokenized_dataset.set_format(type=\"torch\")\ntokenized_dataset\ntokenized_dataset['validation'][-1].keys()\nlen(tokenized_dataset['train'][3]['input_ids'])\nlen(tokenized_dataset['train'][4]['labels'])\ntokenized_dataset['validation'][-1]['labels']"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Gjergj_Plepi.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "id2label = {i:label for i, label in enumerate(labels)}\nlabel2id = {label:i for i, label in enumerate(labels)}\n# Here comes your code for BERT Approach\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id.keys()),  problem_type=\"multi_label_classification\", label2id=label2id, id2label=id2label)\nimport torch\n\nprint(torch.cuda.is_available())\ndevice = torch.device('cuda:0')\nmodel = model.to(device)\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"nlp_bert_approach\",\n    learning_rate=2e-5,\n    auto_find_batch_size=True,\n    num_train_epochs=5,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    weight_decay=0.01,\n    save_strategy=\"steps\",\n    metric_for_best_model='f1_score',\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    push_to_hub=False\n)\n\"\"\"Evaluation\"\"\"\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\n\ndef compute_metrics(eval_preds):\n    threshold=0.5\n    logits, labels = eval_preds\n\n    sigmoid = torch.nn.Sigmoid()\n    probabilities = sigmoid(torch.Tensor(logits)) # a value between 0 and 1\n\n    predictions = np.zeros(probabilities.shape) # 3259 x 11\n    predictions[np.where(probabilities >= threshold)] = 1\n\n    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n    precision = precision_score(y_true=labels, y_pred=predictions, average='micro')\n    recall = recall_score(y_true=labels, y_pred=predictions, average='micro')\n    f1 = f1_score(y_true=labels, y_pred=predictions, average='micro')\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1\n    }\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\ntrainer.train()\n\"\"\"Evaluation on the validation set\"\"\"\ntrainer.evaluate(tokenized_dataset['validation'])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.svm import LinearSVC\nlabels = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n# Here comes your code for the alternative approach\ntext_train = dataset[\"train\"][:][\"Tweet\"]\ntext_test = dataset[\"test\"][:][\"Tweet\"]\ntext_val = dataset[\"validation\"][:][\"Tweet\"]\n\nlabel_train = np.array([dataset[\"train\"][:].get(key) for key in labels]).T.astype(float).tolist()\nlabel_test = np.array([dataset[\"test\"][:].get(key) for key in labels]).T.astype(float).tolist()\nlabel_val = np.array([dataset[\"validation\"][:].get(key) for key in labels]).T.astype(float).tolist()\n\"\"\"Encode the text\"\"\"\ntf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1)) # 1-gram with words (only unigrams).\nencoded_input_matrix = tf_idf_vectorizer.fit_transform(text_train) \nencoded_test_matrix = tf_idf_vectorizer.transform(text_test)\nencoded_val_matrix = tf_idf_vectorizer.transform(text_val) \nencoded_test_matrix.shape\n\"\"\"Initialize the classifier\"\"\"\nsvm = LinearSVC(random_state=42)\nsvm_multilabel = MultiOutputClassifier(svm, n_jobs=1) #SVM as a Multilabel classifier\n\"\"\"Train the classifier\"\"\"\nsvm_multilabel.fit(encoded_input_matrix.toarray(), label_train)\n\"\"\"Test the classifier\"\"\"\npredictions = svm_multilabel.predict(encoded_test_matrix.toarray())\n\"\"\"Evaluate the model\"\"\"\naccuracy = accuracy_score(y_true=label_test, y_pred=predictions)\nprecision = precision_score(y_true=label_test, y_pred=predictions, average='micro')\nrecall = recall_score(y_true=label_test, y_pred=predictions, average='micro')\nf1 = f1_score(y_true=label_test, y_pred=predictions, average='micro')\n\nprint(\"Accuracy: \", accuracy)\nprint(\"Precision: \", precision)\nprint(\"Recall: \", recall)\nprint(\"F1 score\", f1)\n\"\"\"Evaluate on the validation set\"\"\"\npredictions = svm_multilabel.predict(encoded_val_matrix.toarray())\n\naccuracy = accuracy_score(y_true=label_val, y_pred=predictions)\nprecision = precision_score(y_true=label_val, y_pred=predictions, average='micro')\nrecall = recall_score(y_true=label_val, y_pred=predictions, average='micro')\nf1 = f1_score(y_true=label_val, y_pred=predictions, average='micro')\n\nprint(\"Accuracy: \", accuracy)\nprint(\"Precision: \", precision)\nprint(\"Recall: \", recall)\nprint(\"F1 score\", f1)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Gjergj_Plepi.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Gjergj_Plepi.ipynb",
        "question": "### 1. BERT approach\nAdvantages:\n- provides better performance than the traditional method \n- BERT captures contextual information\n- BERT pre-trained on millions of data, making it easily adaptable for this NLP task by a simple fine-tuning\n- Better representation/encodings of the input data (words of the sentences), based on context\nDisadvantages:\n- Computationally very expensive to run (needs GPU) since the model is very large\n- Slower to run than the traditional method\n### 2. TF-IDF + SVM approach\nAdvantages:\n- faster to run \n- computationally not very expensive, the model is small and runs very fast\n- The TF-IDF representation of sentences is interpretable\nDisadvantages:\n- Doesnt consider context at all\n- SVM assumes that the data is linearly separable\n- The TF-IDF representations are very large\nIn this dataset, where we need to assign a set of emotions (labels) to each Tweet (input sentence), context of each word, and how each word relates to other words in the sentence, are very important features. That means that the BERT approach is better suitable for this task, of course is computational resources are available!",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Ali_Sehran.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n\n# Loading the dataset\ndataset = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\n# Printing last data point of the validation set \nprint(dataset['validation'][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Ali_Sehran.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizerFast\nfrom torch import FloatTensor\n\n# Loading the tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Tokenizing the dataset\ndef tokenize_data(dataset):\n    encoded_dataset = tokenizer(dataset['Tweet'], padding='max_length', truncation=True)\n    \n    # Convert labels to tensor of floats\n    labels = [dataset[emotion] for emotion in ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']]\n    labels = FloatTensor(labels).T\n\n    return {**encoded_dataset, 'labels': labels}\n  \n# Downsizing the dataset to 50 samples\nsubset_size = 50\n\n# Initializing an empty dictionary\ndownsized_dataset = {}\n\n# Loop over each split in the dataset\nfor split in dataset.keys():\n    # Randomly sample subset_size elements from the split\n    downsized_dataset[split] = dataset[split].shuffle(seed=42).select(range(subset_size))\n\nencoded_dataset = {}\n\n# Loop over each split in the downsized_dataset\nfor split in downsized_dataset.keys():\n    # Tokenize the downsized split\n    encoded_dataset[split] = downsized_dataset[split].map(tokenize_data, remove_columns=downsized_dataset[split].column_names)\nprint(encoded_dataset['validation'][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Ali_Sehran.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions\n    # Apply a threshold to the predictions\n    threshold = 0.5\n    preds = (preds > threshold).astype(int)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n# BERT APPROCACH\n\nfrom transformers import AutoModelForSequenceClassification,  Trainer, TrainingArguments\n\n# Labels\nlabel_names = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n\n# Lable to ID and ID to label mapping\nlabel_to_id = {label: i for i, label in enumerate(label_names)}\nid_to_label = {i: label for i, label in enumerate(label_names)}\n\n# Loading the model\nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_names), id2label=id_to_label, label2id=label_to_id, problem_type='multi_label_classification')\n\n# Training arguments\ntraining_args = TrainingArguments(\n     output_dir='./results',\n    num_train_epochs=2,  \n    per_device_train_batch_size=8, \n    per_device_eval_batch_size=8,\n    evaluation_strategy='steps',  \n    logging_steps=10, \n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset['validation'],\n    compute_metrics=compute_metrics\n)\n\n# Training the model \ntrainer.train()\n\n# Evaluating the model\ntrainer.evaluate()\n# Alternative Approach\n\nimport pandas as pd\n\n# Convert to pandas DataFrame\ndf = {}\nfor split in encoded_dataset.keys():\n    df[split] = pd.DataFrame(encoded_dataset[split])\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df['train']['input_ids'], df['train']['labels'], test_size=0.2, random_state=42)\n\n# Convert input data to string\nX_train = [str(x) for x in X_train]\nX_test = [str(x) for x in X_test]\n\n# Use TF-IDF for tokenization\nvectorizer = TfidfVectorizer()\n\n# Convert labels to binary array\nmlb = MultiLabelBinarizer()\ny_train = mlb.fit_transform(y_train)\ny_test = mlb.transform(y_test)\n\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Use a logistic regression for classification\nlogreg_clf = OneVsRestClassifier(LogisticRegression())\nlogreg_clf.fit(X_train_tfidf, y_train)\n\n# Evaluate the performance of the model on the validation set using the metrics Accuracy, F1-score, precision, recall\ny_pred_logreg = logreg_clf.predict(X_test_tfidf)\n\nprint(\"Logistic Regression Accuracy: \", accuracy_score(y_test, y_pred_logreg))\nprint(\"Logistic Regression F1 Score: \", f1_score(y_test, y_pred_logreg, average='weighted'))\nprint(\"Logistic Regression Precision: \", precision_score(y_test, y_pred_logreg, average='weighted'))\nprint(\"Logistic Regression Recall: \", recall_score(y_test, y_pred_logreg, average='weighted'))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Ali_Sehran.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Assignment_4_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install datasets\nimport torch\nfrom datasets import load_dataset\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Load the dataset\ndataset = load_dataset('joelniklaus/covid19_emergency_event')\n\n# Check the last data point in the validation set\nvalidation_set = dataset['validation']\nlast_data_point = validation_set[-1]\nprint(last_data_point)"
    },
    {
        "file_name": "Assignment_4_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "!pip install transformers torch\nfrom transformers import BertTokenizer\nimport torch\n\ndef tokenize_data(dataset):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def encode(examples):\n        # Tokenize text and pad to the maximum length\n        tokenized_inputs = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n        # Combine the event labels into a single multi-label array\n        labels = []\n        for i in range(len(examples['text'])):\n            label = [int(examples[event][i]) for event in ['event1', 'event2', 'event3', 'event4', 'event5', 'event6', 'event7', 'event8']]\n            labels.append(label)\n        labels = torch.tensor(labels).float()\n        tokenized_inputs['labels'] = labels\n\n        return tokenized_inputs\n\n    # Apply tokenization and encoding to the entire dataset\n    encoded_dataset = dataset.map(encode, batched=True)\n\n    # Set the format to PyTorch tensors\n    encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n    return encoded_dataset\n\n\nencoded_dataset = tokenize_data(dataset)\n\n# Print keys of the last data point in the validation set\nprint(encoded_dataset['validation'][-1].keys())"
    },
    {
        "file_name": "Assignment_4_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "!pip install accelerate -U\n!pip install transformers[torch]\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nimport numpy as np\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Map labels to integers and vice versa\nlabel2id = {f\"event{i}\": i for i in range(1, 9)}\nid2label = {i: f\"event{i}\" for i in range(1, 9)}\n\n# Load pre-trained BERT model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=len(label2id),\n    problem_type=\"multi_label_classification\",\n    id2label=id2label,\n    label2id=label2id\n)\n\n# Move the model to the GPU if available\nmodel.to(device)\n\n# Compute metrics function\ndef compute_metrics(p):\n    pass\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,  # Reduce the number of epochs\n    per_device_train_batch_size=16,  # Adjust batch size\n    max_steps=500,  # Limit the number of training steps\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import make_pipeline\n\n# Prepare data\ntrain_texts = [example['text'] for example in dataset['train']]\n# Assuming each 'event' key in the dataset is a separate boolean label\ntrain_labels = [[example[event] for event in ['event1', 'event2', 'event3', 'event4', 'event5', 'event6', 'event7', 'event8']] for example in dataset['train']]\n\n\n# Create TF-IDF + SVM pipeline\nmodel = make_pipeline(\n    TfidfVectorizer(),\n    OneVsRestClassifier(SVC())\n)\n\n# Train the model\nmodel.fit(train_texts, train_labels)\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n# Prepare validation data\nvalidation_texts = [example['text'] for example in dataset['validation']]\nvalidation_labels = [[example[event] for event in ['event1', 'event2', 'event3', 'event4', 'event5', 'event6', 'event7', 'event8']] for example in dataset['validation']]\n\n# Predict on validation set\nvalidation_preds = model.predict(validation_texts)\n\n# Evaluate the model\naccuracy = accuracy_score(validation_labels, validation_preds)\nf1 = f1_score(validation_labels, validation_preds, average='weighted')\nprecision = precision_score(validation_labels, validation_preds, average='weighted')\nrecall = recall_score(validation_labels, validation_preds, average='weighted')\n\nprint(f\"Accuracy: {accuracy}\\nF1-Score: {f1}\\nPrecision: {precision}\\nRecall: {recall}\")"
    },
    {
        "file_name": "Assignment_4_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# Here comes your code\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\",'subtask5.english')\nlast_data_point = dataset[\"validation\"][-1]\nprint(\"Last Data Point in Validation Set:\")\nprint(last_data_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport torch\n\n\ndef tokenize_data(dataset):\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    def tokenize_function(data):\n         return tokenizer(data[\"Tweet\"], truncation=True)\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n    return (tokenized_datasets)\n\nlabel_keys = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\ntokenized_dataset = tokenize_data(dataset)\ntokenized_dataset.set_format(\"torch\")\nprint(tokenized_dataset['validation'][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_dataset = dataset['train']\nvalidation_dataset = dataset['validation']\n\ndef tokenize_function(examples):\n    return tokenizer(examples['Tweet'], truncation=True)\n\n# Tokenize the train and validation datasets\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n\ntokenized_train_dataset.set_format(\"torch\")\ntokenized_validation_dataset.set_format(\"torch\")\n\nlabel_map = {0: 'anger', 1: 'anticipation', 2: 'disgust', 3: 'fear', 4: 'joy', 5: 'love', 6: 'optimism', \n             7: 'pessimism', 8: 'sadness', 9: 'surprise', 10: 'trust'}\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=len(label_map),\n    id2label=label_map,\n    label2id={label: i for i, label in label_map.items()},\n    problem_type=\"multi_label_classification\"\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=200,\n)\n\ndef compute_metrics(p):\n    predictions = torch.sigmoid(torch.tensor(p.predictions)).numpy()\n    labels = p.label_ids\n    threshold = 0.5\n    preds = (predictions > threshold).astype('int')\n    \n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1_score\": f1_score(labels, preds, average='micro'),\n        \"precision\": precision_score(labels, preds, average='micro'),\n        \"recall\": recall_score(labels, preds, average='micro'),\n    }\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_validation_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\ntrainer.evaluate()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\n\npipeline = make_pipeline(\n    TfidfVectorizer(),\n    OneVsRestClassifier(SVC())\n)\n\n# pipeline.fit(X_train, y_train)\n\n# y_pred = pipeline.predict(X_val)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Akmalkhon_Khashimov_Nijat_Sadikhov.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Ilaha_Manafova.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\ndataset\nprint(dataset['validation'][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Ilaha_Manafova.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "model_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\ndef tokenize_data(dataset):\n\n    # Tokenize text using BERT tokenizer\n    def tokenize_function(examples):\n        return tokenizer(examples['Tweet'], padding='max_length', truncation=True)\n\n    # Apply tokenization to the dataset\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n    # Encode labels as tensors\n    def encode_labels(example):\n        label_cols = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n        label_values = [example[col] for col in label_cols]\n        example['labels'] = torch.tensor(label_values, dtype=torch.float32)\n        return example\n\n    # Apply label encoding to the tokenized dataset\n    tokenized_dataset = tokenized_dataset.map(encode_labels)\n\n    # Set the format\n    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n    return tokenized_dataset\n\n# Tokenize the dataset\nencoded_dataset = tokenize_data(dataset)\n\n# Print keys of the last data point in the validation set\nlast_val_data = encoded_dataset['validation'][-1]\nprint(last_val_data.keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Ilaha_Manafova.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\n# Here comes your code for the alternative approach"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Ilaha_Manafova.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install transformers[torch]\n!pip install datasets\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\ndataset\nprint(dataset[\"validation\"][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\n\n\ndef tokenize_data(dataset):\n    # here comes your code\n\n    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    encoded_dataset = dataset.map(lambda examples: bert_tokenizer(examples['Tweet'], padding='max_length', truncation=True), batched=True)\n\n    # turn labels into a matrix of floats\n    labels_list = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n\n    def map_labels(example):\n        example['labels'] = [float(example[label]) for label in labels_list]\n        return example\n\n    # convert the dictionary of labels into a matrix of floats\n    labeled_dataset = encoded_dataset.map(map_labels)\n\n    # create a new dataset with only necessary columns\n    columns_to_return = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n    encoded_dataset = labeled_dataset.remove_columns([col for col in labeled_dataset['train'].features if col not in columns_to_return])\n\n    return(encoded_dataset)\n\nencoded_dataset = tokenize_data(dataset)\nprint(encoded_dataset[\"validation\"][-1])\nprint(encoded_dataset[\"validation\"][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "import torch\n\n# Here comes your code for BERT Approach\n\n# import the model\nfrom transformers import BertForSequenceClassification\n\n# define the model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=11)\n\n# import the trainer\nfrom transformers import Trainer, TrainingArguments\n\n# define the training arguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=50,\n    load_best_model_at_end=True,\n    evaluation_strategy=\"steps\",\n    save_strategy='steps',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True\n)\n\n# train the model\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\n\nmodel.to(device)\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids.argmax(-1)\n    preds = pred.predictions.argmax(-1)\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    f1 = f1_score(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# define the trainer\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=encoded_dataset[\"train\"],         # training dataset\n    eval_dataset=encoded_dataset[\"validation\"],             # evaluation dataset\n  compute_metrics=compute_metrics\n    )\n\ntrainer.train()\n# evaluate the model\n\ntrainer.evaluate()\n# to view the plots of metrics, let's use tensorboard\n%load_ext tensorboard\n%tensorboard --logdir logs\n##CODE for alternative approach\n\nimport pandas as pd\nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n\ntrain_data = pd.DataFrame(dataset['train'])\nvalidation_data = pd.DataFrame(dataset['validation'])\ntrain_data = train_data.drop('ID', axis=1)\nvalidation_data = validation_data.drop('ID', axis=1)\n\ntrain_X = train_data['Tweet']\nval_X = validation_data['Tweet']\n\ntrain_Y = train_data.drop('Tweet', axis=1)\nval_Y = validation_data.drop('Tweet', axis=1)\n\n# Tokenizing\ntf_idf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\nX_idf = tf_idf_vectorizer.fit_transform(train_X)\n\ntfidf_tokens = tf_idf_vectorizer.get_feature_names_out()\ndf_tfidfvect = pd.DataFrame(data = X_idf.toarray(),columns = tfidf_tokens)\n\n\n# Training mutliclass SVM\nsvm_classifier = svm.SVC(kernel='linear')\nmulti_label_svm = MultiOutputClassifier(svm_classifier)\n\nmulti_label_svm.fit(X_idf, train_Y);\n\n# evaluating model\nval_X_idf = tf_idf_vectorizer.transform(val_X)\npredictions_svm = multi_label_svm.predict(val_X_idf)\nf1 = f1_score(val_Y, predictions_svm, average='weighted')\nprecision = precision_score(val_Y, predictions_svm, average = 'weighted')\naccuracy = accuracy_score(val_Y, predictions_svm)\nrecall = recall_score(val_Y, predictions_svm, average = 'weighted')\n\nmetrics_svm = {'f1': f1,\n               'accuracy': accuracy,\n               'precision': precision,\n               'recall': recall,\n               }\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1 Score: {f1}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\n\n#plotting performance for both approaches\nimport matplotlib.pyplot as plt\n\nmetrics = ['f1', 'accuracy', 'precision', 'recall']\n# values_model1 = [metrics_bert[metric] for metric in metrics]\nvalues_model2 = [metrics_svm[metric] for metric in metrics]\n\nbar_width = 0.35\nindex = range(len(metrics))\n\nfig, ax = plt.subplots()\n# bar1 = ax.bar(index, values_model1, bar_width, label='Bert')\nbar2 = ax.bar([i + bar_width for i in index], values_model2, bar_width, label='SVM (Alternative Approach)')\n\nax.set_xlabel('Metrics')\nax.set_ylabel('Values')\nax.set_title('Comparison of Performance Metrics')\nax.set_xticks([i + bar_width / 2 for i in index])\nax.set_xticklabels(metrics)\nax.legend()\n\nplt.tight_layout()\nplt.show()"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Orlivskyi.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "import numpy as np\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\nlast_data_point = dataset['validation'][-1]\n\nprint(last_data_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Orlivskyi.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nimport torch\n\n\ndef tokenize_data(dataset, tokenizer):\n    def tokenize_function(record):\n        label_definitions = [col for col in record if col not in ['Tweet', 'ID']]\n        # Tokenize the texts and pad all records to the same length\n        tokenized_batch = tokenizer(\n            record[\"Tweet\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=512\n        )\n        \n        # Create a list of labels for the particular record. Technically, this should be doen outside of this fucntion, because the labels are the same for all the records\n        labels = []\n        for i in range(len(record[\"Tweet\"])):\n            record_labels = [record[col][i] for col in label_definitions]\n            labels.append(record_labels)\n\n        # Add the labels to the tokenized batch\n        tokenized_batch[\"labels\"] = torch.tensor(labels, dtype=torch.float)\n        return tokenized_batch\n\n    # Apply the tokenization to all recoreds of the dataset\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n    # print(tokenized_dataset)\n    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])  # Set the format to PyTorch tensors\n    return tokenized_dataset\n\n# Use BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize + encode the dataset and print the keys of the last record\ntokenized_dataset = tokenize_data(dataset, tokenizer)\nprint(tokenized_dataset['validation'][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Orlivskyi.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\nlabel_columns = [col for col in dataset['validation'].features if col not in ['Tweet', 'ID']]\nnum_labels = len(label_columns)\n\nprint('label_columns:', label_columns)\nprint('num_labels:', num_labels)\n\n# map labels to integers\nlabel2id = {label: idx for idx, label in enumerate(label_columns)}\nid2label = {idx: label for label, idx in label2id.items()}\n\nprint('label2id:', label2id)\nprint('id2label:', id2label)\n\n# initialize model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=num_labels,\n    problem_type=\"multi_label_classification\",\n    id2label=id2label,\n    label2id=label2id\n)\n\n\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# define function which will compute the metrics of the model\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    rec = recall_score(labels, preds, average='weighted')\n    prec = precision_score(labels, preds, average='weighted', zero_division=0)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': prec,\n        'recall': rec\n    }\n\n# initialize training arguments and the trainer\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    compute_metrics=compute_metrics,\n)\n\n# train and then evaluate the model\n#! Takes a very long time\ntrainer.train()\n\nevaluation_results = trainer.evaluate()\n\nprint(\"BERT Evaluation Results:\", evaluation_results)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Orlivskyi.ipynb",
        "question": "#### BERT APPROACH WAS TAKING TOO LONG ON MY MACHINE, SO I WASN'T ABLE TO WAIT UNTIL THE END TO FINISH CALCULATING IT",
        "answer": "# 'label_columns' will contain the list of labels\nlabel_columns = [col for col in dataset['validation'].features if col not in ['Tweet', 'ID']]\nnum_labels = len(label_columns)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\n\n# Extract texts and labels\ntexts = dataset['train']['Tweet']\nlabels = [dataset['train'][label_col] for label_col in label_columns]\nlabels = torch.tensor(labels, dtype=torch.float).numpy()\nlabels = np.array(labels).T  # Transpose to get correct shape\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    texts, labels, test_size=0.2, random_state=42\n)\n\ntfidf_vectorizer = TfidfVectorizer(max_features=512)\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_texts)\nval_tfidf = tfidf_vectorizer.transform(val_texts)\n\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Create an instance of the SVM model\nsvm_model = OneVsRestClassifier(SVC(kernel='linear'))\n\n# Fit the model and make predictions\nsvm_model.fit(train_tfidf, train_labels)\n\nval_preds = svm_model.predict(val_tfidf)\n\n# Calculate metrics of the model\naccuracy = accuracy_score(val_labels, val_preds)\nf1 = f1_score(val_labels, val_preds, average='weighted')\nprecision = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\nrecall = recall_score(val_labels, val_preds, average='weighted')\n\nprint(\"SVM Evaluation Results:\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1-Score: {f1}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Orlivskyi.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Orlivskyi.ipynb",
        "question": "#### Strengths of BERT approach:\n1. BERT approach captures deep contextual relationships between words due to the model's bidirectional training mechanism\nwhich allows to understand the context and meaning of a particular word based on its surrounding neighbors in the sentance.\nIt also allows BERT to be more effective with ambiguous words\n2. Since we are using a pretrained BERT, it has already learned a great amount of information about the language which results in performence boost in tasks with relatively small amount of training data.\n3. Since BERT is a deep neural network based on transformers architecture, it's performance on the tasks of language classification is quite high as compared to simpler approaches.\n#### Weaknesses of BERT approach:\n1. Since BERT is a large model it requires more computational resources to train and use.\n2. Due to the size and complexity of BERT, it also requires significant amount of time for training.\n3. BERT still has a risk of overfitting on small datasets and might require additional fine-tuning to mediate this.\n#### Strengths of TF-IDF and SVM approach:\n1. This appraoch is much more straightforward and easy to use than a complex deep model like BERT\n2. The computatinal resources needed to work with this approach are much lower. When working on this task alternative approach was computed in a few seconds while BERT required more than an hour of time to train.\n3. Results of this alternative approach are typically easier to interpret and reason about, since the methods used are more straightforward and understandable.\n4. If the text classification task happens to have linearly separable data, SVM performs very well on it.\n#### Weaknesses of TF-IDF and SVM approach:\n1. Since TF-IDF is a simple (bag-of-words) tecnique, it does not capture the context. It treats each word independently and does not allow to capture complex language patterns leading to decreased performance.\n2. TF-IDF and SVM approach might not be as resource efficient on large datasets and become increasingly computationally expensive. For instance, in terms of memory consumption for storing feature vectors.\nIn short, BERT has better performance (achieves better results) but requires much more resources and is slow in training. TF-IDF and SVM approach is much more efficient to train but achieves much worse results because\nof the method simplicity and lack of context capturing. For instance, the accuracy score of SVM approach in this task was just about 17%. Results of this approach are, however, easier to interpret.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Jing_Wu.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n\n# Step a: Load the dataset\ndataset = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\n\n# Step b: Print the last data point in the validation set\nvalidation_last_point = dataset['validation'][-1]\nprint(validation_last_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Jing_Wu.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "pip install torch\nfrom transformers import AutoTokenizer\nimport torch\n\ndef tokenize_data(dataset):\n    # Load BERT tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    # Tokenize function\n    def tokenize_function(examples):\n        # Tokenize text\n            examples['Tweet'],\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'  # Return PyTorch tensors\n        )\n        \n        # Convert labels to tensors of floats\n        examples['labels'] = torch.tensor(examples['labels'], dtype=torch.float32)\n        \n        return {**tokenized_text, 'labels': examples['labels']}\n    \n    # Tokenize the dataset\n    encoded_dataset = dataset.map(tokenize_function, batched=True)\n    \n    # Set the format to PyTorch tensors\n    encoded_dataset.set_format(\"torch\")\n    \n    return encoded_dataset\n\n# Usage:\n# Assuming you have already tokenized the dataset using tokenize_data(dataset)\nlast_validation_point_keys = encoded_dataset['validation'][-1].keys()\nprint(last_validation_point_keys)\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\ndef tokenize_data(dataset):\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n    # Tokenize function\n    def tokenize_function(examples):\n#         return tokenizer(examples['text'], padding='max_length', truncation=True)\n        return tokenizer(examples['Tweet'], padding='max_length', truncation=True)\n\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n    \n    # Convert labels to tensors of floats\n    def convert_labels_to_floats(examples):\n#         examples['labels'] = [[float(label) for label in example] for example in examples['labels']]\n#         return examples\n        examples['target'] = [[float(label) for label in example] for example in examples['target']]\n        return examples\n\n    tokenized_dataset = tokenized_dataset.map(convert_labels_to_floats)\n    tokenized_dataset.set_format(\"torch\")\n    \n    return tokenized_dataset\n\n# Load dataset\ndataset = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\n\n# Tokenize dataset\nencoded_dataset = tokenize_data(dataset)\n\n# Print keys of the last data point in the validation set\nlast_validation_point_keys = encoded_dataset['validation'][-1].keys()\nprint(last_validation_point_keys)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Jing_Wu.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset, load_metric\nimport torch\n\n# Load dataset\ndataset = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\n\n# Define label mappings\nlabel2id = {label: i for i, label in enumerate(dataset['train'].features['labels'].names)}\nid2label = {i: label for label, i in label2id.items()}\n\n# Define BERT-based model\nmodel_name = 'bert-base-uncased'\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id,\n    problem_type='multi_label_classification'\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    evaluation_strategy='epoch',\n    num_train_epochs=3,\n    output_dir='./results',\n)\n\n# Define metric for evaluation\nmetric = load_metric('accuracy')\n\n# Define compute_metrics function\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions > 0.5  # Threshold for multilabel classification\n    return metric.compute(predictions=preds, references=labels)\n\n# Define Trainer and train the model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n# Here comes your code for the alternative approach\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer()\nX_train = tfidf_vectorizer.fit_transform(dataset['train']['text'])\nX_val = tfidf_vectorizer.transform(dataset['validation']['text'])\n\n# Convert labels to a multi-label format\ny_train = dataset['train']['labels']\ny_val = dataset['validation']['labels']\n\n# SVM Classifier\nsvm_classifier = SVC(kernel='linear')\nsvm_classifier.fit(X_train, y_train)\n\n# Predictions\nsvm_predictions = svm_classifier.predict(X_val)\n\n# Evaluate performance\naccuracy = accuracy_score(y_val, svm_predictions)\nprecision = precision_score(y_val, svm_predictions, average='weighted')\nrecall = recall_score(y_val, svm_predictions, average='weighted')\nf1 = f1_score(y_val, svm_predictions, average='weighted')\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1 Score: {f1}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Jing_Wu.ipynb",
        "question": "#### Here comes your discussion",
        "answer": "Strengths of BERT Approach:\n\nSemantic Understanding: BERT captures intricate relationships between words, offering a deeper semantic understanding.\nTransfer Learning: Leveraging pre-trained models allows for effective transfer learning, even with limited data.\nWeaknesses of BERT Approach:\n\nResource-Intensive: BERT requires significant computational resources and memory for training and inference.\nData Dependency: Performance heavily relies on the size and quality of training data.\nStrengths of Alternative Approach (TF-IDF + SVM):\n\nSimplicity: TF-IDF and SVM are relatively simple, interpretable, and have fewer computational requirements.\nEfficiency: These methods can work well with smaller datasets.\nWeaknesses of Alternative Approach:\n\nLack of Semantic Understanding: TF-IDF and Bag of Words lack semantic understanding and might struggle with capturing complex relationships between words.\nLimited Generalization: These methods might not generalize well to unseen data compared to pre-trained models like BERT.\nChoosing between these approaches often involves trade-offs between complexity, resource requirements, and the availability of labeled data. BERT might shine with larger datasets and complex relationships, while TF-IDF and SVM could be more efficient with smaller, well-structured datasets."
    },
    {
        "file_name": "Assignment_4_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install datasets\nimport torch\nfrom datasets import load_dataset\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Load the dataset\ndataset = load_dataset('joelniklaus/covid19_emergency_event')\n\n# Check the last data point in the validation set\nvalidation_set = dataset['validation']\nlast_data_point = validation_set[-1]\nprint(last_data_point)"
    },
    {
        "file_name": "Assignment_4_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "!pip install transformers torch\nfrom transformers import BertTokenizer\nimport torch\n\ndef tokenize_data(dataset):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def encode(examples):\n        # Tokenize text and pad to the maximum length\n        tokenized_inputs = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n        # Combine the event labels into a single multi-label array\n        labels = []\n        for i in range(len(examples['text'])):\n            label = [int(examples[event][i]) for event in ['event1', 'event2', 'event3', 'event4', 'event5', 'event6', 'event7', 'event8']]\n            labels.append(label)\n        labels = torch.tensor(labels).float()\n        tokenized_inputs['labels'] = labels\n\n        return tokenized_inputs\n\n    # Apply tokenization and encoding to the entire dataset\n    encoded_dataset = dataset.map(encode, batched=True)\n\n    # Set the format to PyTorch tensors\n    encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n    return encoded_dataset\n\n\nencoded_dataset = tokenize_data(dataset)\n\n# Print keys of the last data point in the validation set\nprint(encoded_dataset['validation'][-1].keys())"
    },
    {
        "file_name": "Assignment_4_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "!pip install accelerate -U\n!pip install transformers[torch]\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nimport numpy as np\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Map labels to integers and vice versa\nlabel2id = {f\"event{i}\": i for i in range(1, 9)}\nid2label = {i: f\"event{i}\" for i in range(1, 9)}\n\n# Load pre-trained BERT model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=len(label2id),\n    problem_type=\"multi_label_classification\",\n    id2label=id2label,\n    label2id=label2id\n)\n\n# Move the model to the GPU if available\nmodel.to(device)\n\n# Compute metrics function\ndef compute_metrics(p):\n    pass\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,  # Reduce the number of epochs\n    per_device_train_batch_size=16,  # Adjust batch size\n    max_steps=500,  # Limit the number of training steps\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import make_pipeline\n\n# Prepare data\ntrain_texts = [example['text'] for example in dataset['train']]\n# Assuming each 'event' key in the dataset is a separate boolean label\ntrain_labels = [[example[event] for event in ['event1', 'event2', 'event3', 'event4', 'event5', 'event6', 'event7', 'event8']] for example in dataset['train']]\n\n\n# Create TF-IDF + SVM pipeline\nmodel = make_pipeline(\n    TfidfVectorizer(),\n    OneVsRestClassifier(SVC())\n)\n\n# Train the model\nmodel.fit(train_texts, train_labels)\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n# Prepare validation data\nvalidation_texts = [example['text'] for example in dataset['validation']]\nvalidation_labels = [[example[event] for event in ['event1', 'event2', 'event3', 'event4', 'event5', 'event6', 'event7', 'event8']] for example in dataset['validation']]\n\n# Predict on validation set\nvalidation_preds = model.predict(validation_texts)\n\n# Evaluate the model\naccuracy = accuracy_score(validation_labels, validation_preds)\nf1 = f1_score(validation_labels, validation_preds, average='weighted')\nprecision = precision_score(validation_labels, validation_preds, average='weighted')\nrecall = recall_score(validation_labels, validation_preds, average='weighted')\n\nprint(f\"Accuracy: {accuracy}\\nF1-Score: {f1}\\nPrecision: {precision}\\nRecall: {recall}\")"
    },
    {
        "file_name": "Assignment_4_Mahdi_Rahimianaraki_And_Golnoosh_Sharifi.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Suyash_Thapa_.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install datasets\n!pip install torch\n!pip install 'transformers[torch]'\n# Here comes your code\nfrom datasets import load_dataset\ndataset = load_dataset('sem_eval_2018_task_1','subtask5.english')\nprint(type(dataset))\nprint(dataset['validation'][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Suyash_Thapa_.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport numpy as np\nfrom datasets import Dataset, load_dataset\n\ndef tokenize_data_function(examples, tokenizer, text_field='text', **kwargs):\n    return tokenizer(examples[text_field], **kwargs)\n\ndef convert_labels_to_array(examples, label_fields):\n    values = examples.select_columns(label_fields)\n    new_examples = examples.add_column('labels', values)\n    new_examples = examples.add_column('labels', list(map(lambda x: x.values(), new_examples['labels'])))\n    new_examples = examples.add_column('labels', list(map(lambda x: np.array(x, dtype=float), new_examples['labels'])))\n\n    return new_examples.remove_columns(label_fields)\n\ndef tokenize_dataset(dataset, tokenizer_name='bert-base-uncased', text_field='text', tokenizer_kwargs=dict(), fn_kwargs=dict()):\n    fn_kwargs['text_field'] = text_field\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, **tokenizer_kwargs)\n    return dataset.map(tokenize_data_function, fn_kwargs={'tokenizer': tokenizer, **fn_kwargs}, batched=True)\n\ndef process_tokenized_data(encoded_dataset, label_fields=['labels']):\n    for split_name in encoded_dataset:\n        encoded_dataset[split_name] = convert_labels_to_array(encoded_dataset[split_name], label_fields)\n\n    encoded_dataset = encoded_dataset.select_columns(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n    encoded_dataset.set_format(\"torch\")\n\n    return encoded_dataset\n\ndataset = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\ntokenized_dataset = tokenize_dataset(dataset, text_field='Tweet', fn_kwargs=dict(text_field='Tweet', padding=\"max_length\", truncation=True))\nout = process_tokenized_data(tokenized_dataset, label_fields=['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'])\nprint(out['validation'][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Suyash_Thapa_.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n\n\nlabels = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n\n# Map labels to integers and vice versa\nid2label = {i: label for i, label in enumerate(labels)}\nlabel2id = {label: i for i, label in enumerate(labels)}\n\nmodel_name = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(labels),\n    id2label=id2label,\n    label2id=label2id,\n    problem_type=\"multi_label_classification\"\n)\n\ntraining_args = TrainingArguments(\n    \"bert_classification\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    warmup_steps=100,\n\n)\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions > 0.5\n    return {\n        'accuracy': (preds == labels).mean(),\n        'f1': f1_score(labels, preds, average='micro'),\n        'precision': precision_score(labels, preds, average='micro'),\n        'recall': recall_score(labels, preds, average='micro')\n    }\n\n\nout['train'] = out['train'].shuffle().select(range(1000))\n# downsample the validation set to 100 samples\nout['validation'] = out['validation'].shuffle().select(range(100))\n\n\n# Initialize Trainer and train the model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=out['train'],\n    eval_dataset=out['validation'],\n    tokenizer=tokenizer\n)\n\ntrainer.train()\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score\n\nsem_eval_2018_dataset = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\n\n\ntrain_df = pd.DataFrame(sem_eval_2018_dataset['train'])\nval_df = pd.DataFrame(sem_eval_2018_dataset['validation'])\nvectorizer = CountVectorizer()\n\n\n\nlabels = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\nX_train = vectorizer.fit_transform(train_df['Tweet'])\ny_train = train_df[labels].values\n\nX_test = vectorizer.transform(val_df['Tweet'])\ny_test = val_df[labels].values\n\n\nc_fication = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = c_fication.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"accuracy of model:\")\nprint(accuracy_score(y_test, y_pred))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Suyash_Thapa_.ipynb",
        "question": "#### Here comes your discussion",
        "answer": "\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# !pip install datasets \nfrom datasets import load_dataset\n\n# Load SemEval 2018 Task 1 dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\",\"subtask5.english\")\n\n# Access the validation, train, test  split\nvalidation_set = dataset[\"validation\"]\ntrain_set = dataset[\"train\"]\ntest_set = dataset[\"test\"]\n\n# Print the last data point in the validation set\nlast_data_point = validation_set[-1]\nprint(last_data_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport numpy as np\n\n# Load the BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Get the list of labels\nlabels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n\n# Define a function to tokenize the dataset\ndef tokenize_data(dataset):\n  # tokenize the text\n  text = dataset[\"Tweet\"]\n\n  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n\n  # tokenize the labels\n  labels_batch = {k: dataset[k] for k in dataset.keys() if k in labels}\n  \n  # create numpy array of shape (batch_size, num_labels)\n  labels_matrix = np.zeros((len(text), len(labels)))\n  \n  # fill numpy array\n  for idx, label in enumerate(labels):\n    labels_matrix[:, idx] = labels_batch[label]\n\n  encoding[\"labels\"] = labels_matrix.tolist()\n  \n  return encoding\n     \n\nencoded_dataset = dataset.map(tokenize_data, batched=True, remove_columns=dataset['train'].column_names)\nencoded_dataset['validation'][-1].keys()\n#`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\n\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score\nimport torch\n\n# Create a dictionary to map the label names to label ids\nid2label = {idx:label for idx, label in enumerate(labels)}\nlabel2id = {label:idx for idx, label in enumerate(labels)}\n\n# Define the model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n                                                           problem_type=\"multi_label_classification\", \n                                                           num_labels=len(labels),\n                                                           id2label=id2label,\n                                                           label2id=label2id)\n\n\n# Define the training arguments\nargs = TrainingArguments(\n    \"bert-finetuned\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    report_to=None\n)\n\n\n    \n# Define a function to compute the metrics\ndef multi_label_metrics(predictions, labels, threshold=0.5):\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = torch.nn.Sigmoid()\n    probs = sigmoid(torch.Tensor(predictions))\n    # next, use threshold to turn them into integer predictions\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs >= threshold)] = 1\n    # finally, compute metrics\n    y_true = labels\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    recall_micro_average = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n\n    accuracy = accuracy_score(y_true, y_pred)\n    # return as dictionary\n    metrics = {'f1': f1_micro_average,\n            'recall': recall_micro_average,\n               'accuracy': accuracy}\n    return metrics\n\n# Define a function to compute the metrics\ndef compute_metrics(p):\n    preds = p.predictions[0] if isinstance(p.predictions, \n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds, \n        labels=p.label_ids)\n    return result\n\n# Define the trainer\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n# Here comes your code for the alternative approach"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Hosam_Fawzy_Mohamed_Elsafty__Rouaa_Maadanli.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Assignment_4_SonaJabrayilova_LeylaHashimli.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "import datasets\n\n# Select the HuggingFaceH4/no_robots dataset\ndataset = datasets.load_dataset(\"dair-ai/emotion\")\n\n# Print the keys of the loaded dataset\nprint(\"Keys of the dataset:\", dataset.keys())\n\n# Choose the appropriate split name for validation\nvalidation_split_name = 'validation'\n\n# Load the validation set\nvalidation_dataset = dataset[validation_split_name]\n\n# Check the last data point in the validation set\nlast_data_point = validation_dataset[-1]\nprint(f\"Last data point: {last_data_point}\")"
    },
    {
        "file_name": "Assignment_4_SonaJabrayilova_LeylaHashimli.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport torch\n\ndef tokenize_data(dataset):\n    # Extract texts and labels\n    texts = dataset[\"text\"]\n    labels = dataset[\"label\"]\n\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    tokenized_dataset = tokenizer(texts, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n\n    # Convert labels to tensors \n    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n    tokenized_dataset[\"labels\"] = labels_tensor\n\n    return tokenized_dataset\n\n\n# If you have multiple examples in your dataset, you can use map() as follows:\ntokenized_dataset = dataset.map(tokenize_data, batched=True)\n\nfor ds in tokenized_dataset:\n    tokenized_dataset[ds] = tokenized_dataset[ds].remove_columns([\"text\", \"label\"])\ntokenized_dataset.set_format(\"torch\")\ntokenized_dataset[validation_split_name][-1].keys()"
    },
    {
        "file_name": "Assignment_4_SonaJabrayilova_LeylaHashimli.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Alternative method using SVM and TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n\n# Vectorize both train and test datasets\ntf_idf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),)\nX = tf_idf_vectorizer.fit_transform(dataset['train']['text'])\nX_test = tf_idf_vectorizer.transform(dataset['test']['text'])\n\n# train and predict using SVM\nsvm_classifier = svm.LinearSVC(dual=True)\nsvm_classifier.fit(X, dataset['train']['label'])\npredictions = svm_classifier.predict(X_test)\n\n# evaluate\ntrue_values = dataset['test']['label']\nprint(f\"\"\"Model: SVM\n        Accuracy: {accuracy_score(true_values, predictions)}\n        Precision: {precision_score(true_values, predictions, average='macro')}\n        Recall: {recall_score(true_values, predictions, average='macro')}\n        F1: {f1_score(true_values, predictions, average='macro')}\"\"\")\n# BERT Approach\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nimport evaluate\nimport numpy as np\n\n# mapping\nlabel_mapping = {\"sadness\": 0, \"joy\": 1, \"love\": 2, 'anger': 3, 'fear': 4, 'surprise': 5}\nid2label = {i: label for i, label in enumerate(label_mapping)}\nlabel2id = {label: i for i, label in enumerate(label_mapping)}\n\n# initialize model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                           num_labels=6,\n                                                           id2label=id2label,\n                                                           label2id=label2id,\n                                                           problem_type=\"multi_label_classification\")\n\n# define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"emotion_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.05,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\n# define evaluation metrics\nclf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n\ndef compute_metrics(eval_predictions):\n    predictions, labels = eval_predictions\n    predictions = np.argmax(predictions, axis=1)\n    return clf_metrics.compute(predictions=predictions, references=labels)\n\n\n# define trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\ntrainer.evaluate()\n\n# I could not fix this error, therefore I could not compare two approaches\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "import datasets\n\n# We load the english subtask of the sem_eval dataset\ntweets = datasets.load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\nprint(tweets[\"validation\"][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "import torch as pt\nfrom transformers import BertTokenizer\n\n# The tokenizer is loaded from huggingface\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# This function will convert the boolean labels to a float tensor\ndef labels_as_float(dataset):\n    label_names = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n    \n    # each attribute is converted as a float tensor and is assigned as a column to labels\n    labels = pt.zeros((len(dataset[\"Tweet\"]), len(label_names)), dtype=pt.float)\n    for i, label_name in enumerate(label_names):\n        labels[:, i] = pt.tensor(dataset[label_name], dtype=pt.float)\n        \n    return labels\n\ndef tokenize_data(dataset):\n    # We avoid loading the tokenizer multiple times by using the global variable\n    global tokenizer\n    \n    # Tweets are tokenized as batch; padding and truncation attributes ensure that all resulting tensors are the same size\n    text = dataset[\"Tweet\"]\n    encoded_dataset = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    \n    # converted labels are added as attribute to the mail dataset\n    encoded_dataset[\"labels\"] = labels_as_float(dataset)\n    \n    return(encoded_dataset)\n# Tokenization is batched as row by row processing is very slow; Original attributes are dropped in the process\ntokenized_tweets = tweets.map(tokenize_data, batched=True, batch_size=1000, remove_columns=['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'])\n# We can see that the tokenized data has the right attribute\nprint(tokenized_tweets[\"validation\"][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoModelForSequenceClassification\n\n# cuda device is preferred if present, else cpu is used for training\ndevice = \"cuda:0\" if pt.cuda.is_available() else \"cpu\"\n\n# We load the bert model from huggingface and move it to the preferred device\nbert_clf = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=11,\n    problem_type=\"multi_label_classification\"\n).to(device)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# This function calculates the predicted labels of the model from output logits\ndef label_from_pred(pred):\n    # We first turn the logits into probability using the sigmoid function\n    sigmoid = pt.nn.Sigmoid()\n    probs = sigmoid(pt.Tensor(pred))\n\n    # The probabilities are quantized with a threshold of 0.5\n    y_pred = pt.zeros(probs.shape)\n    y_pred[pt.where(probs >= 0.5)] = 1\n    \n    return y_pred\n\ndef compute_metrics(predictions):\n    # the expected values can be found at label_ids attribute of the prediction output\n    # and predicted labels is calculated via label_from_pred function\n    actual_labels = predictions.label_ids\n    pred_labels = label_from_pred(predictions.predictions)\n\n    # Accuracy, F1-score, precision, recall metrics are calculated using micro averaging and are returned\n    metrics = {\n        \"accuracy\": accuracy_score(actual_labels, pred_labels),\n        \"precision\": precision_score(actual_labels, pred_labels, average=\"micro\"),\n        \"recall\": recall_score(actual_labels, pred_labels, average=\"micro\"),\n        \"f1_score\": f1_score(actual_labels, pred_labels, average=\"micro\")\n    }\n    \n    return metrics\nfrom transformers import Trainer, TrainingArguments\n\n# We will want to train the model for 5 epochs and save the chechpoint at every step\ntraining_args = TrainingArguments(\n    \"bert_model\",\n    evaluation_strategy = \"steps\",\n    save_strategy = \"steps\",\n    num_train_epochs=5,\n    load_best_model_at_end=True\n)\n\n# The trainer is constructed with the required parameters\ntrainer = Trainer(\n    bert_clf,\n    training_args,\n    train_dataset=tokenized_tweets[\"train\"],\n    eval_dataset=tokenized_tweets[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n# Training is achieved by simply running the train function\ntrainer.train()\n# We can evaluate the trained model using the metrics on the validation set\npred_bert = trainer.predict(tokenized_tweets[\"validation\"])\n\ncompute_metrics(pred_bert)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# We can vectorize the text data using Tfidf\nvectorizer = TfidfVectorizer()\n\nX_train = vectorizer.fit_transform(tweets[\"train\"][\"Tweet\"]).toarray()\n\n# The labels also must be converted to numerical values\ny_train = labels_as_float(tweets[\"train\"])\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# OneVsRest strategy is used to turn the svm into a multilabel classifier\nsvm_clf = OneVsRestClassifier(LinearSVC())\nsvm_clf.fit(X_train, y_train)\n# The performance of the svm model is evaluated on the validation set using the same metrics as the bert model\nX_valid = vectorizer.transform(tweets[\"validation\"][\"Tweet\"]).toarray()\ny_valid = labels_as_float(tweets[\"validation\"])\n\nsvm_pred = svm_clf.predict(X_valid)\nprint(\"Metrics for SVM Model:\")\nprint(\"Accuracy:\", accuracy_score(y_valid, svm_pred))\nprint(\"Recall:\", recall_score(y_valid, svm_pred, average=\"micro\"))\nprint(\"Precision:\", precision_score(y_valid, svm_pred, average=\"micro\"))\nprint(\"F1_score:\", f1_score(y_valid, svm_pred, average=\"micro\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mahan_Akbari_Moghanjoughi.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "data = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n## remove column ID as it's not a useful feature\ndata = data.remove_columns([\"ID\"])\n\nlabels = [label for label in data[\"train\"].features.keys() if label not in [\"Tweet\"]]\n\nprint(labels)\n\ndata[\"validation\"][-1]"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "model_name = \"bert-base-uncased\"\nbert_tokenizer = BertTokenizer.from_pretrained(model_name)\ndef tokenize_data(dataset):\n    tweet = dataset[\"Tweet\"]\n    encoded_dataset = bert_tokenizer(tweet, padding=\"max_length\", truncation=True)\n\n    labels_batch = {label: dataset[label] for label in dataset.keys() if label in labels}\n#     print(labels_batch)\n\n    labels_matrix = np.zeros((len(tweet), len(labels)))\n\n    for i, label in enumerate(labels):\n        labels_matrix[:, i] = labels_batch[label]\n\n#     print(labels_matrix.tolist())\n\n    encoded_dataset[\"labels\"] = labels_matrix.tolist()\n    return(encoded_dataset)\nencoded_dataset = data.map(tokenize_data, batched=True, remove_columns=data['train'].column_names)\nencoded_dataset[\"validation\"][-1].keys()\nencoded_dataset.set_format(\"torch\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\nid2label = {i:label for i, label in enumerate(labels)}\nlabel2id = {label:i for i, label in enumerate(labels)}\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name,\n                                                           problem_type=\"multi_label_classification\",\n                                                           num_labels=len(labels),\n                                                           id2label=id2label,\n                                                           label2id=label2id)\nbatch_size = 16\nmetric_name = \"f1\"\n\nargs = TrainingArguments(\n    output_dir =\"task_3\",\n    overwrite_output_dir = True,\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=metric_name,\n)\ndef multi_label_metrics(predictions, labels, threshold=0.5):\n\n    sigmoid = torch.nn.Sigmoid()\n    probs = sigmoid(torch.Tensor(predictions))\n\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs >= threshold)] = 1\n\n    y_true = labels\n    f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    p = precision(y_true, y_pred, average = 'micro')\n    r = recall(y_true, y_pred, average = 'micro')\n    accuracy = accuracy_score(y_true, y_pred)\n\n    metrics = {'f1': f1,\n               'precision': p,\n               'recall': r,\n               'accuracy': accuracy}\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n    return result\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=bert_tokenizer,\n    compute_metrics=compute_metrics\n)\ntrainer.train()\ntrainer.evaluate()\ntrainer.evaluate(encoded_dataset[\"test\"])\n# Here comes your code for the alternative approach\ndef get_predictions(model, xtest, ytest, log_string):\n    print(f\"Performance for {log_string}:\")\n    y_pred = model.predict(xtest)\n\n#     print(y_pred)\n\n    f1_micro_average = f1_score(ytest, y_pred=y_pred, average='micro')\n    p = precision(ytest, y_pred, average = 'micro')\n    r = recall(ytest, y_pred, average = 'micro')\n    accuracy = accuracy_score(ytest, y_pred)\n    # return as dictionary\n    metrics = {'f1': f1_micro_average,\n               'precision': p,\n               'recall': r,\n               'accuracy': accuracy}\n    print(metrics)\n# convert to DataFrames\n\ntrain = pd.DataFrame(data[\"train\"])\nvalid = pd.DataFrame(data[\"validation\"]) ## only for model tuning, will not be used in final submission\ntest = pd.DataFrame(data[\"test\"])\nimport re\n\ndef clean(string):\n    s = re.sub('\\W+',' ', string )\n    return(s)\n## clean the tweets\n\ntrain.Tweet = train.Tweet.apply(clean)\nvalid.Tweet = valid.Tweet.apply(clean)\ntest.Tweet = test.Tweet.apply(clean)\n\n# print(train)\n## vectorise the tweets using Tf-iDf\n\ntfidf = TfidfVectorizer()\nx_train = tfidf.fit_transform(train.Tweet)\nx_valid = tfidf.transform(valid.Tweet)\nx_test = tfidf.transform(test.Tweet)\n# print(vector_train)\n## convert labels to 0/1\n\nl_train = train[[l for l in train.columns if l not in [\"Tweet\"]]].replace({False: 0, True: 1})\nl_valid = valid[[l for l in valid.columns if l not in [\"Tweet\"]]].replace({False: 0, True: 1})\nl_test = test[[l for l in test.columns if l not in [\"Tweet\"]]].replace({False: 0, True: 1})\n\n# l_train\n## RandomForest\nrf_classifier = rfc(criterion=\"entropy\", random_state=101, n_estimators=200)\nrf_classifier.fit(x_train, l_train)\nget_predictions(rf_classifier, x_test, l_test, \"RandomForestClassifier\")\n## DecisionTree\n\ndt = dtc(criterion=\"entropy\", random_state=101)\ndt.fit(x_train, l_train)\nget_predictions(dt, x_test, l_test, \"DecisionTreeClassifier\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Kashan_U_Z_Khan.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Suraj_Giri_Duc_Manh_Vu.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# load the dataset\nsem_eval_2018_dataset = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\nprint(sem_eval_2018_dataset)\n# get the last data point of the validation set\nprint(sem_eval_2018_dataset['validation'][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Suraj_Giri_Duc_Manh_Vu.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "# define a function to tokenize a dataset\ndef tokenize_function(tokenizer):\n    def inner(examples, text_field='text', **kwargs):\n      return tokenizer(examples[text_field], **kwargs)\n    return inner\n# define a function to merge column and change to float\ndef process_labels_column(examples, label_fields):\n  # get all label columns\n  values = examples.select_columns(label_fields)\n\n  # create a column with the values above (which is a dict)\n  new_examples = examples.add_column('labels', values)\n\n  # map by only take the values and omit the keys\n  new_examples = examples.add_column('labels', list(map(lambda x: x.values(), new_examples['labels'])))\n\n  # map to float\n  new_examples = examples.add_column('labels', list(map(lambda x: np.array(x, dtype=float), new_examples['labels'])))\n\n  return new_examples.remove_columns(label_fields)\n# define the tokenize function\ndef tokenize_data(dataset, tokenizer_name='bert-base-uncased', text_field='text', label_fields=['labels',], tokenizer_kwargs=dict(), fn_kwargs=dict()):\n  fn_kwargs['text_field'] = text_field\n  tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, **tokenizer_kwargs)\n  encoded_dataset = dataset.map(tokenize_function(tokenizer), batched=True, fn_kwargs=fn_kwargs)\n\n  # map label for every subset of the dataset\n  for split in encoded_dataset:\n    encoded_dataset[split] = process_labels_column(encoded_dataset[split], label_fields)\n\n  # select the necessary columns\n  encoded_dataset = encoded_dataset.select_columns(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n  # set value to torch format\n  encoded_dataset.set_format(\"torch\")\n\n  return encoded_dataset\nout = tokenize_data(sem_eval_2018_dataset, text_field='Tweet', label_fields=['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'], fn_kwargs=dict(text_field='Tweet', padding=\"max_length\", truncation=True))\n# check if it is in torch format or not\nprint(out['train']['labels'][0:4])\n# print the key of the last datapoint of the validation set\nprint(out['validation'][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Suraj_Giri_Duc_Manh_Vu.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\n# Defining the label mapping\nlabel_list = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\nnum_labels = len(label_list)\n\n# Mapping labels to integers and vice versa\nid2label = {i: label for i, label in enumerate(label_list)}\nlabel2id = {label: i for i, label in enumerate(label_list)}\n\nprint(\"=====\")\nprint(\"ID2Label \\n\", id2label)\nprint(\"Label2ID \\n\", label2id)\nprint(\"=====\")\n\n# Loading pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,\n    problem_type=\"multi_label_classification\"\n)\n\n# Example input text\ninput_text = \"\"\"America has changed dramatically during recent years. Not only has the number of\n    graduates in traditional engineering disciplines such as mechanical, civil,\n    electrical, chemical, and aeronautical engineering declined, but in most of\n    the premier American universities engineering curricula now concentrate on\n    and encourage largely the study of engineering science. As a result, there\n    are declining offerings in engineering subjects dealing with infrastructure,\n    the environment, and related issues, and greater concentration on high\n    technology subjects, largely supporting increasingly complex scientific\n    developments. While the latter is important, it should not be at the expense\n    of more traditional engineering.\"\"\"\n# Tokenizing input text\ninputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n\n# Performing inference\noutputs = model(**inputs)\npredicted_probabilities = torch.sigmoid(outputs.logits)\n\n# Printing predicted probabilities for each label\nprint(\"The predicted probabilities for dummy input text: \")\nfor i, label in enumerate(label_list):\n    print(f'{label}: {predicted_probabilities[0][i].item()}')\n# Training the model with the HuggingFace Trainer API utilizing the TrainingArguments class and Trainer Class\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n# Defining training arguments\ntraining_args = TrainingArguments(\n    \"bert_classification\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    warmup_steps=100,\n\n)\n\n# Defining compute metrics function\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions > 0.5\n    return {\n        'accuracy': (preds == labels).mean(),\n        'f1': f1_score(labels, preds, average='micro'),\n        'precision': precision_score(labels, preds, average='micro'),\n        'recall': recall_score(labels, preds, average='micro')\n    }\n\nprint(\"=====\")\nprint(\"Original Train Dataset Sample Shape: \")\nprint(out['train'].shape)\nprint(\"Original Validation Dataset Sample Shape\")\nprint(out['validation'].shape)\nprint(\"=====\")\n\n# Initializing Trainer and train the model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=out['train'],\n    eval_dataset=out['validation'],\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n# Here comes your code for Alternative Approach\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Converting HuggingFace Dataset to pandas DataFrame\ntrain_df = pd.DataFrame(sem_eval_2018_dataset['train'])\n# print(train_df)\nval_df = pd.DataFrame(sem_eval_2018_dataset['validation'])\n\n# Extracting labels\nlabels = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n\n# Converting text data to TF-IDF vectors\nvectorizer = TfidfVectorizer()\n\n# Preparing training data\nX_train = vectorizer.fit_transform(train_df['Tweet'])\ny_train = train_df[labels].values\nprint(\"=====\")\nprint(\"First Train Labels: \")\nprint(y_train[0])\nprint(\"=====\")\n\n# Preparing validation data\nX_test = vectorizer.transform(val_df['Tweet'])\ny_test = val_df[labels].values\n\n# Training the model a multi-label SVM classifier\nclf = OneVsRestClassifier(SVC()).fit(X_train, y_train)\n\n# Evaluating the model\ny_pred = clf.predict(X_test)\n\n# Calculating metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='micro')\nrecall = recall_score(y_test, y_pred, average='micro')\nf1 = f1_score(y_test, y_pred, average='micro')\n\nprint(\"Accuracy: \", accuracy)\nprint(\"Precision: \", precision)\nprint(\"Recall: \", recall)\nprint(\"F1 Score: \", f1)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "pip install datasets\npip install numpy torch datasets transformers~=4.28.0 evaluate tqdm --quiet\npip freeze | grep -E '^numpy|^torch|^datasets|^transformers|^evaluate'\nfrom datasets import load_dataset\nimport numpy as np\nimport torch\n\n# clone dataset\n# source: https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech\nraw_datasets = load_dataset(\"ucberkeley-dlab/measuring-hate-speech\")\n\n\n# keep only text and specific targets\nraw_columns = raw_datasets['train'].column_names\nkeep_columns = ['text', 'target_race', 'target_religion', 'target_origin', 'target_gender', 'target_sexuality', 'target_age', 'target_disability']\nremove_columns = set(raw_columns)-set(keep_columns)\n\npreprocessed_datasets = raw_datasets.remove_columns(remove_columns)\ncolumn_mapping = {column:column.split('_')[1] for column in keep_columns if column.startswith('target')}\n\npreprocessed_datasets = preprocessed_datasets.rename_columns(column_mapping)\n\n# get two-way label and label id\nID2LABEL = {}\nLABEL2ID = {}\n\nlabel_id = 0\nfor label in preprocessed_datasets['train'].features.keys():\n    if label in ['text']:\n        continue\n\n    ID2LABEL[label_id] = label\n    LABEL2ID[label] = label_id\n\n    label_id += 1\n\n\ndef create_labels(batch):\n    # one-hot encode targets for training\n    batch['labels'] = [[float(batch[label][i]) for label in LABEL2ID] for i in range(len(batch['text']))]\n    return batch\n\npreprocessed_datasets = preprocessed_datasets.map(create_labels, batched=True, remove_columns=LABEL2ID.keys())\n# make the dataset smaller: 50000 data points\npreprocessed_datasets = preprocessed_datasets[\"train\"].select(range(50000))\n\n\n# set seed for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\nfrom datasets import DatasetDict\n\n# train (80%), validation (10%), test (10%) split\ntrain_test_datasets = preprocessed_datasets.train_test_split(test_size=0.2, seed=SEED)\nvalidation_test_datasets = train_test_datasets['test'].train_test_split(test_size=0.5, seed=SEED)\n\npreprocessed_datasets = DatasetDict({\n    'train': train_test_datasets['train'],\n    'validation': validation_test_datasets['train'],\n    'test': validation_test_datasets['test']\n})\n\nprint(preprocessed_datasets[\"validation\"][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\n#using bert-base-uncased for tokenization\nMODEL = 'bert-base-uncased'\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL)\n\ndef tokenize_data(dataset):\n  return TOKENIZER(dataset['text'], truncation=True)\n\n# performing apply (using map) for train, test and validation dataset\ntokenized_datasets = preprocessed_datasets.map(lambda batch:  tokenize_data(batch), batched=True, remove_columns=['text'])\n\n#setting encoded dataset's format to standard Pytorch\ntokenized_datasets.set_format('torch')\ntokenized_datasets['validation'].features.keys()"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\n\n# get data collator for data loader\ndata_collator = DataCollatorWithPadding(tokenizer=TOKENIZER)\n\n# setup dataloaders with tokenized dataset\n# to shuffle only be train for each epoch\n# in 64 batch sizes with dynamic padding\n\ndataloaders = {}\nfor dataset_type in tokenized_datasets.keys():\n    dataloaders[dataset_type] = DataLoader(\n        dataset=tokenized_datasets[dataset_type],\n        batch_size=64,\n        shuffle=(dataset_type == 'train'),\n        collate_fn=data_collator,\n    )\n# get current device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL,\n    problem_type='multi_label_classification',\n    num_labels=len(LABEL2ID),\n    label2id=LABEL2ID,\n    id2label=ID2LABEL,\n)\n\n# move model to device\nmodel.to(device)\nfrom transformers import AdamW, get_scheduler\n\n# setup optimizer and scheduler\nscheduler_name = 'linear'\noptimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0, no_deprecation_warning=True)\nnum_training_epochs = 1\nnum_training_steps = num_training_epochs * len(dataloaders['train'])\nnum_warmup_steps = 0\nlr_scheduler = get_scheduler(\n    name=scheduler_name,\n    optimizer=optimizer,\n    num_training_steps=num_training_steps,\n    num_warmup_steps=num_warmup_steps,\n)\n\nprint(f\"           SCHEDULER NAME: {scheduler_name}\")\nprint(f\"                OPTIMIZER: {optimizer.__class__.__name__}\")\nprint(f\"NUMBER OF TRAINING EPOCHS: {num_training_epochs}\")\nprint(f\" NUMBER OF TRAINING STEPS: {num_training_steps}\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, classification_report, multilabel_confusion_matrix, f1_score, precision_score, recall_score\n\n\ndef compute_classification_report(y_test, y_pred):\n  # Print classification report\n  report = classification_report(y_test, y_pred)\n  print('Classification Report:\\n', report)\n\n  # Making predictions and evaluating on the validation set\n  print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n  print(\"F1 Score:\", f1_score(y_test, y_pred, average='micro'))\n  print(\"Precision:\", precision_score(y_test, y_pred, average='micro'))\n  print(\"Recall:\", recall_score(y_test, y_pred, average='micro'))\n\n  # Compute confusion matrix for each class\n  confusion_matrices = multilabel_confusion_matrix(y_test, y_pred)\n\n  # Plot confusion matrix for each class\n  for i, confusion_matrix in enumerate(confusion_matrices):\n      plt.figure(figsize=(5, 4))\n      sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n                  xticklabels=['Predicted 0', 'Predicted 1'],\n                  yticklabels=['Actual 0', 'Actual 1'])\n      plt.title(f'Confusion Matrix for Class {i}')\n      plt.show()\n\n  return report\nfrom tqdm.notebook import tqdm\n\ndef train(model, dataloader):\n    # setup train metrics\n    loss = 0\n    train_predictions = []\n    train_labels = []\n\n    # set to train mode\n    model.train()\n    # iterate through dataloader\n    for batch in tqdm(dataloader):\n        # zero the gradients\n        optimizer.zero_grad()\n\n        # predict batch in current device\n        batch.to(device)\n        outputs = model(**batch)\n\n        # compute multilabel outputs\n        predictions = torch.nn.functional.sigmoid(outputs.logits).cpu()\n        predictions = (predictions >= 0.50).int().numpy()\n        labels = batch['labels']\n\n        # backprop and update learning rate\n        outputs.loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n        # accumulate train metrics\n        loss += outputs.loss.item()\n        train_predictions += predictions.tolist()\n        train_labels += labels.tolist()\n\n    # compute train metrics\n    loss /= len(dataloader)\n    report = compute_classification_report(np.array(train_labels), np.array(train_predictions))\n    return {\n        'loss': loss,\n        'report': report,\n    }\ndef evaluate(model, dataloader):\n    # setup evaluation metrics variables\n    loss = 0\n    evaluate_predictions = []\n    evaluate_labels = []\n\n    # set to evaluation mode\n    model.eval()\n    with torch.no_grad():\n        # iterate through dataloader\n        for batch in tqdm(dataloader):\n            # predict batch in current device\n            batch.to(device)\n            outputs = model(**batch)\n\n            # compute multilabel outputs\n            predictions = torch.nn.functional.sigmoid(outputs.logits).cpu()\n            predictions = (predictions >= 0.50).cpu().numpy()\n            labels = batch['labels']\n\n            # accumulate evaluation metrics\n            loss += outputs.loss.item()\n            evaluate_predictions += predictions.tolist()\n            evaluate_labels += labels.tolist()\n\n    # compute evaluation metrics\n    loss /= len(dataloader)\n    report = compute_classification_report(np.array(evaluate_labels), np.array(evaluate_predictions))\n\n    return {\n        'loss': loss,\n        'classification_report': report,\n    }\n# Zero-shot prompting\ntest_metrics = evaluate(model, dataloaders['test'])\nprint(f\"TEST REPORT:\\n {test_metrics['classification_report']}\")\nprint(f\"TEST LOSS: {test_metrics['loss']:.5f}\")\n# Fine-tune the model\nfor epoch in range(num_training_epochs):\n    train_metrics = train(model, dataloaders['train'])\n    validation_metrics = evaluate(model, dataloaders['validation'])\n\n    print(f\"EPOCH {epoch+1}\", end=\" | \")\n    print(f\"TRAIN LOSS: {train_metrics['loss']:.5f}\", end=\" | \")\n    print(f\"VALIDATION LOSS: {validation_metrics['loss']:.5f}\", end=\" | \")\n    print(f\"VALIDATION REPORT:\\n {validation_metrics['classification_report']}\")\n    # print(f\"VALIDATION F1: {validation_metrics['report']:.5f}\")\n# evaluate the fine-tuned model\ntest_metrics = evaluate(model, dataloaders['test'])\nprint(f\"TEST REPORT:\\n {test_metrics['classification_report']}\")\nprint(f\"TEST LOSS: {test_metrics['loss']:.5f}\")\ntest_metrics = evaluate(model, dataloaders['validation'])\nprint(f\"VALIDATION REPORT:\\n {test_metrics['classification_report']}\")\nprint(f\"VALIDATION LOSS: {test_metrics['loss']:.5f}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_ZainUlAbedin_ShahzebQamar.ipynb",
        "question": "#### Alternative Approach (TF-IDF and SVM)",
        "answer": "# Importing necessary libraries and modules\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Extracting texts and labels for the training dataset\ntrain_texts = preprocessed_datasets[\"train\"]['text']\ntrain_labels = preprocessed_datasets[\"train\"]['labels']\n\n# Extracting texts and labels for the test dataset\ntest_texts = preprocessed_datasets[\"test\"]['text']\ntest_labels = preprocessed_datasets[\"test\"]['labels']\n\n# Extracting texts and labels for the validation dataset\nval_texts = preprocessed_datasets[\"validation\"]['text']\nval_labels = preprocessed_datasets[\"validation\"]['labels']\n\n\n# TF-IDF Vectorization\n# Using TF-IDF to convert texts into numerical vectors\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)\nX_tfidf = tfidf_vectorizer.fit_transform(train_texts)\nX_test_tfidf = tfidf_vectorizer.transform(test_texts)\nX_val_tfidf = tfidf_vectorizer.transform(val_texts)\n\n# Training a Support Vector Machine (SVM) model\n# Using OneVsRestClassifier for multi-label classification and linear kernel for SVM\nsvm_model = OneVsRestClassifier(SVC(kernel='linear'))\nsvm_model.fit(X_tfidf, train_labels)\n\n# Making predictions and evaluating on the test set\npredictions_test = svm_model.predict(X_test_tfidf)\nprint(\"\\nMetrics for Test Set:\")\nprint(\"Accuracy:\", accuracy_score(test_labels, predictions_test))\nprint(\"F1 Score:\", f1_score(test_labels, predictions_test, average='micro'))\nprint(\"Precision:\", precision_score(test_labels, predictions_test, average='micro'))\nprint(\"Recall:\", recall_score(test_labels, predictions_test, average='micro'))\n\n# Making predictions and evaluating on the validation set\npredictions_val = svm_model.predict(X_val_tfidf)\n\n#compute validation metrics\nccr = compute_classification_report(val_labels,predictions_val)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Team Members:\n1. Shruti Nair s76snair\n2. Ahmad Javed s63ajave",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# Here comes your code\nfrom datasets import load_dataset\nfrom rich import print   #used to print the values in a better format and more readable manner\n\n\n# a) Load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n# b) Print the last data point in the validation set\nlast_validation_point = dataset[\"validation\"][-1]\nprint(last_validation_point)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "import torch\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ndef tokenize_data(example):\n    # Extract relevant information\n    tweet_text = example[\"Tweet\"]\n    labels = [example[label] for label in example if label != \"Tweet\" and label != \"ID\"]\n\n    # Tokenize the text using BERT tokenizer\n    tokenized_data = tokenizer(tweet_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n\n    # Convert labels to tensor\n    label_tensor = torch.tensor(labels, dtype=torch.float)\n\n    # Add labels to the tokenized data\n    tokenized_data[\"labels\"] = label_tensor\n\n    # Filter out unwanted keys\n    tokenized_data = {key: tokenized_data[key].squeeze() for key in ['input_ids', 'token_type_ids', 'attention_mask', 'labels']}\n\n    return tokenized_data\n\n# Apply the tokenize_data function to the dataset using map\nencoded_dataset = dataset.map(tokenize_data, remove_columns=dataset['train'].column_names)\n\n# Set the format to PyTorch tensors\nencoded_dataset.set_format(\"torch\")\n\n# Print the keys of the last data point in the validation set\nlast_validation_point = encoded_dataset[\"validation\"][-1]\nprint(last_validation_point.keys())\n# dataset is sampled due to CPU limitations and computing resources\n\ntrain_dataset_sampled =  encoded_dataset[\"train\"].shuffle(seed=42).select(range(2000))\neval_dataset_sampled = encoded_dataset[\"validation\"].shuffle(seed=42).select(range(500))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import precision_recall_fscore_support\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device used: \", device)\nmodel = model.to(device)\n\n# Define label mapping dictionaries\nlabel_mapping = {\n    \"anger\": 0,\n    \"anticipation\": 1,\n    \"disgust\": 2,\n    \"fear\": 3,\n    \"joy\": 4,\n    \"love\": 5,\n    \"optimism\": 6,\n    \"pessimism\": 7,\n    \"sadness\": 8,\n    \"surprise\": 9,\n    \"trust\": 10,\n}\n\nid2label = {i: label for i, label in enumerate(label_mapping)}\nlabel2id = {label: i for i, label in enumerate(label_mapping)}\n\n# Define the text classification model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=len(label_mapping),\n    id2label=id2label,\n    label2id=label2id,\n    problem_type=\"multi_label_classification\"\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_classification\",\n    per_device_train_batch_size=4,\n    evaluation_strategy=\"steps\",\n    eval_steps=200,\n    save_total_limit=2,\n    save_steps=500,\n    num_train_epochs=2,\n    logging_dir=\"./logs\"\n)\n\n# Define custom compute_metrics function for precision, recall, and F1\ndef compute_metrics(p):\n    predictions, labels = p.predictions, p.label_ids\n    predictions = torch.sigmoid(torch.tensor(predictions)).numpy()\n    threshold = 0.5  # Adjust threshold as needed\n    predictions[predictions >= threshold] = 1\n    predictions[predictions < threshold] = 0\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n\n    #Compute accuracy\n    correct_predictions = (predictions == labels).all(axis=1).sum()\n    total_samples = labels.shape[0]\n    accuracy = correct_predictions / total_samples\n\n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_sampled, #encoded_dataset[\"train\"],\n    eval_dataset=eval_dataset_sampled, #encoded_dataset[\"validation\"],\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n# Here comes your code for the alternative approach\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.svm import LinearSVC\n\n# Load the dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\n# Assuming your dataset has 'Tweet' and the emotion labels\ntexts = dataset['train']['Tweet']\nlabels = dataset['train'].select_columns(['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism',\n                                          'pessimism', 'sadness', 'surprise', 'trust']).to_pandas().values\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Tokenization\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)  # X_train is the training text data\nX_val_tfidf = tfidf_vectorizer.transform(val_texts)\n\n# Create the SVM\nsvm = LinearSVC(random_state=42)\n\n# Make it an Multilabel classifier\nmultilabel_classifier = MultiOutputClassifier(svm, n_jobs=-1)\n\n# Fit the data to the Multilabel classifier\n# whole dataset is used for training\n\nmultilabel_classifier = multilabel_classifier.fit(X_train_tfidf, train_labels)\n\n# Get predictions for test data\ny_test_pred = multilabel_classifier.predict(X_val_tfidf)\n# Model metrics\n\naccuracy = accuracy_score(val_labels, y_test_pred)  # val_labels is the validation labels\nf1 = f1_score(val_labels, y_test_pred, average='weighted')\nprecision = precision_score(val_labels, y_test_pred, average='weighted')\nrecall = recall_score(val_labels, y_test_pred, average='weighted')\n\n# Print the evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"F1-score:\", f1)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "#### BERT-based Classification:\n**Strengths:**\n**1. Contextual Understanding**: BERT captures context and semantics, providing a good understanding of language.\n**2. Transfer Learning**: Pre-trained BERT models can be fine-tuned on specific tasks, using knowledge learned from a vast amount of pre-training data and datasets, which often leads to superior performance.\n**Weaknesses:**\n**1. Computational Intensity**: Training and using BERT models demand significant computational resources, making it impractical for resource-constrained environments.\n**2. Large Model Size**: BERT models have a large number of parameters, leading to substantial storage requirements and making deployment challenging in some cases.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Shruti_Nair_Ahmad_Javed.ipynb",
        "question": "#### TF-IDF with SVM:\n**Strengths:**\n**1. Simplicity and Efficiency**: TF-IDF with SVM is straightforward to implement and computationally efficient, making it suitable for quick deployment in scenarios with limited resources.\n**2. Scalability**: TF-IDF can scale efficiently to large datasets, and SVM is known for its scalability, making this approach suitable for handling substantial amounts of text data.\n**Weaknesses:**\n**1. Lack of Contextual Understanding**: TF-IDF treats each document as a bag of words, ignoring contextual relationships between words, which can limit its performance on tasks requiring deep understanding.\n**2. Word Ambiguity**: TF-IDF struggles with words that have multiple meanings, treating them uniformly without considering their context.\n**3. Feature Engineering Dependency**: TF-IDF heavily relies on feature engineering, and the choice of features can significantly impact model performance, requiring careful consideration.\n**Conclusion**\nBERT excels in tasks demanding contextual understanding, while TF-IDF with SVM remains a practical choice for simpler tasks and resource-efficient applications.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_SimonWelz_JonBreid.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n\ndataset = load_dataset('sem_eval_2018_task_1', 'subtask5.english')"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_SimonWelz_JonBreid.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "import torch\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\ndef tokenize_data(dataset):\n     # Tokenize the text\n    #max_length = max(len(tokenizer.encode(tweet)) for tweet in dataset[\"Tweet\"])\n    tokenized_inputs = tokenizer(dataset[\"Tweet\"], \n                                 padding='max_length', \n                                 truncation=True, \n                                 max_length=71)\n\n    # Convert labels to a tensor of floats\n    labels = [dataset[label] for label in [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"]]\n    labels_tensor = torch.FloatTensor(labels)\n    tokenized_inputs[\"labels\"] = labels_tensor\n    return(tokenized_inputs)\nencoded_dataset = dataset.map(tokenize_data)\nencoded_dataset.set_format(\"torch\")\nlast_encoded_data_point_validation = encoded_dataset[\"validation\"][-1]\nprint(last_encoded_data_point_validation.keys())\nmax_length = max(len(tokenizer.encode(tweet)) for tweet in dataset[\"train\"][\"Tweet\"])\nprint(max_length)\nmax_length = max(len(tokenizer.encode(tweet)) for tweet in dataset[\"validation\"][\"Tweet\"])\nprint(max_length)\nmax_length = max(len(tokenizer.encode(tweet)) for tweet in dataset[\"test\"][\"Tweet\"])\nprint(max_length)\nfor i, data in enumerate(encoded_dataset[\"train\"]):\n    #print(f\"Data point {i}:\")\n    for key in data:\n        if isinstance(data[key], torch.Tensor):\n            if(data[key].shape != torch.Size([3]) and data[key].shape != torch.Size([11]) and data[key].shape != torch.Size([])):   \n                print(f\"  {key}: {data[key].shape}\")\n                print(f\" {key}: {data[key]}\")\n                print(f\"Data point {i}:\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_SimonWelz_JonBreid.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoModelForSequenceClassification\nlabels = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"]\nlabel2id = {label: i for i, label in enumerate(labels)}\nid2label = {i: label for i, label in enumerate(labels)}\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=len(labels),\n    problem_type=\"multi_label_classification\",\n    id2label=id2label,\n    label2id=label2id\n)\nmodel.to(device)\nimport transformers\nimport accelerate\nprint(transformers.__version__)\nprint(accelerate.__version__)\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # Output directory\n    num_train_epochs=5,              # Total number of training epochs\n    per_device_train_batch_size=8,   # Batch size per device during training\n    per_device_eval_batch_size=16,   # Batch size for evaluation\n    warmup_steps=1000,                # Number of warmup steps for learning rate scheduler\n    weight_decay=0.02,               # Strength of weight decay\n    logging_dir='./logs',            # Directory for storing logs\n)\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\ndef compute_metrics(p):\n    # Convert predictions to 0 or 1 based on a threshold (e.g., 0.5)\n    preds = np.where(p.predictions >= 0.5, 1, 0)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(p.label_ids, preds)\n    f1 = f1_score(p.label_ids, preds, average='samples')  # 'samples' for multi-label\n    precision = precision_score(p.label_ids, preds, average='samples')\n    recall = recall_score(p.label_ids, preds, average='samples')\n\n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,                         \n    args=training_args,                  # TrainingArguments we defined earlier\n    train_dataset=encoded_dataset[\"train\"],         # Training dataset\n    eval_dataset=encoded_dataset[\"validation\"],     # Evaluation dataset\n    compute_metrics=compute_metrics,     # Function to compute metrics\n)\ntrainer.train()\ntrainer.evaluate()\ntrain_tweets = [d['Tweet'] for d in encoded_dataset[\"train\"]]\ntest_tweets = [d['Tweet'] for d in encoded_dataset[\"test\"]]\n\nemotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\ntrain_labels = [[d[emotion].item() for emotion in emotions] for d in encoded_dataset[\"train\"]]\ntest_labels = [[d[emotion].item() for emotion in emotions] for d in encoded_dataset[\"test\"]]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=5000)\nX_train = vectorizer.fit_transform(train_tweets)\nX_test = vectorizer.transform(test_tweets)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfinal_model = OneVsRestClassifier(LogisticRegression(C=10, penalty='l2', solver='liblinear'))\nfinal_model.fit(X_train, train_labels)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\ndef evaluate_model(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n\n    # Calculating metrics for each label\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='micro')\n    recall = recall_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n\n    print(\"Accuracy:\", accuracy)\n    print(\"Precision:\", precision)\n    print(\"Recall:\", recall)\n    print(\"F1 Score:\", f1)\n\n    # Detailed report\n    print(\"\\nClassification Report:\\n\")\n    print(classification_report(y_test, y_pred, target_names=emotions))\nprint(\"Logistic Regression Model Evaluation\")\nevaluate_model(final_model, X_test, test_labels)"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_SimonWelz_JonBreid.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_SimonWelz_JonBreid.ipynb",
        "question": "### Bert Model\n#### Strength:\n- Model shows higher F1, precision and recall scores compared to LR. This indicates better overall perfomance in correctly identifying and classifying different emotions\n- BERT Transformer architecture allows it to capture complex word relationships and context, which is likely contributing to its higher precision and recall\n- BERT inherently understands language, nuances, reducing the need to extensive data preprocessing and feature engineering\n#### Weakness:\n- BERT model arre resource intensive in terms of both training and inference time. \n- The architecture of BERT is complex, which can make it harder to finetune and optimize for sprecific tasks.\n- Despite higher precision and recall the overall accuracy is relatively low. This might indicate difficulties in handling ceratin classes or an imbalance in the dataset.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_SimonWelz_JonBreid.ipynb",
        "question": "### Logistic Regression\n#### Strength \n- LR is a simpler and faster model compared to BERT. Its easier to understand, implement and requires less computational power\n- The model shows good precision and recall on some emotion classes like anger, joy and fear. This indicates its effictiveness in certain contexts\n#### Weakness\n- The model has lower F1, precsion and recall compared to BERT sugesting it is less effective at capturing the complexities of the text\n- LR perfomance heavily relies on the quality of feature extraction. It lacks the inherent understanding of language context that BERT possesses.\n- The model might not perform well on imbalanced datasets or with minority classes, as indicated by lower macro and weighted averages in the classification report.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\nimport pandas as pd\n# loading the dataset using the datasets library\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\n# check the last element of the validation set\nprint(dataset['validation'][-1])\n\n# creating the dataframes of the train, test and validation sets\ndf_train = pd.DataFrame(dataset['train'])\ndf_test = pd.DataFrame(dataset['test'])\ndf_validation = pd.DataFrame(dataset['validation'])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer, BertModel\nimport numpy as np\n\n# loading the tokenizer from the transformers library\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# extrating labels from the dataset\nlabels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n\n# tokenizing function that will be applied to the dataset with the map method\ndef tokenize_function(examples):\n    # tokenizing the text\n    tokenized_text = tokenizer(examples['Tweet'], padding=True, truncation=True)\n\n    # creating the labels matrix with shape of (batch_size, num_labels)\n    labels_batch = {l: examples[l] for l in examples.keys() if l in labels}\n    labels_matrix = np.zeros((len(examples['Tweet']), len(labels)))\n    for idx, label in enumerate(labels):\n        labels_matrix[:, idx] = labels_batch[label]\n\n    # adding the labels matrix to the tokenized text\n    tokenized_text['labels'] = labels_matrix\n    return tokenized_text\n\n# tokenizing the dataset using map method and tokenize_function\ndef tokenize_data(dataset):\n    return dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names)\n\ntokenized_datasets = tokenize_data(dataset)\n\n# printing the keys of the last element of the tokenized validation set\nprint(tokenized_datasets['validation'][-1].keys())\n\n# converting the tokenized datasets to torch tensors\ntokenized_datasets.set_format('torch')"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nfrom transformers import EvalPrediction\nimport torch\n\n# converting ids to labels and vice versa\nid_to_label = {idx:label for idx, label in enumerate(labels)}\nlabel_to_id = {label:idx for idx, label in enumerate(labels)}\n\n# setting the training arguments\ntraining_args = TrainingArguments(\n    \"multi-label-classification\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n    push_to_hub=False,\n)\n\n# loading the model (bart-base-uncased) for multi-label classification task\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=len(labels),id2label=id_to_label, label2id=label_to_id)\n\n    \ndef multi_label_metrics(predictions, labels, threshold=0.5):\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = torch.nn.Sigmoid()\n    probs = sigmoid(torch.Tensor(predictions))\n    # next, use threshold to turn them into integer predictions\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs >= threshold)] = 1\n    # finally, compute metrics\n    y_true = labels\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average = 'micro')\n    recall = recall_score(y_true, y_pred, average = 'micro')\n    # return as dictionary\n    metrics = {'f1': f1_micro_average,\n               'percision': precision,\n               'accuracy': accuracy,\n               'recall': recall}\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, \n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds, \n        labels=p.label_ids)\n    return result\n\n# setting the trainer using the model, training arguments and compute_metrics function\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\ntrainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n# Here comes your code for the alternative approach\n\n\"\"\"\n    For the alternative approach, we will use the TF-IDF vectorizer to vectorize the tweets and then use a logistic regression model to classify the tweets.\n\"\"\"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# tokenizing the tweets using the TF-IDF vectorizer\nvectorizer = TfidfVectorizer(max_features=2500, max_df=0.9)\nvectorizer.fit(df_train['Tweet'])\nX_train_tfidf = vectorizer.transform(df_train['Tweet'])\nX_validation_tfidf = vectorizer.transform(df_validation['Tweet'])\ny_train = df_train[labels]\ny_validation = df_validation[labels]\n\nclf = MultiOutputClassifier(LogisticRegression()).fit(X_train_tfidf, y_train)\npredictions = clf.predict(X_validation_tfidf)\nprint(classification_report(y_validation, predictions))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Assignment_4_Ilhom_Khalimov.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\ndisplay(dataset[\"validation\"][-1])"
    },
    {
        "file_name": "Assignment_4_Ilhom_Khalimov.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "import transformers\nimport torch\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ndef tokenize_data(dataset):\n    return dataset.map(lambda x: tokenizer(x[\"Tweet\"], truncation=True), batched=True)\n\nencoded = tokenize_data(dataset)\nencoded.set_format(\"torch\")\ndisplay(encoded[\"validation\"][-1].keys())\n\n# For a.2, the task asked to give labels as numbers to the model\n# but there was no model that was asked for in the task, so I'm not sure\n# where they were supposed to go\n\n# But anyway below is the code that would be there if there was a model\n# that consumed the labels as numbers\n\n# Extract dynamically\n# labels = dataset[\"validation\"][2:]\n\n# Or manually\nlabels = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n\nlabel2id = {label:i for i, label in enumerate(labels)}\nid2label = {i:label for i, label in enumerate(labels)}\n\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", \n    problem_type=\"multi_label_classification\", \n    id2label=id2label,\n    label2id=label2id\n)"
    },
    {
        "file_name": "Assignment_4_Ilhom_Khalimov.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\n# Here comes your code for the alternative approach"
    },
    {
        "file_name": "Assignment_4_Ilhom_Khalimov.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install datasets\n# Part A\nfrom datasets import load_dataset\n\n## Load the sem_eval_2018_task_1 dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n\ndataset\n# Part B\n## Print the last data point of the validation set\ndataset['validation'][-1]"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "!pip install torch\n! pip install -U transformers\nlabel_list = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\nfrom transformers import BertTokenizer\nfrom torch import FloatTensor\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_data(dataset):\n    encoded_dataset = tokenizer(dataset['Tweet'], padding='max_length', truncation=True)\n\n    labels = [dataset[label] for label in label_list]\n    labels = FloatTensor(labels).T\n\n    return {**encoded_dataset, 'labels': labels}\n\nencoded_dataset = {}\n\nfor split in dataset.keys():\n    encoded_dataset[split] = dataset[split].map(tokenize_data, remove_columns=dataset[split].column_names)\nencoded_dataset[\"validation\"][-1].keys()"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "#Runtime restarting required after installation\n! pip install -U accelerate\n# Bert Approach\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n## Define a text classification model with pre-trained BERT base\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_list))\n\n## Create label mapping dictionaries\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for i, label in enumerate(label_list)}\n\n## Set problem_type to \"multi_label_classification\"\nproblem_type = \"multi_label_classification\"\n\n## Train the BERT-based model using HuggingFace's Trainer API\n## Create a function to compute metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions > 0.6\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1_micro': f1,\n        'precision_micro': precision,\n        'recall_micro': recall,\n    }\n\n\n## Set up TrainingArguments and Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    save_steps=1000,\n    save_total_limit=2,\n    learning_rate=2e-5,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    logging_dir=\"./logs\",\n    logging_steps=200,\n    do_train=True,\n    do_eval=True,\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset['validation'],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)\n\n## Train the model\ntrainer.train()\n\n## Evaluate the model\nresults = trainer.evaluate()\nresults\nimport pandas as pd\n\n# Convert to pandas DataFrame\ndf = {}\nfor split in dataset.keys():\n    df[split] = pd.DataFrame(dataset[split])\n## Get multi labels from the dataframes\ntrian_labels = df['train'].iloc[:,-len(label_list):]\ntest_labels = df['test'].iloc[:,-len(label_list):]\n# Alternate (TF-IDF for tokenization and SVM for classification)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n\nX_train_tfidf = tfidf_vectorizer.fit_transform(df['train']['Tweet'])\nX_test_tfidf = tfidf_vectorizer.transform(df['test']['Tweet'])\n\n# Multi-label SVM Classifier\nsvm_classifier = MultiOutputClassifier(SVC(kernel='linear', C=1.0), n_jobs=-1)\nsvm_classifier.fit(X_train_tfidf, trian_labels)\n\n# Predictions on the validation set\npredictions = svm_classifier.predict(X_test_tfidf)\n\n# Evaluate the model\naccuracy = accuracy_score(test_labels, predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, predictions, average='micro')\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1 Score: {f1}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Muhammad_Ibrahim_Afsar_Khan.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Yagmur_Caglar.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install datasets\n!pip install transformers\n!pip install torch\n!pip install accelerate\n# Here comes your code\nfrom datasets import load_dataset\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\nprint(dataset['validation'][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Yagmur_Caglar.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "labels_in_dataset = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\ncolumns_in_dataset = ['ID', 'Tweet'] + labels_in_dataset\nfrom transformers import BertTokenizer\nfrom torch.utils.data import Dataset\nimport torch\n\ndef tokenize_data(dataset):\n    # here comes your code\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def create_tokenized_dataset(data):\n      max_length = 128\n      inputs = tokenizer(data['Tweet'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=max_length)\n      #list of 0, 1 with respect to the labels\n      labels = [float(data[label]) for label in labels_in_dataset]\n      labels += [0.0] * (max_length - len(labels))  #padding\n      inputs['labels'] = torch.tensor(labels, dtype=torch.float).unsqueeze(0)\n      return inputs\n\n    def create_tokenized_dataset_batched(batch):\n      max_length=128\n      inputs = tokenizer(batch['Tweet'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=max_length)\n      inputs['labels'] = torch.stack([torch.tensor([float(item) for item in batch[label]]) for label in labels_in_dataset]).t()\n      return inputs\n\n    #Huggingface: The primary purpose of map() is to speed up processing functions.\n    #It allows you to apply a processing function to each example in a dataset\n    #encoded_dataset = dataset.map(create_tokenized_dataset, remove_columns=columns_in_dataset)\n    encoded_dataset = dataset.map(create_tokenized_dataset_batched, remove_columns=columns_in_dataset, batched=True)\n    encoded_dataset.set_format(\"torch\")\n    return encoded_dataset\nencoded_dataset = tokenize_data(dataset)\nencoded_dataset['validation'][-1].keys()\nencoded_dataset['validation'][-1]"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Yagmur_Caglar.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport torch\n\ndef compute_metrics(p):\n    predictions, labels = p.predictions, p.label_ids\n    predictions = torch.sigmoid(torch.tensor(predictions)).numpy()\n    return {\n        'accuracy': accuracy_score(labels, (predictions > 0.5).astype(int)),\n        'precision': precision_score(labels, (predictions > 0.5).astype(int), average='micro'),\n        'recall': recall_score(labels, (predictions > 0.5).astype(int), average='micro'),\n        'f1': f1_score(labels, (predictions > 0.5).astype(int), average='micro'),\n    }\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n!nvidia-smi\n# Here comes your code for BERT Approach\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n\n#create label2id and id2label dictionaries\nlabel2id = {label: i for i, label in enumerate(labels_in_dataset)}\nid2label = {i: label for label, i in label2id.items()}\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels_in_dataset), id2label=id2label, label2id=label2id, problem_type=\"multi_label_classification\")\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntraining_args = TrainingArguments(\n    num_train_epochs=4,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=False,\n    load_best_model_at_end=True,\n    output_dir=\"/content/bert_output\",\n    logging_dir=\"/content/logs\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset['validation'],\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\nresult = trainer.evaluate()\nfor item in result:\n  print(f\"{item}: {result[item]}\")\n# Here comes your code for the alternative approach\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multioutput import MultiOutputClassifier\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n#TF-IDF vectorizer and SVC classifier\nvectorizer = TfidfVectorizer(max_features=5000)\nsvm = LinearSVC(random_state=42)\nmultilabel_classifier = MultiOutputClassifier(svm, n_jobs=-1)\ntfidf_svm_model = make_pipeline(vectorizer, multilabel_classifier)\n\n#fit the model on the training data and predict\ntfidf_svm_model.fit(dataset['train']['Tweet'], encoded_dataset['train']['labels'])\npredictions = tfidf_svm_model.predict(dataset['validation']['Tweet'])\naccuracy = accuracy_score(encoded_dataset['validation']['labels'], predictions)\nprecision = precision_score(encoded_dataset['validation']['labels'], predictions, average='micro')\nrecall = recall_score(encoded_dataset['validation']['labels'], predictions, average='micro')\nf1 = f1_score(encoded_dataset['validation']['labels'], predictions, average='micro')\n\nprint(\"TF-IDF + SVM Model Metrics:\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1 Score: {f1}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Yagmur_Caglar.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Maria_Artemyeva.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\nprint(dataset[\"validation\"][-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Maria_Artemyeva.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from datasets import load_dataset\nfrom transformers import BertTokenizer\n\n#tokenize text\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ndef tokenize_data(example):\n    return tokenizer(example['Tweet'], padding='max_length', truncation=True)   \nencoded_dataset = dataset.map(tokenize_data, batched=True)\n#get labels for model\nlabel_cols = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness',\n                 'surprise', 'trust']\nfor el in encoded_dataset.keys():\n    labels = []\n    for e in encoded_dataset[el]:\n        label = [float(e[col]) for col in label_cols]\n        labels.append(label)\n\n    label_list = [label for label in labels]\n    encoded_dataset[el] = encoded_dataset[el].remove_columns(label_cols)\n    encoded_dataset[el] = encoded_dataset[el].add_column('labels', label_list)\n#set format\nencoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\nprint(encoded_dataset['validation'][-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Maria_Artemyeva.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import BertTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_cols),\n                                                           id2label={i: label for i, label in enumerate(label_cols)},\n                                                           label2id={label: i for i, label in enumerate(label_cols)},\n                                                           problem_type=\"multi_label_classification\")\ntraining_args = TrainingArguments(\n    output_dir=\"my_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,)\n#evaluate\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions > 0.5\n    acc = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n#train\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset['train'],\n    eval_dataset=encoded_dataset['validation'],\n    compute_metrics=compute_metrics)\ntrainer.train()\neval_results = trainer.evaluate()\n#tf-idf+regression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\n\n#get tweets and labels for training and validation\ntrain_tweets = dataset['train']['Tweet']\ntrain_labels = np.array(encoded_dataset['train']['labels']) \nval_tweets = dataset['validation']['Tweet']\nval_labels = np.array(encoded_dataset['validation']['labels'])\n#tf-idf\nvectorizer = TfidfVectorizer()\ntrain_tfidf = vectorizer.fit_transform(train_tweets)\nval_tfidf = vectorizer.transform(val_tweets)\n#get labels\ntrain_ls = np.array([np.argmax(label) for label in train_labels])\nval_ls = np.array([np.argmax(label) for label in val_labels])\n#train\nmodel =  LogisticRegression(max_iter=1000)\nmodel.fit(train_tfidf, train_ls)\n#predict\npredictions = model.predict(val_tfidf)\n#evaluate\naccuracy = accuracy_score(val_ls, predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(val_ls, predictions, average='weighted',zero_division=0)\nprint(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Maria_Artemyeva.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Assignment_4_Bondarenko_Nikita.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\n# load dataset\ndataset = load_dataset(\"civility-lab/incivility-arizona-daily-star-comments\")\n\n#get last data point and print it\nlast_data_point = dataset['validation'][-1]\nprint(last_data_point)"
    },
    {
        "file_name": "Assignment_4_Bondarenko_Nikita.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nimport torch\n\ndef tokenize_data(examples):\n    \n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    # tokenize batches of text\n    encoded_batch = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n    # identify the label columns (exclude 'text' and '__index_level_0__')\n    label_columns = [col for col in examples if col not in ['text', '__index_level_0__']]\n\n    # combine all label fields into a single tensor for each example in the batch\n    labels = [[example for example in examples[col]] for col in label_columns]\n    labels_tensor = torch.tensor(labels, dtype=torch.float).T  # transpose to match the batch size\n\n    # Add the labels to the encoded batch\n    encoded_batch['labels'] = labels_tensor\n\n    return encoded_batch\nencoded_dataset = dataset.map(tokenize_data, batched=True)\n\nencoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n#print the keys of the last data point in the validation set\nvalidation_set = encoded_dataset['validation']\nlast_data_point = validation_set[-1]\nprint(last_data_point.keys())"
    },
    {
        "file_name": "Assignment_4_Bondarenko_Nikita.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nnum_labels = 10\n\n# label mappings\nlabel2id = {f'label_{i}': i for i in range(num_labels)}\nid2label = {i: f'label_{i}' for i in range(num_labels)}\n\n# load the pre-trained BERT model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=num_labels,\n    problem_type=\"multi_label_classification\",\n    id2label=id2label,\n    label2id=label2id\n)\n\n# define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,              # number of training epochs\n    per_device_train_batch_size=8,   \n    per_device_eval_batch_size=16,   \n    warmup_steps=500,              \n    weight_decay=0.01,               \n    logging_dir='./logs',            \n    logging_steps=10,\n)\n\n# define the compute_metrics function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    \n    f1 = f1_score(labels, predictions, average='weighted')\n    precision = precision_score(labels, predictions, average='weighted')\n    recall = recall_score(labels, predictions, average='weighted')\n    \n    return {\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n"
    },
    {
        "file_name": "Assignment_4_Bondarenko_Nikita.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Assignment_4_Niklas_Dobberstein.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "from datasets import load_dataset\ndataset_name = 'poem_sentiment'\n\n'''\nid: index of the example\nverse_text: The text of the poem verse\nlabel: The sentiment label. Here\n0 = negative\n1 = positive\n2 = no impact\n3 = mixed (both negative and positive)\n'''\ndataset = load_dataset(dataset_name)\n\nvalidation_set = dataset['validation']\nlast_data_point = validation_set[-1]\nprint(last_data_point)"
    },
    {
        "file_name": "Assignment_4_Niklas_Dobberstein.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nnum_labels = 4\n\ndef tokenize_data(batch):\n    text_tokens = tokenizer(batch['verse_text'], padding=True, truncation=True, return_tensors='pt')\n    \n    labels_tensor = torch.zeros((len(batch['label']), num_labels), dtype=torch.float32)\n    for i, label in enumerate(batch['label']):\n        # print(i,label,labels_tensor.shape )\n        labels_tensor[i, label] = 1.0\n    \n    text_tokens['labels'] = labels_tensor\n    \n    return text_tokens\n\nencoded_dataset = dataset.map(tokenize_data, batched=True)\nprint(encoded_dataset)\nencoded_dataset.set_format(\"torch\")\n\nvalidation_set = encoded_dataset['validation']\nlast_data_point = validation_set[-1]\nlast_data_point\n# for dsetType in [\"train\", \"validation\", \"test\"]:\n#     dset = encoded_dataset[dsetType]\n#     # Pad all input_ids to the same length in the dataset to te max length\n#     max_len = max([len(input_id) for input_id in dset[\"input_ids\"] ])\n#     encoded_dataset[dsetType] = dset.map(lambda e: pad_mapping(e,max_len))# = [ for input_id in dset[\"input_ids\"] ]"
    },
    {
        "file_name": "Assignment_4_Niklas_Dobberstein.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# !pip install scikit-learn\n# !pip install \"transformers[torch]\"\n# !pip install bert_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_metric\nfrom torch.utils.data import DataLoader\nimport numpy as np\nmodel_bert = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4,problem_type=\"multi_label_classification\")\nmetric = load_metric(\"bertscore\")\n\nlabel_mapping = {0: \"negative\", 1: \"positive\", 2: \"no impact\", 3: \"mixed\"}\nid2label = {i: label_mapping[i] for i in range(4)}\nlabel2id = {v: k for k, v in id2label.items()}\n\nmodel_bert.config.id2label = id2label\nmodel_bert.config.label2id = label2id\n\ndef compute_metrics_bert(p):\n    # print(p.predictions)\n    predictions = np.argmax(np.array(p.predictions), axis=1)\n    labels = np.argmax(p.label_ids,axis=1)\n    # print(predictions.shape, p.label_ids, p.label_ids.shape)\n    \n    return {\n        \"accuracy\": accuracy_score(labels, predictions),\n        \"f1\": f1_score(labels, predictions,average='micro'),\n        \"precision\": precision_score(labels, predictions,average='micro'),\n        \"recall\": recall_score(labels, predictions,average='micro'),\n    } \n\ntraining_args_bert = TrainingArguments(\n    per_device_train_batch_size=4,\n    output_dir=\"./bert_model\",\n    evaluation_strategy=\"epoch\",\n    save_total_limit=2,\n    learning_rate=2e-5,\n    num_train_epochs=3\n)\n\ntrainer_bert = Trainer(\n    model=model_bert,\n    args=training_args_bert,\n    compute_metrics=compute_metrics_bert,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n)\n\ntrainer_bert.train()\nresults_bert = trainer_bert.evaluate(encoded_dataset[\"validation\"])\nprint(results_bert)\nprint(\"BERT:\")\nprint(f\"Accuracy: {results_bert['eval_accuracy']:.4f}\")\nprint(f\"F1-Score: {results_bert['eval_f1']:.4f}\")\nprint(f\"Precision: {results_bert['eval_precision']:.4f}\")\nprint(f\"Recall: {results_bert['eval_recall']:.4f}\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(dataset['train']['verse_text'])\nX_val_tfidf = vectorizer.transform(dataset['validation']['verse_text'])\n\nmodel_alternative = svm.SVC(kernel=\"rbf\", C = 1e3)\nmodel_alternative.fit(X_train_tfidf, dataset['train']['label'])\n\nresults_bert = trainer_bert.evaluate()\n\nprint(\"BERT Approach:\")\nprint(f\"Accuracy: {results_bert['eval_accuracy']:.4f}\")\nprint(f\"F1-Score: {results_bert['eval_f1']:.4f}\")\nprint(f\"Precision: {results_bert['eval_precision']:.4f}\")\nprint(f\"Recall: {results_bert['eval_recall']:.4f}\")\n\npredictions_alternative = model_alternative.predict(X_val_tfidf)\n\naccuracy_alternative = accuracy_score(dataset['validation']['label'], predictions_alternative)\nf1_alternative = f1_score(dataset['validation']['label'], predictions_alternative, average='micro')\nprecision_alternative = precision_score(dataset['validation']['label'], predictions_alternative, average='micro')\nrecall_alternative = recall_score(dataset['validation']['label'], predictions_alternative, average='micro')\n\nprint(\"\\nAlternative Approach (TF-IDF + SVC):\")\nprint(f\"Accuracy: {accuracy_alternative:.4f}\")\nprint(f\"F1-Score: {f1_alternative:.4f}\")\nprint(f\"Precision: {precision_alternative:.4f}\")\nprint(f\"Recall: {recall_alternative:.4f}\")"
    },
    {
        "file_name": "Assignment_4_Niklas_Dobberstein.ipynb",
        "question": "#### Here comes your discussion",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Carl_Jacobsen.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "# Here comes your code\nfrom datasets import load_dataset\n#Import all the data.\ntrain_data = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\", split =\"train\")\nvalid_data = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\", split =\"validation\")\ntest_data = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\", split =\"test\")\n#Gives an example.\nprint(valid_data[-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Carl_Jacobsen.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "import torch\nfrom transformers import BertTokenizer, BertModel\nfrom datasets import Dataset\n#The tokenizer I use.\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n#Takes a dataset and return the wanted set.\ndef tokenize_data(dataset):\n    #Create a dictionary with the same keys as dataset without \"ID\".\n    tokenized_data = dict.fromkeys(dataset.features.keys(), [])\n    del tokenized_data[\"ID\"]\n\n    #Takes the values of the dataset to the same location in the wanted format.\n    for key in dataset.features.keys():\n        if key == \"ID\":\n            pass #tokenized_data[key] = dataset[key]\n        elif key == \"Tweet\":\n            tokenized_data[key] = [tokenizer(text, return_tensors='pt')[\"input_ids\"]\n                                  for text in dataset[key]]\n        else:\n            tokenized_data[key] = [float(boolean) for boolean in dataset[key]]\n        \n    return Dataset.from_dict(tokenized_data)\n#b)\nprint(tokenize_data(valid_data)[-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Carl_Jacobsen.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "# Here comes your code for BERT Approach\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nmodel_name = \"bert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type = \"multi_label_classification\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n#Like in the exercise.\ntraining_args = TrainingArguments(\n    output_dir=\"my_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\ntrainer = Trainer(\n    model = model,\n    args=training_args,\n    train_dataset=tokenize_data(train_data),\n    eval_dataset=tokenize_data(test_data),\n)\n#I can't find how to specify the training labels and training values for the trainer.\n#trainer.train()\n# Here comes your code for the alternative approach\n#I hace choosen the TF-idf and Logistic Regression.\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import confusion_matrix\n#Apllies TF-idf to the data.\ndef get_X(data):\n    list_of_all_texts = [text for text in train_data[\"Tweet\"]]\n    vectorizer = TfidfVectorizer()\n    return vectorizer.fit_transform(list_of_all_texts)\n\n#Gets the labels of the data as an np array.\ndef get_y(data):\n    y = np.array(\n        [train_data[key] for key in train_data.features.keys() if key not in [\"ID\", \"Tweet\"]]\n    )\n    return np.transpose(y)\n#Since we have multiple labels I used MultiOutputClassifier.\nclf = MultiOutputClassifier(LogisticRegression())\nclf.fit(get_X(train_data), get_y(train_data))\nclf.score(get_X(test_data), get_y(train_data))"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Carl_Jacobsen.ipynb",
        "question": "#### Here comes your discussion",
        "answer": "\"\"\"\nSince I didn't finish the first exercise I can't compare.\nFurther I can't make sense of F1-score, precision and recall in the mutlilabel case.\nBut my score for b) is bad so i guess a) should be better.\n\"\"\""
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 1 (5 points)\nThe goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\na) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\nb) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n**Hint:** If you don't have access to GPU, you can downsample the dataset.",
        "answer": "!pip install datasets\n# Here comes your code\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\ndataset\nvalidation_set = dataset['validation']\nprint(validation_set[-1])"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 2 (8 points)\na) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n**Hints:**\n1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\nb) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n**Hint:** The output should be as follows:\n`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`",
        "answer": "from transformers import BertTokenizer\nfrom torch.utils.data import Dataset\nimport torch\n\ndef tokenize_data(dataset):\n    # load tokenizer\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    # tokenize the text and convert labels to tensors\n    def tokenize_sample(sample):\n        # tokenize the text\n        tokens = tokenizer(sample['Tweet'], padding=\"max_length\", truncation=True)\n\n        # convert labels to tensors\n        col_names = list(sample.keys())[2:]\n        # print(col_names)\n\n        labels = torch.FloatTensor([sample[key] for key in col_names])\n\n        # combine\n        tokens['labels'] = labels\n        # print(tokens.keys())\n\n        return tokens\n\n    # use map()\n    encoded_dataset = dataset.map(tokenize_sample, remove_columns=dataset.column_names)\n\n    # set the format\n    encoded_dataset.set_format(\"torch\")\n\n    return encoded_dataset\nencoded_val_dataset = tokenize_data(dataset[\"validation\"])\nprint(encoded_val_dataset[-1].keys())"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "### Task 3 (17 points)\nImplement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\na) **BERT Approach:**\n``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n**Hints:**\n- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n**Hints:**\n- Utilize `TrainingArguments` and `Trainer` classes.\n- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\nb) **Alternative Approach:**\n``1.`` Choose an alternative approach for tokenization and classification. For sample, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n**Hints:**\n  - Use scikit-learn library for the  implementations.\n``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n__Hints:__\n  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\nc) **Discussion:**\n Discuss the strengths and weaknesses of each approach.\n__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.",
        "answer": "label_names = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n# from transformers import AutoModelForSequenceClassification, AdamW\n# from torch.utils.data import DataLoader\n# from tqdm import tqdm\n# import numpy as np\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n\n\nlabel_to_id = {i:j for i,j in enumerate(label_names)}\nid_to_label = {v: k for k, v in label_to_id.items()}\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=len(label_to_id),\n    id2label=id_to_label,\n    label2id=label_to_id,\n    problem_type=\"multi_label_classification\"\n)\n\nnum_epochs = 3\nlearning_rate = 2e-5\nbatch_size = 8\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nencoded_val_dataset = tokenize_data(dataset[\"validation\"])\nencoded_test_dataset = tokenize_data(dataset[\"test\"])\n\n\n# due to limited computation resources, only use 25% of the train dataset\nfraction = 0.25\nnum_samples = int(len(dataset[\"train\"]) * fraction)\nsmall_train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in range(num_samples)])\n\nencoded_train_dataset = tokenize_data(small_train_dataset)\n\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n# model.train()\n# train_dataloader = DataLoader(encoded_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n# mean_loss = []\n# for epoch in range(num_epochs):\n#     total_loss = 0\n#     progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f'Epoch {epoch + 1}')\n\n#     for batch_idx, batch in progress_bar:\n#         optimizer.zero_grad()\n\n#         batch = {k: v.to(device) for k, v in batch.items()}\n\n\n#         # Forward pass\n#         outputs = model(**batch)\n\n#         loss = outputs.loss\n#         loss.backward()\n#         optimizer.step()\n#         total_loss += loss.item()\n#         mean_loss.append(loss.item())\n#         m_l = np.mean(mean_loss)\n#         # Update tqdm description for each batch\n#         progress_bar.set_postfix({'Current Loss': loss.item(), 'Mean Loss': m_l})\n\n\n#     # Update tqdm description for the epoch\n#     progress_bar.set_postfix({'Epoch Loss': total_loss})\n#     progress_bar.close()\n# Define TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_output\",\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=10,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n)\n\ndef compute_metrics(p):\n    preds = p.predictions > 0.5\n    labels = p.label_ids\n\n    accuracy = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n    }\n\n# Trainer initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train_dataset,\n    eval_dataset=encoded_val_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n\nX_train = small_train_dataset['Tweet']\ny_train = small_train_dataset.to_pandas().iloc[:,2:].astype(float).values\n\nX_val = dataset[\"validation\"][\"Tweet\"]\ny_val = dataset[\"validation\"].to_pandas().iloc[:,2:].astype(float).values\n\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_val_tfidf = tfidf_vectorizer.transform(X_val)\n\n# SVM model for multi-label classification\nsvm_model = OneVsRestClassifier(SVC(kernel='linear', probability=True))\nsvm_model.fit(X_train_tfidf, y_train)\n\n# make predictions\ny_pred_svm = svm_model.predict(X_val_tfidf)\n\n# evaluate performance\naccuracy_svm = accuracy_score(y_val, y_pred_svm)\nf1_svm = f1_score(y_val, y_pred_svm, average='weighted')\nprecision_svm = precision_score(y_val, y_pred_svm, average='weighted')\nrecall_svm = recall_score(y_val, y_pred_svm, average='weighted')\n\nprint(\"SVM Performance for Multi-label Classification:\")\nprint(f\"Accuracy: {accuracy_svm}\")\nprint(f\"F1 Score: {f1_svm}\")\nprint(f\"Precision: {precision_svm}\")\nprint(f\"Recall: {recall_svm}\")"
    },
    {
        "file_name": "Intro2NLP_Assignment_4_Amr_Mohamed_Gaber_Soliman_Moustafa.ipynb",
        "question": "#### Here comes your discussion",
        "answer": "\"\"\"\nBERT Approach:\n\nAccuracy: 18.40%\nF1 Score: 53.16%\nPrecision: 38.50%\nRecall: 85.86%\n------------------\nTF-IDF with SVM Approach:\n\nAccuracy: 13.99%\nF1 Score: 42.54%\nPrecision: 68.32%\nRecall: 32.90%\n------------------\nComparison:\n\n1) The BERT approach outperforms the TF-IDF with SVM approach in terms of recall\nand overall F1 score.\n2) The TF-IDF with SVM approach has a higher precision, indicating a lower rate\nof false positives.\n3) Both approaches exhibit relatively low accuracy, highlighting the difficulty\nof the task or the need for further model refinement.\n------------------\nNotes:\n\n- The BERT approach with the full dataset and multiple epochs is expected to\noutperform the TF-IDF with SVM approach in terms of all the metrics.\n\n- Monitoring the model's performance on a validation set during training is\ncrucial to avoid overfitting.\n\n- The choice between the two approaches depends on various factors, including\ncomputational resources, dataset size, and the specific requirements of the task.\nFurther hyperparameter tuning or using more advanced techniques might improve\nthe performance of both approaches.\n\"\"\"\n\"\"\"\nBERT Approach:\n(Powerful for contextual understanding but resource-intensive)\n    Strengths:\n        1) Contextual Understanding:\n            Excels at understanding contextual nuances in language.\n        2) Transfer Learning:\n            Leverages pre-training for general language patterns.\n        3) Complex Relationships:\n            Well-suited for tasks requiring understanding of intricate relationships.\n    Weaknesses:\n        1) Computational Resources:\n            Demands significant computational resources for training and inference.\n        2) Training Time:\n            Training is time-consuming, not ideal for rapid deployment.\n        3) Interpretability:\n            Complex architecture makes interpretation challenging.\n\n------------------\nTF-IDF with SVM Approach:\n(Simple and interpretable, efficient for certain tasks but lacks contextual depth)\n    Strengths:\n        1) Simplicity:\n            Simple and easy to implement.\n        2) Interpretability:\n            Provides clear decision boundaries for interpretation.\n        3) Efficiency on Sparse Data and High-Dimensional Data.\n    Weaknesses:\n        1) Limited Contextual Understanding.\n        2) Dependency on Feature Engineering.\n        3) Limited Generalization:\n            May not generalize well to diverse datasets.\n\n\"\"\""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_RaoRohilPrakash.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "from itertools import permutations\n\ndef all_possible_tags(tags):\n    word_positions = len(tags)\n    all_possible_tags_dict = {}\n\n    tag_permutations = list(permutations(tags))\n\n    for i, tag_sequence in enumerate(tag_permutations, start=1):\n        all_possible_tags_dict[str(i)] = list(tag_sequence)\n\n    return all_possible_tags_dict\n\n# Example usage\ntags = [\"VB\", \"TO\", \"NN\", \"PPSS\"]\nall_possible_tags_dict  = all_possible_tags(tags)\nprint(all_possible_tags_dict )\nimport csv\n\ndef read_transition_probabilities(file_path):\n    transition_probabilities = {}\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        headers = next(reader)[1:]  # Skip the first column and store headers\n\n        for row in reader:\n            if len(row) > 1:\n                current_tag = row[0]\n                probabilities = {header: float(prob) for header, prob in zip(headers, row[1:])}\n                transition_probabilities[current_tag] = probabilities\n\n    return transition_probabilities\n\n# Example usage\nfile_path = \"transition_probabilities.csv\"\ntransition_probabilities = read_transition_probabilities(file_path)\n\nprint(transition_probabilities)\nimport csv\n\ndef read_emission_probabilities(file_path):\n    emission_probabilities = {}\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        headers = next(reader)[1:]  # Skip the first column and store headers\n\n        for row in reader:\n            if len(row) > 1:\n                current_tag = row[0]\n                probabilities = {header: float(prob) for header, prob in zip(headers, row[1:])}\n                emission_probabilities[current_tag] = probabilities\n\n    return emission_probabilities\n\n# Example usage\nfile_path = \"emission_probabilities.csv\"\nemission_probabilities = read_emission_probabilities(file_path)\n\nprint(emission_probabilities)\ndef all_possible_tags_probability(tag_sequences, transition_probabilities, emission_probabilities):\n    all_possible_tags_probability_dict = {}\n\n    for seq_id, tags in tag_sequences.items():\n        probability = 1.0\n        prev_tag = '<s>'\n\n        for tag in tags:\n            # Calculate transition probability\n            if tag in transition_probabilities[prev_tag]:\n                probability *= transition_probabilities[prev_tag][tag]\n            else:\n                probability = 0.0\n                break\n\n            # Calculate emission probability\n            word = ' '.join(tags)\n            if word in emission_probabilities[tag]:\n                probability *= emission_probabilities[tag][word]\n            else:\n                probability = 0.0\n                break\n\n            prev_tag = tag\n\n        all_possible_tags_probability_dict[seq_id] = probability\n\n    return all_possible_tags_probability_dict\n\n# Example usage:\nresult = all_possible_tags_probability(all_possible_tags_dict, transition_probabilities, emission_probabilities)\nprint(\"The probability for each possible tag sequence >> all_possible_tags_probability_dict =\", result)\ndef all_possible_tags_probability(tag_sequences, transition_probabilities, emission_probabilities):\n    all_possible_tags_probability_dict = {}\n\n    for seq_id, tags in tag_sequences.items():\n        probability = 1.0\n        prev_tag = '<s>'\n\n        for tag, word in zip(tags, tags):\n            # Calculate transition probability\n            if tag in transition_probabilities[prev_tag]:\n                probability *= transition_probabilities[prev_tag][tag]\n            else:\n                probability = 0.0\n                break\n\n            # Calculate emission probability for each word in the sequence\n            if tag in emission_probabilities and word in emission_probabilities[tag]:\n                probability *= emission_probabilities[tag][word]\n            else:\n                probability = 0.0\n                break\n\n            prev_tag = tag\n\n        all_possible_tags_probability_dict[seq_id] = probability\n\n    return all_possible_tags_probability_dict\n\n# Example usage:\n# (same as before)\n\nresult = all_possible_tags_probability(all_possible_tags_dict, transition_probabilities, emission_probabilities)\nprint(\"The probability for each possible tag sequence >> all_possible_tags_probability_dict =\", result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_RaoRohilPrakash.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transition_probabilities, emission_probabilities):\n    words = sentence.split()\n    tags = list(transition_probabilities.keys())\n\n    # Initialization Step\n    viterbi_table = [{}]\n    for tag in tags:\n        transition_prob = transition_probabilities.get('<s>', {}).get(tag, 0)\n        emission_prob = emission_probabilities.get(tag, {}).get(words[0], 0)\n\n        viterbi_table[0][tag] = {\n            'prob': transition_prob * emission_prob,\n            'prev_tag': None\n        }\n        \n    # Recursion Step\n    for t in range(1, len(words)):\n        viterbi_table.append({})\n        for tag in tags:\n            max_prob = 0\n            prev_tag_selected = None\n\n            for prev_tag in tags:\n                transition_prob = transition_probabilities.get(prev_tag, {}).get(tag, 0)\n                emission_prob = emission_probabilities.get(tag, {}).get(words[t], 0)\n\n                prev_prob = viterbi_table[t - 1][prev_tag]['prob'] if prev_tag in viterbi_table[t - 1] else 0\n\n                prob = prev_prob * transition_prob * emission_prob\n\n                if prob > max_prob:\n                    max_prob = prob\n                    prev_tag_selected = prev_tag\n\n            viterbi_table[t][tag] = {\n                'prob': max_prob,\n                'prev_tag': prev_tag_selected\n            }\n    # Termination Step\n    last_tag = max(viterbi_table[-1], key=lambda x: viterbi_table[-1][x]['prob'])\n    max_prob = viterbi_table[-1][last_tag]['prob']\n\n    # Backtrack to find the most probable tag sequence\n    tag_sequence = [last_tag]\n    for t in range(len(words) - 1, 0, -1):\n        last_tag = viterbi_table[t][last_tag]['prev_tag']\n        tag_sequence.insert(0, last_tag)\n\n    most_probable_tag_seq_dict = {'tag_sequence': tag_sequence, 'probability': max_prob}\n    return most_probable_tag_seq_dict\n\n\nsentence = \"I want to race\"\nresult = viterbi(sentence, transition_probabilities, emission_probabilities)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_RaoRohilPrakash.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Muskaan_Chopra.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd\nimport numpy as np\nfrom itertools import product\n\ndef all_possible_tags(sentence, emission_probabilities):\n    # Load emission probabilities from CSV\n    emission_df = pd.read_csv(emission_probabilities, index_col='Unnamed: 0')\n    emission_df.columns = emission_df.columns.str.strip()\n\n    words = sentence.split()\n    all_possible_tags_dict = {}\n\n    # Get all possible tags for each word in the sentence\n    for i, word in enumerate(words):\n        possible_tags = emission_df.index[emission_df[word.strip()] > 0].tolist()\n        all_possible_tags_dict[str(i+1)] = possible_tags\n\n    return all_possible_tags_dict\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):\n    # Load transition probabilities from CSV\n    transition_df = pd.read_csv(transition_probabilities, index_col='Unnamed: 0')\n    transition_df.columns = transition_df.columns.str.strip()\n\n    # Load emission probabilities from CSV\n    emission_df = pd.read_csv(emission_probabilities, index_col='Unnamed: 0')\n    emission_df.columns = emission_df.columns.str.strip()\n\n    words = sentence.split()\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {'tag_sequence': [], 'probability': 0}\n\n    # Calculate the log probability for each possible tag sequence\n    for i, tag_sequence in enumerate(product(*all_possible_tags_dict.values())):\n        log_probability = 0\n        for j, tag in enumerate(tag_sequence):\n            if j == 0:\n                log_probability += np.log(transition_df.loc['<s>', tag] + 1e-10) + np.log(emission_df.loc[tag, words[j]] + 1e-10)\n            else:\n                log_probability += np.log(transition_df.loc[tag_sequence[j-1], tag] + 1e-10) + np.log(emission_df.loc[tag, words[j]] + 1e-10)\n        all_possible_tags_probability_dict[str(i+1)] = np.exp(log_probability)  # Convert back to regular probability\n\n        # Update the most probable tag sequence if necessary\n        if log_probability > np.log(most_probable_tag_seq_dict['probability'] + 1e-10):\n            most_probable_tag_seq_dict = {'tag_sequence': tag_sequence, 'probability': np.exp(log_probability)}  # Convert back to regular probability\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\nimport pandas\nemission_df=pd.read_csv('emission_probabilities.csv')\ntransition_df = pd.read_csv('transition_probabilities.csv')\nprint(emission_df.columns)\nemission_df\ntransition_df\n# Define your sentence and file paths\nsentence = \"I want to race\"\nemission_probabilities_file = \"emission_probabilities.csv\"\ntransition_probabilities_file = \"transition_probabilities.csv\"\n\n\n# Call the all_possible_tags function\nall_possible_tags_dict = all_possible_tags(sentence, emission_probabilities_file)\nprint(\"All possible tag sequences:\")\nfor key, value in all_possible_tags_dict.items():\n    print(f\"{key}: {value}\")\n\n# Call the all_possible_tags_probability function\nall_possible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(\n    sentence, all_possible_tags_dict, transition_probabilities_file, emission_probabilities_file\n)\nprint(\"\\nProbabilities for each possible tag sequence:\")\nfor key, value in all_possible_tags_probability_dict.items():\n    print(f\"{key}: {value}\")\n\nprint(\"\\nMost probable tag sequence:\")\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Muskaan_Chopra.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "import pandas as pd\nimport numpy as np\nfrom itertools import product\n\ndef viterbi(sentence, transition_probabilities, emission_probabilities):\n    # Load transition probabilities from CSV\n    transition_df = pd.read_csv(transition_probabilities, index_col='Unnamed: 0')\n    transition_df.columns = transition_df.columns.str.strip()\n\n    # Load emission probabilities from CSV\n    emission_df = pd.read_csv(emission_probabilities, index_col='Unnamed: 0')\n    emission_df.columns = emission_df.columns.str.strip()\n\n    words = sentence.split()\n    tags = transition_df.columns\n\n    # Initialization\n    viterbi = pd.DataFrame(index=tags, columns=range(len(words)))\n    backpointer = pd.DataFrame(index=tags, columns=range(len(words)))\n\n    for tag in tags:\n        viterbi.loc[tag, 0] = np.log(transition_df.loc['<s>', tag] + 1e-10) + np.log(emission_df.loc[tag, words[0]] + 1e-10)\n\n    # Recursion\n    for word_index in range(1, len(words)):\n        for tag in tags:\n            max_prob, prev_tag = max((viterbi.loc[prev_tag, word_index-1] + np.log(transition_df.loc[prev_tag, tag] + 1e-10) + np.log(emission_df.loc[tag, words[word_index]] + 1e-10), prev_tag) for prev_tag in tags)\n            viterbi.loc[tag, word_index] = max_prob\n            backpointer.loc[tag, word_index] = prev_tag\n\n    # Termination\n    max_prob, prev_tag = max((viterbi.loc[tag, len(words)-1], tag) for tag in tags)\n    most_probable_tag_seq = [prev_tag]\n    for i in range(len(words)-1, 0, -1):\n        most_probable_tag_seq.insert(0, backpointer.loc[most_probable_tag_seq[0], i])\n\n    most_probable_tag_seq_dict = {'tag_sequence': most_probable_tag_seq, 'probability': np.exp(max_prob)}  # Convert back to regular probability\n\n    return most_probable_tag_seq_dict\n# Define your sentence and file paths\nsentence = \"I want to race\"\nemission_probabilities_file = \"emission_probabilities.csv\"\ntransition_probabilities_file = \"transition_probabilities.csv\"\n\n# Call the viterbi function\nmost_probable_tag_seq_dict = viterbi(sentence, transition_probabilities_file, emission_probabilities_file)\n\n# Print the most probable tag sequence and its probability\nprint(\"Most probable tag sequence:\")\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Muskaan_Chopra.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Mauel_Maximilian.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "def read_csv_to_dict_of_dicts(file_path):\n    with open(file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        headers = next(reader)[1:]\n        result = {header.strip(): {} for header in headers}\n\n\n        for row in reader:\n            if (len(row) == 0):\n              continue\n            row_key = row[0]\n            for header, value in zip(headers, row[1:]):\n                result[header.strip()][row_key] = float(value) if value else 0\n        return result\n\nems = read_csv_to_dict_of_dicts('emission_probabilities.csv') # {word: {state: prob}}\ntrs = read_csv_to_dict_of_dicts('transition_probabilities.csv') # {cur_state: {prev_state: prob}} , state := tag\nimport numpy as np\nimport csv\nfrom itertools import product\n\ndef all_possible_tags(sentence, emission_probabilities):\n    words = sentence.split()\n    possible_tags_per_word = [list(emission_probabilities[word].keys()) for word in words]\n\n    tag_combinations = list(product(*possible_tags_per_word))\n\n    all_possible_tags_dict = {str(index + 1): list(combination) for index, combination in enumerate(tag_combinations)}\n\n    return all_possible_tags_dict\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):\n    # better impl would be using log probs but for this small example\n    # as an educational tool i stuck with 'normal' probabilities\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {\"tag_sequence\": [], \"probability\": 0}\n\n    words = sentence.split()\n\n    for index, tag_sequence in all_possible_tags_dict.items():\n        probability = 1.0\n\n        for i, tag in enumerate(tag_sequence):\n            word = words[i]\n            emission_prob = emission_probabilities[word].get(tag, 0)\n            prev_tag = tag_sequence[i - 1] if i > 0 else \"<s>\"\n\n            transition_prob = transition_probabilities.get(tag, {}).get(prev_tag, 0)\n\n            probability *= emission_prob * transition_prob\n\n        all_possible_tags_probability_dict[index] = probability\n\n        if probability > most_probable_tag_seq_dict[\"probability\"]:\n            most_probable_tag_seq_dict = {\n                \"tag_sequence\": tag_sequence,\n                \"probability\": probability\n            }\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\nsentence = \"I want to race\"\nall_tags = all_possible_tags(sentence, ems)\nall_probs, most_prob_seq = all_possible_tags_probability(sentence, all_tags, trs, ems)\nmost_prob_seq#, all_probs"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Mauel_Maximilian.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transition_probabilities, emission_probabilities):\n    words = sentence.split()\n    states = list(transition_probabilities.keys())\n\n    V = [{}]\n    path = {}\n\n    for state in states:\n        V[0][state] = transition_probabilities[state]['<s>'] * emission_probabilities[words[0]].get(state, 0)\n        path[state] = [state]\n\n    for t in range(1, len(words)):\n        V.append({})\n        newpath = {}\n\n        for current_state in states:\n            (prob, state) = max(\n                (V[t - 1][prev_state] * transition_probabilities[current_state].get(prev_state, 0) * emission_probabilities[words[t]].get(current_state, 0), prev_state) for prev_state in states\n            )\n            V[t][current_state] = prob\n            newpath[current_state] = path[state] + [current_state]\n\n        path = newpath\n\n    (prob, state) = max((V[t][state], state) for state in states)\n    most_probable_tag_seq_dict = {\n        \"tag_sequence\": path[state],\n        \"probability\": prob\n    }\n\n    return most_probable_tag_seq_dict\nviterbi(sentence, trs, ems)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Mauel_Maximilian.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": "f\"1) {1.0 * 0.2 * 0.2 * 0.1 * 0.9 * 0.4 * 0.8 * 0.9}, 2) {1.0 * 0.2 * 0.2 * 0.1 * 0.5 ** 4}\"\n# 2) If every hidden state has the same probability, then the hidden states provides no extra information and are therefore useless in this context."
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Ishfaq_Herbrik.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "def all_possible_tags(sentence, emission_probabilities):\n    words = sentence.split()\n    all_possible_tags_dict = {}\n\n    for i, word in enumerate(words):\n        possible_tags = []\n        for tag in emission_probabilities:\n            if emission_probabilities[tag].get(word, 0) > 0:\n                possible_tags.append(tag)\n        all_possible_tags_dict[str(i + 1)] = possible_tags\n\n    return all_possible_tags_dict\n\n\ndef all_possible_tags_probability(\n    sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities\n):\n    from itertools import product\n\n    words = sentence.split()\n    all_sequences = list(product(*all_possible_tags_dict.values()))\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {\"tag_sequence\": [], \"probability\": 0}\n\n    for sequence in all_sequences:\n        prob = 1\n        prev_tag = \"<s>\"\n        for i, tag in enumerate(sequence):\n            trans_prob = transition_probabilities[prev_tag].get(tag, 0)\n            emis_prob = emission_probabilities[tag].get(words[i], 0)\n            prob *= trans_prob * emis_prob\n            prev_tag = tag\n\n        all_possible_tags_probability_dict[str(sequence)] = prob\n\n        if prob > most_probable_tag_seq_dict[\"probability\"]:\n            most_probable_tag_seq_dict[\"tag_sequence\"] = sequence\n            most_probable_tag_seq_dict[\"probability\"] = prob\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\nimport pandas as pd\n\n# Read the data from the CSV files and print results\ndf_transition_probabilities = pd.read_csv(\"transition_probabilities.csv\", index_col=0)\ntransition_probabilities = df_transition_probabilities.to_dict(orient=\"index\")\n# Post-process the dictionary to format it correctly\ntransition_probabilities = {\n    str(index): {str(column).strip(): value for column, value in row.items()}\n    for index, row in transition_probabilities.items()\n}\n\ndf_emission_probabilities = pd.read_csv(\"emission_probabilities.csv\", index_col=0)\nemission_probabilities = df_emission_probabilities.to_dict(orient=\"index\")\n# Post-process the dictionary to format it correctly\nemission_probabilities = {\n    str(index): {str(column).strip(): value for column, value in row.items()}\n    for index, row in emission_probabilities.items()\n}\n\nsentence = \"I want to race\"\nall_possible_tags_dict = all_possible_tags(sentence, emission_probabilities)\n(\n    all_possible_tags_probability_dict,\n    most_probable_tag_seq_dict,\n) = all_possible_tags_probability(\n    sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities\n)\n\nprint(\"All possible tags:\")\nprint(all_possible_tags_dict)\nprint(\"All possible tags probability:\")\nprint(all_possible_tags_probability_dict)\nprint(\"Most probable tag sequence:\")\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Ishfaq_Herbrik.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transition_probabilities, emission_probabilities):\n    # Split the sentence into words\n    words = sentence.split()\n    states = list(transition_probabilities.keys())[1:]  # exclude the start symbol <s>\n    T = len(words)\n\n    # Initialize the Viterbi and backpointer tables\n    viterbi = {s: [0] * T for s in states}\n    backpointer = {s: [0] * T for s in states}\n\n    # Initialization step\n    for s in states:\n        viterbi[s][0] = transition_probabilities[\"<s>\"][s] * emission_probabilities[\n            s\n        ].get(words[0], 0)\n        backpointer[s][0] = \"<s>\"\n\n    # Recursion step\n    for t in range(1, T):\n        for s in states:\n            max_tr_prob = max(\n                viterbi[prev_s][t - 1] * transition_probabilities[prev_s][s]\n                for prev_s in states\n            )\n            for prev_s in states:\n                if (\n                    viterbi[prev_s][t - 1] * transition_probabilities[prev_s][s]\n                    == max_tr_prob\n                ):\n                    max_prob = max_tr_prob * emission_probabilities[s].get(words[t], 0)\n                    viterbi[s][t] = max_prob\n                    backpointer[s][t] = prev_s\n\n    # Termination step\n    max_prob = max(value[T - 1] for value in viterbi.values())\n    previous = None\n    # Get the best last state\n    for st, data in viterbi.items():\n        if data[T - 1] == max_prob:\n            best_tag_sequence = st\n            previous = st\n            break\n\n    # Follow the back pointers to find the best path\n    best_path = [best_tag_sequence]\n    for t in range(T - 1, 0, -1):\n        best_path.insert(0, backpointer[previous][t])\n        previous = backpointer[previous][t]\n\n    # Create the output dictionary\n    most_probable_tag_seq_dict = {\"tag_sequence\": best_path, \"probability\": max_prob}\n\n    return most_probable_tag_seq_dict\n# Running the Viterbi algorithm\nviterbi(sentence, transition_probabilities, emission_probabilities)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Ishfaq_Herbrik.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Sinem_DU00f6nmez.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd\nimport itertools\n\n# Load transition and emission probabilities\ntransition_probabilities = pd.read_csv('transition_probabilities.csv', index_col=0)\nemission_probabilities = pd.read_csv('emission_probabilities.csv', index_col=0)\n\n# Remove spaces from the index and columns\ntransition_probabilities.index = transition_probabilities.index.str.strip()\ntransition_probabilities.columns = transition_probabilities.columns.str.strip()\nemission_probabilities.index = emission_probabilities.index.str.strip()\nemission_probabilities.columns = emission_probabilities.columns.str.strip()\n\ndef all_possible_tags(sentence, emission_probabilities):\n    words = sentence.split()\n    tags_list = emission_probabilities.columns.tolist()\n\n    # Create a dictionary to store all possible tag sequences\n    all_possible_tags_dict = {}\n    all_possible_tags = list(itertools.product(tags_list, repeat=len(words)))\n\n    # all_possible_tags = list(itertools.permutations(tags_list, len(words)))\n    for i, tag in enumerate(all_possible_tags):\n        all_possible_tags_dict[str(i+1)] = tag\n    return all_possible_tags_dict\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):\n    words = sentence.split()\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {'tag_sequence': [], 'probability': 0}\n\n    # Generate all possible tag sequences\n\n    for i, tag_sequence in enumerate(list(all_possible_tags_dict.values())):\n        probability = 1\n        for j, tag in enumerate(tag_sequence):\n            word = words[j]\n            if j == 0:\n                probability *= emission_probabilities.loc[word, tag]\n            else:\n                previous_tag = tag_sequence[j-1]\n                probability *= transition_probabilities.loc[previous_tag, tag] * emission_probabilities.loc[word, tag]\n\n        all_possible_tags_probability_dict[str(i+1)] = probability\n\n        # Update the most probable tag sequence\n        if probability > most_probable_tag_seq_dict['probability']:\n            most_probable_tag_seq_dict['tag_sequence'] = tag_sequence\n            most_probable_tag_seq_dict['probability'] = probability\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\n\n# Test the functions\nsentence = \"I want to race\"\nall_possible_tags_dict = all_possible_tags(sentence, emission_probabilities.T)\nall_possible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities.T)\n\nprint(all_possible_tags_dict)\nprint(all_possible_tags_probability_dict)\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Sinem_DU00f6nmez.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "import numpy as np\ndef viterbi(sentence, transition_probabilities, emission_probabilities):\n\n    # Create a mapping from tag to index\n    state_to_index = {}\n    for i, state in enumerate(transition_probabilities.index):\n        state_to_index[state] = i\n\n    index_to_state = {v:k for k,v in state_to_index.items()}\n\n    word_to_index = {}\n    for i, word in enumerate(emission_probabilities.index):\n        word_to_index[word] = i\n\n    index_to_word = {v:k for k,v in word_to_index.items()}\n\n    # Split the sentence into words\n    observations = sentence.split()\n    T = len(observations)\n    N = len(transition_probabilities)\n\n    # Initialize the viterbi and backpointer matrices\n    viterbi = np.zeros((N, T))\n    backpointer = np.zeros((N, T), dtype=int)\n\n    # Initialization\n    for s in range(N):\n        viterbi[s, 0] = transition_probabilities.loc[index_to_state[s], index_to_state[0]] * emission_probabilities.loc[observations[0],index_to_state[s]]\n        backpointer[s, 0] = 0\n\n    # Recursion\n    for t in range(1, T):\n        for s in range(N):\n            viterbi[s, t] = max([viterbi[s_prime, t-1] * transition_probabilities.loc[index_to_state[s_prime], index_to_state[s]] * emission_probabilities.loc[observations[t], index_to_state[s]] for s_prime in range(N)])\n            backpointer[s, t] = np.argmax([viterbi[s_prime, t-1] * transition_probabilities.loc[index_to_state[s_prime], index_to_state[s]] * emission_probabilities.loc[observations[t],index_to_state[s]] for s_prime in range(N)])\n\n    # Termination\n    bestpathprob = max(viterbi[:, T-1])\n    bestpathpointer = np.argmax(viterbi[:, T-1])\n\n    # Path (state sequence) backtracking\n    bestpath = []\n    for t in range(T-1, -1, -1):\n        bestpath.insert(0, bestpathpointer)\n        bestpathpointer = backpointer[bestpathpointer, t]\n\n    # Convert state sequence to tag sequence\n    most_probable_tag_seq_dict = {'tag_sequence': [index_to_state[state] for state in bestpath], 'probability': bestpathprob}\n    # tag_sequence = [state_to_tag[state] for state in bestpath]  # assuming you have a state_to_tag dictionary\n\n    # most_probable_tag_seq_dict = {'tag_sequence': tag_sequence, 'probability': bestpathprob}\n\n    return most_probable_tag_seq_dict\n\nviterbi(sentence, transition_probabilities.T, emission_probabilities.T)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Sinem_DU00f6nmez.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd \nimport numpy as np \nfrom itertools import product\n# strip all column names of emmisions \ndef strip_column_names(df):\n    df.columns = df.columns.str.strip()\n    return df\ndef all_possible_tags(sentence, emission_probabilities):\n\n    all_possible_tags_dict = {}\n    \n    #Here comes your code\n    emmision = pd.read_csv(emission_probabilities)\n    emmision = strip_column_names(emmision)\n    sentence_list = sentence.split()\n    options = []\n    for i, word in enumerate(sentence_list):\n        non_zero = emmision.loc[emmision[word] != 0]\n        non_zero_pos = non_zero.iloc[:,0].tolist()\n        options.append( non_zero_pos )\n        \n    # make combinations of all possible options \n    all_possible_tags = list(product(*options))\n    \n    for i, tag in enumerate(all_possible_tags):\n        all_possible_tags_dict[i+1] = tag\n    return(all_possible_tags_dict)\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n    transition = pd.read_csv(transisition_probabilities)\n    transition = strip_column_names(transition)\n    transition.set_index('Unnamed: 0', inplace=True)\n\n    emmision = pd.read_csv(emission_probabilities)\n    emmision = strip_column_names(emmision)\n    emmision.set_index('Unnamed: 0', inplace=True)\n    \n    sentence =  sentence.split()\n    \n    for key in all_possible_tags_dict.keys():\n        tag_sequence = all_possible_tags_dict[key]\n        tag_prob = 1\n        for i, tag in enumerate(tag_sequence):\n            word = sentence[i]\n            # get emmision probability\n            em_prob = emmision.loc[tag, word]\n            \n            # get transition probability\n            current_tag = tag \n            previous_tag = tag_sequence[i-1] if i > 0 else '<s>'\n            trans_prob = transition.loc[previous_tag, current_tag]\n            tag_prob *= em_prob * trans_prob \n        all_possible_tags_probability_dict[key] = tag_prob\n    \n    # argmax of all possible tag sequences to get most probable tag sequence\n    max_key = max(all_possible_tags_probability_dict, key=all_possible_tags_probability_dict.get)\n    max_prob = all_possible_tags_probability_dict[max_key]\n    most_probable_tag_seq_dict['tag_sequence'] = all_possible_tags_dict[max_key]\n    most_probable_tag_seq_dict['probability'] = max_prob\n            \n\n    return(all_possible_tags_probability_dict, most_probable_tag_seq_dict)\nall_possible_tags_dict = all_possible_tags('I want to race', './emission_probabilities.csv')\npossible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(sentence='I want to race', all_possible_tags_dict=all_possible_tags_dict, transisition_probabilities='./transition_probabilities.csv', emission_probabilities='./emission_probabilities.csv')\nprint('All possible tags: ')\nprint(possible_tags_probability_dict)\n\nprint('Most probable tag sequence: ')\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transition_probabilities, emission_probabilities):\n    most_probable_tag_seq_dict = {}\n    #Here comes your code \n    sentence = sentence.split()\n    # Initialization \n    viterbi_matrix = pd.DataFrame(index=transition_probabilities.columns, columns=sentence)\n    backpointer_matrix = pd.DataFrame(index=transition_probabilities.columns, columns=sentence)\n\n    for tag in transition_probabilities.columns:\n        viterbi_matrix.loc[tag, sentence[0]] = transition_probabilities.loc['<s>', tag] * emission_probabilities.loc[tag, sentence[0]]\n        backpointer_matrix.loc[tag, sentence[0]] = '<s>'\n\n    # Recursion \n    for t in range(1, len(sentence)):\n        for tag in transition_probabilities.columns:\n            max_prob = 0\n            max_tag = ''\n            for prev_tag in transition_probabilities.columns:\n                prob = viterbi_matrix.loc[prev_tag, sentence[t - 1]] * transition_probabilities.loc[prev_tag, tag] * emission_probabilities.loc[tag, sentence[t]]\n                if prob > max_prob:\n                    max_prob = prob\n                    max_tag = prev_tag\n\n            viterbi_matrix.loc[tag, sentence[t]] = max_prob\n            backpointer_matrix.loc[tag, sentence[t]] = max_tag\n\n    # Termination \n    max_final_prob = 0\n    max_final_tag = ''\n    for tag in transition_probabilities.columns:\n        final_prob = viterbi_matrix.loc[tag, sentence[-1]]\n        if final_prob > max_final_prob:\n            max_final_prob = final_prob\n            max_final_tag = tag\n\n    # Backtracking\n    tag_sequence = [max_final_tag]\n    for t in range(len(sentence) - 1, 0, -1):\n        max_final_tag = backpointer_matrix.loc[max_final_tag, sentence[t]]\n        tag_sequence.insert(0, max_final_tag)\n\n    most_probable_tag_seq_dict = {'tag_sequence': tag_sequence, 'probability': max_final_prob}\n    return most_probable_tag_seq_dict\n\n\n\ntransition_probs = pd.read_csv('./transition_probabilities.csv').set_index('Unnamed: 0')\nemission_probs = pd.read_csv('./emission_probabilities.csv').set_index('Unnamed: 0')\n\ntransition_probs = strip_column_names(transition_probs)\nemission_probs = strip_column_names(emission_probs)\n\nsentence = \"I want to race\"\n\nresult = viterbi(sentence, transition_probs, emission_probs)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### 1)\n### STATES St = SUNNY - CLOUDY - RAINY - SUNNY\n### Sequence Xt =  DRY - WET - WET - DRY\n p(Xt | St) = p(Dry|Sunny)*p(Sunny) * \n p(Wet|Cloudy) * p(Cloudy|Sunny) * \n p(Wet|Rainy) * p(Rainy|Sunny) * \n p(Dry|Sunny) * p(Sunny|Rainy) = \n (0.9*0.4) * (0.4*0.2) * (0.8*0.2) * (0.9*0.1) = 0.0004147",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_MohamedAmrMohamedFathiRoshdi.ipynb",
        "question": "### 2 ) In that case, our emission model essentially randomly predicts the weather state based on the hidden state with 50% probability of the weather being wet or dry. This would have the effect of lowering the certainity of the probability calculated in part 1) since we are more uncertain of the liklihood of the dry state given the sunny hidden state, which appears in the first and last steps of the sequence. Thus our new, more uncertain result would be : \n (0.5*0.4) * (0.4*0.2) * (0.8*0.2) * (0.5*0.1) = 0.000128\n### which is 3 times less than our original probability.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Jan_Rogge.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "def all_possible_tags(sentence, emission_probabilities):\n    #get tags from the emission probabilities, first col in df\n    tags = emission_probabilities.iloc[:, 0].to_list()\n\n    #also split the words into a list, used mainly to know the number of words\n    #(this wouldnt catch weird cases where two words are not seperated by \" \")\n    words = sentence.split(\" \")\n    \n    #all tags is the cartesian product of tags, repeated words many times \n    all_tags = list(map(list, product(tags, repeat=len(words))))#product gives a tuple, map makes this a list of lists, instead of list of tuples\n\n    all_possible_tags_dict = {}\n    for i in range(len(all_tags)):\n        all_possible_tags_dict.update({i: all_tags[i]})#add all the tags into the dict\n\n    return(all_possible_tags_dict)\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n\n    #mappings from tags to indeces in the database, cause I was too stupid too read the file in properly\n    trans_dict = { \"VB\": 1, \"TO\": 2, \"NN\": 3, \"PPSS\": 4}\n    emis_dict =  { \"VB\": 0, \"TO\": 1, \"NN\": 2, \"PPSS\": 3}\n\n    words = sentence.split(\" \")#split the sentence into words, used to access the emission probabilities\n\n    index = 0\n    for seq in all_possible_tags_dict.values():#for every value in our dict, so all sequences\n        #we want to multiply the transmission and emission probalities, and sum them up in prob for every tag in the sequence:\n        prob = 0\n        for i in range(len(seq)):#go over the entire sequence\n            if i == 0:#if we are at the first index of the sequence, we need the transition from start\n                emis_index = emis_dict[seq[0]]\n                prob += transisition_probabilities.loc[i, \" \"+ seq[i]] * emission_probabilities.loc[emis_index, \" \"+ words[i]]\n            \n            else:#otherwise get the current transition and emission:\n                trans_index = trans_dict[seq[i-1]]\n                emis_index = emis_dict[seq[i]]\n                prob += transisition_probabilities.loc[trans_index, \" \"+ seq[i]] * emission_probabilities.loc[emis_index, \" \"+ words[i]]\n\n        all_possible_tags_probability_dict.update({index : prob})#add the probabilities to the dict\n        index += 1\n\n    values_array = [*all_possible_tags_probability_dict.values()]#get the values of the probability dict in a new array\n\n    max_index = max((v, i) for i, v in enumerate(values_array) )[1]#get the index of the max probability\n    most_probable_tag_seq_dict.update({\"tag_sequence\": all_possible_tags_dict[max_index],\"prob\" : all_possible_tags_probability_dict[max_index] })\n\n    return(all_possible_tags_probability_dict, most_probable_tag_seq_dict)\n\n\nemission_file_path = \"emission_probabilities.csv\"\nemission_probabilities = pd.read_csv(emission_file_path)\n\ntransisition_file_path = \"transition_probabilities.csv\"\ntransisition_probabilities = pd.read_csv(transisition_file_path)\n\n\nall_possible_tags_dict = all_possible_tags(\"i want to race\", emission_probabilities)\nall_possible_tags_probability(\"I want to race\", all_possible_tags_dict, transisition_probabilities, emission_probabilities)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Jan_Rogge.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transisition_probabilities, emission_probabilities):\n\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    return(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Jan_Rogge.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**Answer:** The probability can be calculated by multiplying over all emission and transition probabilities: \n$$\n\\begin{align*}\n    p(y|x) &= \\Pi p(y_i | x_i)\\\\ \n    &= p(y=sunny) * p(y=dry| x=sunny) * p(y=cloudy| sunny) * p(y=wet| x=cloudy)* p(y=rainy|x=cloudy) * p(y=wet| x=rainy) * p(y=sunny|x=rainy) * p(y=dry| x=sunny) \\\\\n    &= 0.4 * 0.9 * 0.2 * 0.4 * 0.2 * 0.8 * 0.1 * 0.1 = 4.1472 * 10^{-4}\n\\end{align*}\n$$\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.\n**Answer:**: The model would still be able to transition between different weather in an expected manor. The main change would be that the model would have difficulty modelling whether the days are wet or dry on a sunny day. All sequences including sunny days would therefore have vastly different probabilities as can be seen when we recalculate the values of **1)**:\n\\begin{align*}\n    p(y|x) &= \\Pi p(y_i | x_i)\\\\ \n    &= 0.4 * 0.5 * 0.2 * 0.4 * 0.2 * 0.8 * 0.1 * 0.5 = 1.28 * 10^{-4}\n\\end{align*}\n$$",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Gjergj_Plepi.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd\n\"\"\"Reading the files\"\"\"\ntransition_probabilities = pd.read_csv('transition_probabilities.csv')\nemission_probabilities = pd.read_csv('emission_probabilities.csv')\ntransition_probabilities.columns = transition_probabilities.columns.str.replace(' ', '') #remove spaces from column names\ntransition_probabilities\nemission_probabilities.columns = emission_probabilities.columns.str.replace(' ', '') #remove spaces from column names\nemission_probabilities\nimport numpy as np\nimport itertools\n\ndef all_possible_tags(sentence, emission_probabilities):\n\n    all_possible_tags_dict = {}\n    #Here comes your code\n    words = sentence.split() # [I, want, to, race]\n\n    possible_tags_for_each_word = {} #will store each possible tag for each word. If prob (word|tag) = 0, word cant possibly have tag as a tag\n\n    \"\"\"If would consider all possible combinations, even those with probability 0, then we would get 4^4=256 possible combinations\"\"\"\n\n    for word in words:\n        possible_tags_for_each_word[word] = list(emission_probabilities['Unnamed:0'][np.where(emission_probabilities[word] > 0)[0]])\n    \n    combinations = list(itertools.product(*possible_tags_for_each_word.values())) #find all possible combinations\n\n    all_possible_tags_dict = {i+1: list(combination) for i, combination in enumerate(combinations)}\n\n\n    return(all_possible_tags_dict)\n\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    words = sentence.split() # [I, want, to, race]\n\n    most_probable_tag_seq_dict['tag_sequence'] = []\n    most_probable_tag_seq_dict['probability'] = 0\n   \n\n    tags = list(transition_probabilities['Unnamed:0']) # ['<s>', 'VB', 'TO', 'NN', 'PPSS']\n\n    for key, combination in all_possible_tags_dict.items():\n        seq_length = len(combination)\n\n        prob = 1\n\n        for i in range(seq_length):\n            if i == 0:    \n                transition = transition_probabilities[combination[i]][0]\n            else:\n                transition = transition_probabilities[combination[i]][tags.index(combination[i-1])]\n            \n            emission = emission_probabilities[words[i]][tags.index(combination[i])-1]\n\n            prob = prob * transition * emission\n        \n\n        all_possible_tags_probability_dict[key] = prob\n        \n\n        if prob > most_probable_tag_seq_dict['probability']:\n            most_probable_tag_seq_dict['probability']  = prob\n            most_probable_tag_seq_dict['tag_sequence'] = combination\n                \n\n    return(all_possible_tags_probability_dict, most_probable_tag_seq_dict)\nsentence = \"I want to race\"\npossible_tags = all_possible_tags(sentence, emission_probabilities) \nprint(possible_tags)\n\nall_possible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(sentence, possible_tags, transition_probabilities, emission_probabilities)\nprint(all_possible_tags_probability_dict)\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Gjergj_Plepi.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transition_probabilities, emission_probabilities):\n\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    words = sentence.split() # [I, want, to, race]\n    tags = list(emission_probabilities['Unnamed:0']) # ['VB', 'TO', 'NN', 'PPSS']\n\n    viterbi_matrix = np.zeros( (len(tags), len(words)) ) # 4 x 4, rows -> hidden states (POS tags) ; columns -> observations (words)\n    backpointer = np.zeros( (len(tags), len(words)) )\n\n    \"\"\"Intialization step -> filling the first column\"\"\"\n    for i in range(viterbi_matrix.shape[0]): #for each state/tag\n        viterbi_matrix[i, 0] = transition_probabilities[tags[i]][0] * emission_probabilities[words[0]][i]\n        backpointer[i, 0] = 0\n    \n    \"\"\"Recursion step\"\"\"\n    for j in range(1, viterbi_matrix.shape[1]): #for the remaining observations/words\n        for i in range(viterbi_matrix.shape[0]): #for each state/tag\n            max_prob = 0\n            max_prob_index = -1\n            for k in range(viterbi_matrix.shape[0]):\n                prob = viterbi_matrix[k, j-1] * transition_probabilities[tags[i]][k+1] * emission_probabilities[words[j]][i]\n                if prob > max_prob:\n                    max_prob = prob\n                    max_prob_index = k\n            \n            viterbi_matrix[i, j] = max_prob\n            backpointer[i, j] = max_prob_index\n    \n    \"\"\"Termination step\"\"\"\n    bestpath_prob = np.max(viterbi_matrix[:, -1])\n    bestpath_pointer = np.argmax(viterbi_matrix[:, -1])\n\n    most_probable_tag_seq_dict['tag_sequence'] = []\n    most_probable_tag_seq_dict['probability'] = bestpath_prob\n\n    #store in the path the last max state/tag\n    most_probable_tag_seq_dict['tag_sequence'].append( tags[bestpath_pointer] )\n    \n    prev_state_index = backpointer[bestpath_pointer, -1]\n\n    for i in range(backpointer.shape[1]-1, 0, -1): # i -> 3, 2, 1\n        most_probable_tag_seq_dict['tag_sequence'].insert(0, tags[ int(prev_state_index) ] )\n        prev_state_index = backpointer[int(prev_state_index), i-1]\n\n\n    return(most_probable_tag_seq_dict)\nsentence = \"I want to race\"\n\nmost_probable_tag_seq_dict = viterbi(sentence, transition_probabilities, emission_probabilities)\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Gjergj_Plepi.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": "from IPython.display import Image\nImage(filename='nlp_assignment5_task_3_1.png')\nImage(filename='nlp_assignment5_task_3_2.png')"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Ali_Sehran.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import itertools\ndef all_possible_tags(sentence, emission_probabilities):\n    # Split the sentence into words\n    words = sentence.split()\n    word_count = len(words)\n\n    all_tags = [emission_probabilities['Unnamed: 0']]*word_count\n  \n    # Generate all possible combinations of tags for the words in the sentence\n    all_combinations = list(itertools.product(*all_tags))\n    \n    # Convert combinations to a dictionary with an index as the key\n    all_possible_tags_dict = {str(index+1): combination for index, combination in enumerate(all_combinations)}\n    \n    return all_possible_tags_dict\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):  \n    words = sentence.split()\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    # Transpose transition_probabilities and consider the the first row as keys\n    transition_probabilities = transition_probabilities.set_index(transition_probabilities['Unnamed: 0']).transpose()\n    emission_probabilities = emission_probabilities.set_index(emission_probabilities['Unnamed: 0']).transpose()\n    \n    for key, value in all_possible_tags_dict.items():\n        probability = 1.0\n        for index, tag in enumerate(value):\n            if index == 0:\n                probability *= transition_probabilities['<s>'][tag]\n            else:\n                probability *= transition_probabilities[value[index-1]][tag]\n            probability *= emission_probabilities[tag][words[index]]\n        all_possible_tags_probability_dict[key] = probability\n\n        if probability > most_probable_tag_seq_dict.get(\"probability\", 0):\n            most_probable_tag_seq_dict[\"tag_sequence\"] = list(value)\n            most_probable_tag_seq_dict[\"probability\"] = probability\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\nimport pandas as pd\n\n# Load the transition and emission probabilities\ntransition_probabilities = pd.read_csv('./transition_probabilities.csv',)\ntransition_probabilities.columns = transition_probabilities.columns.str.strip()\n# Ignore spaces in the column names\nemission_probabilities = pd.read_csv('./emission_probabilities.csv')\nemission_probabilities.columns = emission_probabilities.columns.str.strip()\nsentence = 'I want to race'\nall_possible_tags_dict = all_possible_tags(sentence, emission_probabilities)\nall_possible_tags_probability_dict, most_probable_tag_seq = all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities)\nmost_probable_tag_seq"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Ali_Sehran.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "import numpy as np\n\ndef viterbi(sentence, transition_probabilities, emission_probabilities):\n    # Transpose transition_probabilities and emission  and consider the the first row as keys\n    transition_probabilities = transition_probabilities.set_index(transition_probabilities['Unnamed: 0']).transpose()\n    emission_probabilities = emission_probabilities.set_index(emission_probabilities['Unnamed: 0']).transpose()\n    words = sentence.split()\n    states = transition_probabilities.index.to_list()[1:]\n    num_states = len(states)\n    num_words = len(words)\n\n    # Initialize matrices\n    viterbi_matrix = np.zeros((num_states, num_words))\n    backpointer_matrix = np.zeros((num_states, num_words), dtype=int)\n\n    # Initialization step\n    for s in range(num_states):\n        viterbi_matrix[s, 0] = transition_probabilities['<s>'][states[s]] * emission_probabilities[states[s]][words[0]]\n        backpointer_matrix[s, 0] = 0\n\n    # Recursion step\n    for t in range(1, num_words):\n        for s in range(num_states):\n            viterbi_matrix[s, t] = max(viterbi_matrix[s2, t-1] * transition_probabilities[states[s2]][states[s]] * emission_probabilities[states[s]][words[t]] for s2 in range(num_states))\n            backpointer_matrix[s, t] = np.argmax([viterbi_matrix[s2, t-1] * transition_probabilities[states[s2]][states[s]] for s2 in range(num_states)])\n\n    # Termination step\n    best_path_prob = max(viterbi_matrix[s, num_words-1] for s in range(num_states))\n    best_path_pointer = np.argmax([viterbi_matrix[s, num_words-1] for s in range(num_states)])\n\n    # Path reconstruction\n    best_path = [states[best_path_pointer]]\n    for t in range(num_words-1, 0, -1):\n        best_path_pointer = backpointer_matrix[best_path_pointer, t]\n        best_path.insert(0, states[best_path_pointer])\n\n    most_probable_tag_seq_dict = {\n        \"tag_sequence\": best_path,\n        \"probability\": best_path_prob\n    }\n\n    return most_probable_tag_seq_dict\nviterbi(sentence, transition_probabilities, emission_probabilities )"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Ali_Sehran.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": "# Approach Followed in the solution is inspired from the following link\n# https://www.youtube.com/watch?v=RWkHJnFj5rY"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Ali_Sehran.ipynb",
        "question": "### Code implementation for confirming the above answer",
        "answer": "initial_probabilities = {'Sunny': 0.4, 'Cloudy': 0.3, 'Rainy': 0.3}\ntransition_probabilities = {\n    'Sunny': {'Sunny': 0.7, 'Cloudy': 0.2, 'Rainy': 0.1},\n    'Cloudy': {'Sunny': 0.3, 'Cloudy': 0.5, 'Rainy': 0.2},\n    'Rainy': {'Sunny': 0.1, 'Cloudy': 0.4, 'Rainy': 0.5}\n}\nemission_probabilities = {\n    'Sunny': {'Dry': 0.9, 'Wet': 0.1},\n    'Cloudy': {'Dry': 0.6, 'Wet': 0.4},\n    'Rainy': {'Dry': 0.2, 'Wet': 0.8}\n}\n# Hidden states and observations\nhidden_states = ['Sunny', 'Cloudy', 'Rainy', 'Sunny']\nobservations = ['Dry', 'Wet', 'Wet', 'Dry']\n# Calculate the probability of the first observation being in each state\nprob = initial_probabilities[hidden_states[0]] \n\n# Multiply by the emission probability of the first observation\nprob *= emission_probabilities[hidden_states[0]][observations[0]]\n\n# Multiply by the transition and emission probabilities for the rest of the sequence\nfor i in range(1, len(hidden_states)):\n    prob *= (transition_probabilities[hidden_states[i-1]][hidden_states[i]] *\n             emission_probabilities[hidden_states[i]][observations[i]])\nprint('Probility:', prob)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Ali_Sehran.ipynb",
        "question": "### 2\nChange in probabilities for wet and dry on sunny day will affect the information. Such that now less information will be there while determining the hidden state as 'dry' or 'wet' observable state won't infer anything strongly (in case of sunny). As for the above task 1, the model will be less certain (will lower the probability) for the sequence to occur.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Weiberg.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd\nimport itertools\n\ndef all_possible_tags(sentence, emission_probabilities):\n\n    emi_probs = pd.read_csv(emission_probabilities, index_col=0)\n    emi_probs.rename(columns={x: x.strip() for x in emi_probs.columns.values}, inplace=True)\n    words = sentence.split()\n    # print(emi_probs[0])\n    \n    result = []\n    for word in words:\n        p = emi_probs[word]\n        sublist = []\n        for i, x in enumerate(p):\n            if x > 0.0:\n                sublist.append(emi_probs.index[i])\n        result.append(sublist)\n    result = list(itertools.product(*result))\n    return {str(i): result[i] for i in range(len(result))}\n\ntags = (all_possible_tags(\"I want to race\", \"./emission_probabilities.csv\"))\n\n# def all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n\n#     all_possible_tags_probability_dict = {}\n#     most_probable_tag_seq_dict = {}\n    \n#     emi_probs = pd.read_csv(emission_probabilities, index_col=0)\n#     emi_probs.rename(columns={x: x.strip() for x in emi_probs.columns.values}, inplace=True)\n#     trans_probs = pd.read_csv(transisition_probabilities, index_col=0)\n#     trans_probs.rename(columns={x: x.strip() for x in trans_probs.columns.values}, inplace=True)\n#     words = sentence.split()\n\n# #     probs = trans_probs.iloc[0] #Starting probability\n    \n#     result = []\n#     for word in words:\n#         p = emi_probs[word]\n#         sublist = []\n#         for i, x in enumerate(p):\n#             if x > 0.0:\n#                 sublist.append(emi_probs.index[i], x)\n#         result.append(sublist)\n#     result = list(itertools.product(*result))\n#     return {str(i): result[i] for i in range(len(result))}\n\n#     return(possible_tags_probability_dict, most_probable_tag_seq_dict)\n\n# print(all_possible_tags_probability(\"I want to race\", tags, \"./transition_probabilities.csv\", \"./emission_probabilities.csv\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Weiberg.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "import numpy as np\n\ndef viterbi(sentence, transisition_probabilities, emission_probabilities):\n\n    most_probable_tag_seq_dict = {}\n    emi_probs = pd.read_csv(emission_probabilities, index_col=0)\n    emi_probs.rename(columns={x: x.strip() for x in emi_probs.columns.values}, inplace=True)\n    trans_probs = pd.read_csv(transisition_probabilities, index_col=0)\n    trans_probs.rename(columns={x: x.strip() for x in trans_probs.columns.values}, inplace=True)\n\n    words = sentence.split()\n\n    K = len(trans_probs.columns)\n    T = len(words)\n\n    T1 = np.zeros((K, T))\n    T2 = np.zeros((K, T))\n\n    T1[:, 0] = trans_probs.iloc[0]\n\n\n    for j in range(1, T):\n        for i in range(0, K):\n            T1[i, j] = np.max(T1[:, j-1] @ trans_probs.to_numpy()[1:, i] * emi_probs[words[j]])\n            T2[i, j] = np.argmax(T1[:, j-1] @ trans_probs.to_numpy()[1:, i] * emi_probs[words[j]])\n\n    zT = np.argmax(T1[:, T-1])\n    \n    sequence = []\n    sequence.insert(0, trans_probs.columns[zT])\n    for j in range(T, 1, -1):\n        zT = int(T2[zT, j-1])\n        sequence.insert(0, trans_probs.columns[zT])\n    return {\"tag_sequence\": sequence, \"probability\": np.max(T1[:, T-1])}\n\nprint(viterbi(\"I want to race\", \"./transition_probabilities.csv\", \"./emission_probabilities.csv\"))"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Weiberg.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd\nfrom itertools import product\n\ndef all_possible_tags(sentence, emission_probabilities):\n    \"\"\"\n    Generate all possible POS tag sequences for a given sentence,\n    \n    Args:\n    sentence (str)\n    emission_probabilities (pd.DataFrame)\n    \n    Returns:\n    dict: A dictionary with indices as keys and lists of POS tags as values.\n    \"\"\"\n    words = sentence.split()\n    possible_tags_per_word = []\n\n    for word in words:\n        # Check if word is in the DataFrame columns\n        if word in emission_probabilities.columns:\n            # Filter POS tags with more than zero probability\n            possible_tags = emission_probabilities.index[emission_probabilities[word] > 0].tolist()\n        else:\n            # skip if it's not found\n            continue\n\n        possible_tags_per_word.append(possible_tags)\n\n    # Generate all combinations of POS tags\n    all_combinations = list(product(*possible_tags_per_word))\n    all_combinations_dict = {index + 1: list(comb) for index, comb in enumerate(all_combinations)}\n\n    return all_combinations_dict\n\ndef all_possible_tags_probability(sentence, all_possible_tags, transition_probabilities, emission_probabilities):\n    \"\"\"\n    Calculate the probability for each possible tag sequence and find the most probable tag sequence, refactored.\n\n    Args:\n    sentence (str);\n    all_possible_tags (dict);\n    emission_probabilities (pd.DataFrame);\n    transition_probabilities (pd.DataFrame);\n\n    Returns:\n    dict, dict: A dictionary of probabilities for each tag sequence and the most probable tag sequence with its probability.\n    \"\"\"\n    words = sentence.split()\n    all_possible_tags_probability_dict = {}\n    max_probability = 0\n    most_probable_tag_seq = None\n\n    for seq_index, tag_sequence in all_possible_tags.items():\n        probability = 1\n        prev_tag = \"<s>\"  # Start symbol\n\n        for i, tag in enumerate(tag_sequence):\n            word = words[i]\n            \n            # Calculate emission probability, default to a small value if not found\n            emission_prob = emission_probabilities.loc[tag, word] if word in emission_probabilities.columns and tag in emission_probabilities.index else 1e-10\n\n            # Calculate transition probability, default to a small value if not found\n            transition_prob = transition_probabilities.loc[prev_tag, tag] if prev_tag in transition_probabilities.index and tag in transition_probabilities.columns else 1e-10\n\n            # Update the sequence probability\n            probability *= emission_prob * transition_prob\n\n            prev_tag = tag\n\n        all_possible_tags_probability_dict[seq_index] = probability\n\n        # Update the most probable sequence if current one has a higher probability\n        if probability > max_probability:\n            max_probability = probability\n            most_probable_tag_seq = tag_sequence\n\n    # Format the most probable tag sequence and its probability\n    most_probable_tag_seq_dict = {\"tag_sequence\": most_probable_tag_seq, \"probability\": max_probability}\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\n# Example usage\ntransition_probabilities_df = pd.read_csv('transition_probabilities.csv')\nemission_probabilities_df = pd.read_csv('emission_probabilities.csv')\n\ntransition_probabilities_df.set_index('Unnamed: 0', inplace=True)\nemission_probabilities = emission_probabilities_df.set_index('Unnamed: 0')\n\n# Rename the index for clarity\nemission_probabilities.index.name = 'POS'\n\n# Trimming spaces from the DataFrame's column names\nemission_probabilities.columns = emission_probabilities.columns.str.strip()\n\nsentence = \"I want to race\"\nall_tags_dict = all_possible_tags(sentence, emission_probabilities)\nprobabilities_dict, most_probable_tag_seq_dict = all_possible_tags_probability(\n   sentence, all_tags_dict, transition_probabilities_df, emission_probabilities)\n\nprint(all_tags_dict)\nprint(probabilities_dict)\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Muslimbek_Abduvaliev.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "import numpy as np\n\ndef viterbi(sentence, transition_probabilities, emission_probabilities):\n    \"\"\"\n    Viterbi Algorithm according to the steps provided in the image.\n    \n    Args:\n    sentence (str)\n    transition_probabilities (pd.DataFrame)\n    emission_probabilities (pd.DataFrame)\n    \n    Returns:\n    list, float: The most probable POS tag sequence and its probability.\n    \"\"\"\n    # Initialization\n    states = emission_probabilities.index\n    T = len(sentence.split())\n    V = np.zeros((len(states), T))\n    backpointer = np.zeros((len(states), T), dtype=int)\n    \n    # First observation initialization\n    for s, state in enumerate(states):\n        V[s, 0] = transition_probabilities.loc['<s>', state] * emission_probabilities.loc[state, sentence.split()[0]]\n        backpointer[s, 0] = 0\n\n    # Recursion\n    for t in range(1, T):\n        for s, state in enumerate(states):\n            prob, backpointer_state = max(\n                [(V[s_prime, t - 1] * transition_probabilities.loc[states[s_prime], state] * emission_probabilities.loc[state, sentence.split()[t]], s_prime)\n                 for s_prime in range(len(states))],\n                key=lambda x: x[0]\n            )\n            V[s, t] = prob\n            backpointer[s, t] = backpointer_state\n\n    # Termination\n    bestpathprob = max(V[:, T - 1])\n    bestpathpointer = np.argmax(V[:, T - 1])\n\n    # Path backtracking\n    bestpath = [states[bestpathpointer]]\n    for t in range(T - 1, 0, -1):\n        bestpathpointer = backpointer[bestpathpointer, t]\n        bestpath.insert(0, states[bestpathpointer])\n\n    return bestpath, bestpathprob\n# Re-reading the transition probabilities from the uploaded CSV file\ntransition_probs_path = 'transition_probabilities.csv'\ntransition_probs_df = pd.read_csv(transition_probs_path, index_col=0)\n\n# Re-reading the emission probabilities from the uploaded CSV file\nemission_probs_path = 'emission_probabilities.csv'\nemission_probs_df = pd.read_csv(emission_probs_path)\n\n# The emission DataFrame needs the first column to be set as the index.\nemission_probs_df.set_index(emission_probs_df.columns[0], inplace=True)\nemission_probs_df.index.name = 'POS'\n\n# Trim spaces from the DataFrame's column names and index\ntransition_probs_df.columns = transition_probs_df.columns.str.strip()\ntransition_probs_df.index = transition_probs_df.index.str.strip()\nemission_probs_df.columns = emission_probs_df.columns.str.strip()\n\n# Run the Viterbi algorithm with the given sentence and probabilities\nviterbi_result = viterbi(\"I want to race\", transition_probs_df, emission_probs_df)\nviterbi_result"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Muslimbek_Abduvaliev.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd\nfrom itertools import product\n\n# Getting data from .csv files\nemission_probabilities = pd.read_csv('emission_probabilities.csv',index_col=[0])\nemission_probabilities.columns = emission_probabilities.columns.str.strip()\ntransition_probabilities = pd.read_csv('transition_probabilities.csv',index_col=[0])\ntransition_probabilities.columns = transition_probabilities.columns.str.strip()\n\n\ndef all_possible_tags(sentence, emission_probabilities):\n    words = sentence.split()\n    all_tags = []\n\n    # Extract possible tags for each word\n    for word in words:\n        tags = emission_probabilities.loc[emission_probabilities[word] > 0].index.tolist()\n        all_tags.append(tags)\n    # print(all_tags)\n    \n    # List all combinations of tags\n    all_tag_combinations = list(product(*all_tags))\n    \n    all_possible_tags_dict = {str(i+1): list(comb) for i, comb in enumerate(all_tag_combinations)}\n    \n    return all_possible_tags_dict\n\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):\n    words = sentence.split()\n    all_possible_tags_probability_dict = {}\n    max_probability = 0\n    most_probable_tag_seq = None\n\n    # Calculate the probability for each tag sequence\n    for sequence_id, tags in all_possible_tags_dict.items():\n        probability = 1\n        prev_tag = '<s>'\n\n        for i, tag in enumerate(tags):\n            # Both Transition probability and Emission probability\n            transition_prob = transition_probabilities.loc[prev_tag, tag]\n            emission_prob = emission_probabilities.loc[tag, words[i]]\n\n            probability *= (transition_prob * emission_prob)\n            prev_tag = tag\n\n        all_possible_tags_probability_dict[sequence_id] = probability\n\n        # Does this sequence have the highest probability \n        if probability > max_probability:\n            max_probability = probability\n            most_probable_tag_seq = tags\n\n    # output : the most probable tag sequence\n    most_probable_tag_seq_dict = {'tag_sequence': most_probable_tag_seq, 'probability': max_probability}\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\n\nsentence = \"I want to race\"\nall_tags_dict = all_possible_tags(sentence, emission_probabilities)\nall_tags_prob_dict, most_probable_tag_seq = all_possible_tags_probability(sentence, all_tags_dict, transition_probabilities, emission_probabilities)\n\nprint('all_tags_dict : ', all_tags_dict) \nprint('all_tags_prob_dict :',all_tags_prob_dict )\nprint('most_probable_tag_seq : ' , most_probable_tag_seq)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "import numpy as np\n\ndef viterbi(sentence, transition_probabilities, emission_probabilities):\n    # Split sentence ti words\n    words = sentence.split()\n    states = list(emission_probabilities.index)\n\n    viterbi_score = np.zeros((len(states), len(words)))\n    path = np.zeros((len(states), len(words)), dtype=int)\n\n    # Initialization step\n    for i, state in enumerate(states):\n        viterbi_score[i, 0] = transition_probabilities.loc[\"<s>\", state] * emission_probabilities.loc[state, words[0]]\n\n    # Recursion step\n    for t in range(1, len(words)):  \n        for i, state in enumerate(states):\n            scores_with_transitions = viterbi_score[:, t-1] * transition_probabilities.loc[states, state].values * emission_probabilities.loc[state, words[t]]\n            viterbi_score[i, t] = np.max(scores_with_transitions)\n            path[i, t] = np.argmax(scores_with_transitions)\n\n    # Termination step\n    max_final_score = np.max(viterbi_score[:, -1])\n    final_state = np.argmax(viterbi_score[:, -1])\n    \n    # Backtrack to find the most probable path\n    most_probable_path = [final_state]\n    for t in range(len(words) - 1, 0, -1):\n        most_probable_path.insert(0, path[most_probable_path[0], t])\n\n    # Convert state indices to state names\n    most_probable_tag_sequence = [states[state_index] for state_index in most_probable_path]\n\n    # Format the output\n    most_probable_tag_seq_dict = {'tag_sequence': most_probable_tag_sequence, 'probability': max_final_score}\n    \n    return most_probable_tag_seq_dict\n\n\nsentence = \"I want to race\"\nmost_probable_tag_seq_dict = viterbi(sentence, transition_probabilities, emission_probabilities)\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "##Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Fatemeh_Movassaghalehnagh.ipynb",
        "question": "### 1) Total probability\ntotal_probability = (initial_prob_sunny * emit_sunny_dry * trans_sunny_to_cloudy * emit_cloudy_wet * \n                     trans_cloudy_to_rainy * emit_rainy_wet * trans_rainy_to_sunny * emit_sunny_dry)\ntotal_probability = 0.00041472\n### 2)\nSince 'Dry' and 'Wet' have the same emission probabilities when it's Sunny, the model HMM becomes less efficient and could not figure out the weather state from seeing 'Dry' or 'Wet'. Before, the model was more likely to predict 'Dry' (90% chance) in sunny weather. But changing both 'Dry' and 'Wet' chances to 50% each in sunny weather makes it harder to tell the weather from these predictions, as both are equally likely now.\nFor the sequence Dry, Wet, Wet, Dry in the Sunny, Cloudy, Rainy, Sunny, the probability will change. as the probability for 'Dry' and 'Wet' in the Sunny state is now equal (0.5). the overall probability will increase. This is because the original computation was significantly influenced by the high chance  (0.9) of for  'Dry' in the Sunny state.\"\nAlso, observing 'Wet' during a Sunny prediction would no longer significantly challenge the model's prediction, as it would have with the original emission probabilities.\nAnd in General the model become less accurate in predicting the actual weather conditions, because one of the key distinguishing features (the high probability of 'Dry' in Sunny conditions) has been changed.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Orlivskyi.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "from itertools import product\n\nsentence = \"I want to race\"\nwords = sentence.split()\n\ndef all_possible_tags(sentence, transtion_file_path, emission_file_path):\n    print('list(read_transition_probs_from_csv(transtion_file_path):', list(read_transition_probs_from_csv(transtion_file_path).keys()))\n    tags = list(read_transition_probs_from_csv(transtion_file_path).keys())\n    tags.remove('<s>')\n\n    words = sentence.split()\n    num_words = len(words)\n\n    print(tags)\n\n    # generate all combinations of tags\n    all_combinations = product(tags, repeat=num_words)\n\n    # convert combinations to a dictionary format\n    all_tags_dict = {}\n    for i, combination in enumerate(all_combinations, 1):\n        all_tags_dict[str(i)] = list(combination)\n\n    return all_tags_dict\n\ndef all_possible_tags_probability(tag_sequences, transtion_file_path, emission_file_path):\n    transition_probs = read_transition_probs_from_csv(transtion_file_path)\n    emission_probs = read_emission_probs_from_csv(emission_file_path)\n    \n    words = list(emission_probs.keys())\n\n    probabilities_dict = {}\n    max_prob = 0\n    most_probable_seq = None\n\n    for seq_id, sequence in tag_sequences.items():\n        prob = calculate_sequence_probability(sequence, words, transition_probs, emission_probs)\n\n        probabilities_dict[seq_id] = prob\n\n        if prob > max_prob:\n            max_prob = prob\n            most_probable_seq = sequence\n\n    return probabilities_dict, {'tag_sequence': most_probable_seq, 'probability': max_prob}\n\ndef calculate_sequence_probability(sequence, words, transition_probs, emission_probs):\n    probability = 1.0\n\n    # init probability\n    start_tag = sequence[0]\n    probability *= transition_probs['<s>'].get(start_tag)\n\n    # multiply by emission probability of the first word\n    probability *= emission_probs[words[0]].get(start_tag)\n\n    # iterate over remaining tags\n    for i in range(1, len(sequence)):\n        prev_tag = sequence[i - 1]\n        current_tag = sequence[i]\n        word = words[i]\n\n        # multiply by the transition probability from the previous tag to the current\n        probability *= transition_probs[prev_tag].get(current_tag)\n\n        # multiply by the emission probability of the current word\n        probability *= emission_probs[word].get(current_tag)\n\n    return probability\n\nimport csv\nfrom pprint import pprint\n\ndef read_emission_probs_from_csv(file_path):\n    emission_probs = {}\n\n    with open(file_path, 'r') as csvfile:\n        reader = csv.reader(csvfile)\n        # read the first row, skip first cell\n        words = next(reader)[1:]\n\n        for row in reader:\n            if not row:\n                continue\n            # the first cell in each row represents a tag\n            tag = row[0].strip()\n            # strings to floats\n            probabilities = [float(p.strip()) for p in row[1:]]\n\n            for word, prob in zip(words, probabilities):\n                if word not in emission_probs:\n                    emission_probs[word.strip()] = {}\n                emission_probs[word.strip()][tag.strip()] = prob\n\n    return emission_probs\n\ndef read_transition_probs_from_csv(file_path):\n    transition_probs = {}\n\n    with open(file_path, 'r') as csvfile:\n        reader = csv.reader(csvfile)\n        # read the first row, skip first cell\n        tags = next(reader)[1:]\n\n        for row in reader:\n            if not row:\n                continue\n            # the first cell in each row represents a tag\n            current_tag = row[0].strip()\n            # strings to floats\n            probabilities = [float(p.strip()) for p in row[1:]]\n\n            transition_probs[current_tag.strip()] = {tag.strip(): prob for tag, prob in zip(tags, probabilities)}\n\n    return transition_probs\n\n\ntranstion_file_path = 'transition_probabilities.csv'\ntransition_probs = read_transition_probs_from_csv(transtion_file_path)\npprint(transition_probs)\n\n\nemission_file_path = 'emission_probabilities.csv'\nemission_probs = read_emission_probs_from_csv(emission_file_path)\npprint(emission_probs)\n\n\n\nall_tags = all_possible_tags(sentence, transtion_file_path, emission_file_path)\n\nprint(\"All possible tags:\", all_tags)\n\nprobabilities, most_probable = all_possible_tags_probability(all_tags, transtion_file_path, emission_file_path)\n\nprint(\"All tag sequences with probabilities:\", probabilities)\nprint(\"Most probable tag sequence:\", most_probable)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Orlivskyi.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "sentence = 'I want to race'\n\nimport numpy as np\n\ndef viterbi(sentence, transisition_probabilities, emission_probabilities):\n    words = sentence.split()\n    observations = words\n    states = list(transisition_probabilities.keys())\n    start_prob = transisition_probabilities['<s>']\n    states.remove('<s>')\n    # number of states\n    N = len(states)\n    # number of observations\n    T = len(observations)\n    \n    # path probability matrix viterbi[N,T]\n    viterbi = np.zeros((N, T))\n    # backpointer to the best path\n    backpointer = np.zeros((N, T), dtype=int)\n    \n    # initialization step\n    for s in range(N):\n        viterbi[s, 0] = start_prob[states[s]] * emission_probabilities[observations[0]][states[s]]\n        backpointer[s, 0] = 0\n    \n    # recursion step\n    for t in range(1, T):\n        for s in range(N):\n            prob_max, state_max = max(\n                (viterbi[s_prev, t-1] * transisition_probabilities[states[s_prev]][states[s]] * emission_probabilities[observations[t]][states[s]], s_prev)\n                for s_prev in range(N)\n            )\n            viterbi[s, t] = prob_max\n            backpointer[s, t] = state_max\n    \n    # termination step\n    best_path_prob = max(viterbi[:, T-1])\n    best_path_pointer = np.argmax(viterbi[:, T-1])\n    \n    # path reconstruction step\n    best_path = [states[best_path_pointer]]\n    for t in range(T-1, 0, -1):\n        best_path_pointer = backpointer[best_path_pointer, t]\n        best_path.insert(0, states[best_path_pointer])\n    \n    return {'tag_sequence': best_path, 'probability': best_path_prob}\n\n\n\ntranstion_file_path = 'transition_probabilities.csv'\ntransition_probs = read_transition_probs_from_csv(transtion_file_path)\n# pprint(transition_probs)\n\n\nemission_file_path = 'emission_probabilities.csv'\nemission_probs = read_emission_probs_from_csv(emission_file_path)\n# pprint(emission_probs)\n\nresult = viterbi(sentence, transition_probs, emission_probs)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Orlivskyi.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Jing_Wu.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "def all_possible_tags(sentence, emission_probabilities):\n\n    all_possible_tags_dict = {}\n    #Here comes your code\n\n    return(all_possible_tags_dict)\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    return(possible_tags_probability_dict, most_probable_tag_seq_dict)\nimport pandas as pd\n\ndef all_possible_tags(sentence, emission_probabilities):\n    all_possible_tags_dict = {}\n    \n    # Tokenize the sentence\n    words = sentence.split()\n    num_words = len(words)\n    \n    # Generate all possible tag sequences using recursion\n    def generate_tag_sequences(index):\n        if index == num_words:\n            return [[]]\n        \n        sequences = []\n        for tag in emission_probabilities.index:\n            next_sequences = generate_tag_sequences(index + 1)\n            for seq in next_sequences:\n                sequences.append([tag] + seq)\n        \n        return sequences\n    \n    tag_sequences = generate_tag_sequences(0)\n    \n    # Map tag sequences to their probabilities\n    for i, tags in enumerate(tag_sequences, start=1):\n        probability = 1.0\n        for j, word in enumerate(words):\n            try:\n                probability *= emission_probabilities.loc[tags[j], word]\n            except KeyError:\n                # Handle the case where the word is not in the emission probabilities DataFrame\n                probability = 0.0\n                break\n        all_possible_tags_dict[str(i)] = tags\n    \n    return all_possible_tags_dict\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {'tag_sequence': None, 'probability': 0.0}\n    \n    for key, tags in all_possible_tags_dict.items():\n        probability = 1.0\n        for i in range(len(tags)):\n            if i == 0:\n                try:\n                    probability *= emission_probabilities.loc[tags[i], sentence.split()[i]]\n                except KeyError:\n                    # Handle the case where the word is not in the emission probabilities DataFrame\n                    probability = 0.0\n                    break\n            else:\n                try:\n                    probability *= transition_probabilities.loc[tags[i-1], tags[i]] * emission_probabilities.loc[tags[i], sentence.split()[i]]\n                except KeyError:\n                    # Handle the case where the word is not in the emission probabilities DataFrame\n                    probability = 0.0\n                    break\n        \n        all_possible_tags_probability_dict[key] = probability\n        \n        # Update most probable tag sequence\n        if probability > most_probable_tag_seq_dict['probability']:\n            most_probable_tag_seq_dict['tag_sequence'] = tags\n            most_probable_tag_seq_dict['probability'] = probability\n    \n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\n\n# Load emission and transition probabilities from CSV files\nemission_probabilities = pd.read_csv('emission_probabilities.csv', index_col=0).T  # Transpose the DataFrame\ntransition_probabilities = pd.read_csv('transition_probabilities.csv', index_col=0)\n\nsentence = \"I want to race\"\nall_possible_tags_dict = all_possible_tags(sentence, emission_probabilities)\nall_possible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities)\n\nprint(\"All Possible Tag Sequences:\")\nprint(all_possible_tags_dict)\n\nprint(\"\\nAll Possible Tag Sequences with Probabilities:\")\nprint(all_possible_tags_probability_dict)\n\nprint(\"\\nMost Probable Tag Sequence:\")\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Jing_Wu.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transisition_probabilities, emission_probabilities):\n\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    return(most_probable_tag_seq_dict)\nimport pandas as pd\nimport numpy as np\n\ndef viterbi(sentence, transition_probabilities, emission_probabilities):\n    # Create a list of unique tags\n    tags = list(transition_probabilities.columns.str.strip())\n    print(tags)\n    \n    # Split the sentence into words\n    words = sentence.split()\n\n    # Initialize matrices for Viterbi algorithm\n    n = len(tags)\n    m = len(words)\n    \n    print(emission_probabilities['VB'])\n    \n    \n    \n    viterbi_matrix = np.zeros((n, m))\n    backpointer_matrix = np.zeros((n, m), dtype=int)\n    \n#     # Initialize the first column of the Viterbi matrix\n#     for i, tag in enumerate(tags):\n#         viterbi_matrix[i, 0] = emission_probabilities[tag].loc[words[0]] * transition_probabilities[tag].loc['<s>']\n#     print(viterbi_matrix)\n        # Initialize the first column of the Viterbi matrix\n    for i, tag in enumerate(tags):\n        try:\n            viterbi_matrix[i, 0] = emission_probabilities[tag].loc[words[0]] * transition_probabilities.loc['<s>', tag]\n        except KeyError:\n            # Handle the case when the word or tag is not found in the emission/transition probabilities\n            viterbi_matrix[i, 0] = 0.0\n\n    print(\"Initialization Step:\")\n    print(viterbi_matrix)\n    # Implement the recursion step\n    for t in range(1, m):\n        for i, current_tag in enumerate(tags):\n            # Calculate the probabilities for each possible transition\n            transition_prob = np.array(\n                [viterbi_matrix[j, t - 1] * transition_probabilities[current_tag].loc[tags[j]] for j in range(n)]\n            )\n\n            # Find the maximum probability and store it in the Viterbi matrix\n            viterbi_matrix[i, t] = np.max(transition_prob) * emission_probabilities[current_tag].loc[words[t]]\n\n            # Update the backpointer matrix with the index of the maximum probability transition\n            backpointer_matrix[i, t] = np.argmax(transition_prob)\n\n    # Implement the termination step\n    best_last_tag_index = np.argmax(viterbi_matrix[:, -1])\n    best_path = [best_last_tag_index]\n\n    # Backtrack to find the best path\n    for t in range(m - 1, 0, -1):\n        best_path.append(backpointer_matrix[best_path[-1], t])\n\n    best_path.reverse()\n\n    # Calculate the probability of the best path\n    probability = np.max(viterbi_matrix[:, -1])\n\n    # Map tag indices to tag names\n    best_tag_sequence = [tags[i] for i in best_path]\n\n    # Create the output dictionary\n    most_probable_tag_seq_dict = {\"tag_sequence\": best_tag_sequence, \"probability\": probability}\n\n    return most_probable_tag_seq_dict\n\n# Example usage\ntransition_probabilities = pd.read_csv(\"transition_probabilities.csv\", index_col=0)\nemission_probabilities = pd.read_csv(\"emission_probabilities.csv\", index_col=0).T\n\nsentence = \"I want to race\"\nresult = viterbi(sentence, transition_probabilities, emission_probabilities)\nprint(result)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Jing_Wu.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": "import numpy as np\n\n# Define HMM parameters\nstates = ['Sunny', 'Cloudy', 'Rainy']\nobservations = ['Dry', 'Wet']\n\n# Transition probabilities\ntransition_matrix = np.array([\n    [0.7, 0.2, 0.1],\n    [0.3, 0.5, 0.2],\n    [0.1, 0.4, 0.5]\n])\n\n# Emission probabilities\nemission_matrix = np.array([\n    [0.9, 0.1],\n    [0.6, 0.4],\n    [0.2, 0.8]\n])\n\n# Initial probabilities\ninitial_probabilities = np.array([0.4, 0.3, 0.3])\n\n# Observation sequence\nobservation_sequence = ['Dry', 'Wet', 'Wet', 'Dry']\n\n# Hidden state sequence\nhidden_state_sequence = ['Sunny', 'Cloudy', 'Rainy', 'Sunny']\n\ndef forward_algorithm(observation_sequence, hidden_state_sequence, transition_matrix, emission_matrix, initial_probabilities):\n    n_states = len(states)\n    n_observations = len(observation_sequence)\n    \n    forward_matrix = np.zeros((n_states, n_observations))\n    \n    # Initialization\n    for i in range(n_states):\n        forward_matrix[i, 0] = initial_probabilities[i] * emission_matrix[i, observations.index(observation_sequence[0])]\n    \n    # Recursion\n    for t in range(1, n_observations):\n        for j in range(n_states):\n            forward_matrix[j, t] = np.sum(forward_matrix[i, t-1] * transition_matrix[i, j] * emission_matrix[j, observations.index(observation_sequence[t])] for i in range(n_states))\n    \n    # Termination\n    probability = np.sum(forward_matrix[i, -1] for i in range(n_states))\n    \n    return probability\n\n# Part 1\nprobability_part1 = forward_algorithm(observation_sequence, hidden_state_sequence, transition_matrix, emission_matrix, initial_probabilities)\nprint(f\"Part 1: Probability of observing 'Dry, Wet, Wet, Dry' given 'Sunny, Cloudy, Rainy, Sunny': {probability_part1:.6f}\")\n\n# Part 2: Modify emission probabilities for 'Dry' and 'Wet' in the 'Sunny' state\nemission_matrix[0, :] = 0.5  # Both 'Dry' and 'Wet' probabilities are set to 0.5 in the 'Sunny' state\n\n# Part 3: Recalculate probability with modified emission probabilities\nprobability_part2 = forward_algorithm(observation_sequence, hidden_state_sequence, transition_matrix, emission_matrix, initial_probabilities)\nprint(f\"Part 2: Probability of observing 'Dry, Wet, Wet, Dry' given 'Sunny, Cloudy, Rainy, Sunny' (with modified emissions): {probability_part2:.6f}\")\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "def all_possible_tags(sentence, emission_probabilities):\n\n    tags_dict = {}\n    \n    \n    #Here comes your code\n\n    return(tags_dict)\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    return(possible_tags_probability_dict, most_probable_tag_seq_dict)\n\n# sentence = \"I want to race\"\n\n# ep = pd.read_csv(\"emission_probabilities.csv\", header=0)\n# ep = ep.set_index(ep.columns[0])\n# print(ep)\n\n# tp = pd.read_csv(\"transition_probabilities.csv\", header=0)\n# tp = tp.set_index(tp.columns[0])\n# print(tp)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Kashan_U_Z_Khan.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transisition_probabilities, emission_probabilities):\n\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    return(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Kashan_U_Z_Khan.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.",
        "answer": "# display given image (Markdown code wasn't working)\n\nfrom IPython.display import Image\nImage(filename='HMM.png')"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Kashan_U_Z_Khan.ipynb",
        "question": "#### 1)\np1 = p(sunny|start) * p(dry|sunny) = 0.4 * 0.9 = 0.36\np2 = p(cloudy|sunny) * p(wet|cloudy) = 0.2 * 0.4 = 0.08\np3 = p(rainy|cloudy) * p(wet|rainy) = 0.2 * 0.8 = 0.16\np4 = p(sunny|rainy) * p(sunny|dry) = 0.1 * 0.9 = 0.09\nsequence_probability = p1 * p2 * p3 * p4 = 0.00041472",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Kashan_U_Z_Khan.ipynb",
        "question": "#### 2)\nIf Dry and Wet prob in the sunny state are both 0.5, the HMM would be able to model less information from the probabilities as both the events are equally likely.\nIn the case of the above sequence, the HMM model would use the observed sequence of the Wet and Dry conditions to predict the hidden states (weather). However with the Sunny weather being equally likely for both the conditions, predicting the weather would become a little inconclusive leading a less confident prediction of hidden states.",
        "answer": "\n"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Suraj_Giri_Vu_Duc_Manh.ipynb",
        "question": "### Import modules",
        "answer": "import numpy as np\nimport itertools"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Suraj_Giri_Vu_Duc_Manh.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "trasition_array = np.genfromtxt(\"./transition_probabilities.csv\", delimiter=\", \", dtype=str)\nemission_array = np.genfromtxt(\"./emission_probabilities.csv\", delimiter=\", \", dtype=str)\ndef create_probability_dict(array):\n  f = array[1:,0]\n  t = array[0,1:]\n\n  probabilities = dict()\n\n  for i in range(f.shape[0]):\n    f_label = f[i]\n    probabilities[f_label] = dict()\n    for j in range(t.shape[0]):\n      t_label = t[j]\n      probabilities[f_label][t_label] = array[i + 1, j + 1].astype(float)\n\n  return probabilities\ntransisition_probabilities = create_probability_dict(trasition_array)\nemission_probabilities = create_probability_dict(emission_array)\ndef calculate_probability(sentence, tags, transisition_probabilities, emission_probabilities):\n  prob = 1.0\n  previous_tag = \"<s>\"\n\n  for i in range(len(sentence)):\n    current_tag = tags[i]\n    current_word = sentence[i]\n    prob = prob * transisition_probabilities[previous_tag][current_tag] * emission_probabilities[current_tag][current_word]\n\n    previous_tag = current_tag\n\n  return prob\ndef all_possible_tags(sentence, emission_probabilities):\n    sentence = sentence.split(\" \")\n    l = len(sentence)\n\n    tags = list(emission_probabilities.keys())\n    list_of_tags = [tags,] * l\n    choices = list(itertools.product(*list_of_tags))\n\n    all_possible_tags_dict = {str(i + 1):list(item) for i, item in enumerate(choices)}\n    return(all_possible_tags_dict)\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n    sentence = sentence.split(\" \")\n    l = len(sentence)\n\n    all_possible_tags_probability_dict = {}\n\n    for idx in all_possible_tags_dict:\n      tags = all_possible_tags_dict[idx]\n      prob = calculate_probability(sentence, tags, transisition_probabilities, emission_probabilities)\n      all_possible_tags_probability_dict[idx] = prob\n\n    k, v = max(all_possible_tags_probability_dict.items(), key = lambda x: x[1])\n\n    most_probable_tag_seq_dict = {\n        \"tag_sequence\": all_possible_tags_dict[k],\n        \"probability\": v\n    }\n    #Here comes your code\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\nall_possible_tags_dict = all_possible_tags(\"I want to race\", emission_probabilities)\nprint(all_possible_tags_dict)\nall_possible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(\"I want to race\", all_possible_tags_dict, transisition_probabilities, emission_probabilities)\nprint(most_probable_tag_seq_dict)\nprint(all_possible_tags_probability_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Suraj_Giri_Vu_Duc_Manh.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def create_trasition_prob_matrix(transisition_probabilities):\n  n = len(transisition_probabilities) - 1\n  trasition_prob_matrix = np.zeros((n, n), dtype=float)\n  i = 0\n\n  for k in transisition_probabilities:\n    if k != '<s>':\n      trasition_prob_matrix[i] = np.array(list(transisition_probabilities[k].values()))\n      i += 1\n\n  return trasition_prob_matrix\ndef viterbi(sentence, transisition_probabilities, emission_probabilities):\n  sentence = sentence.split(\" \")\n  l = len(sentence)\n  n = len(emission_probabilities)\n  tags = list(emission_probabilities.keys())\n  trasition_prob_matrix = create_trasition_prob_matrix(transisition_probabilities)\n\n  # initialization step\n  probabilities = np.zeros((l, n), dtype=float)\n  backtrack_pointers = np.zeros((l, n), dtype=int)\n\n  probabilities[0] = np.array([transisition_probabilities['<s>'][tag] for tag in tags]) * np.array([emission_probabilities[tag][sentence[0]] for tag in tags])\n  backtrack_pointers[0] -= 1\n\n  # recursion step\n  for i in range(1, l):\n    current_word = sentence[i]\n\n    # calculate prob (n, n)\n    current_probabilities = probabilities[i - 1:i,:].T * trasition_prob_matrix * np.expand_dims(np.array([emission_probabilities[tag][current_word] for tag in tags], dtype=float), 0)\n\n    # get max prob and argmax prob\n    max_prob = np.max(current_probabilities, axis=0)\n    argmax_prob = np.argmax(current_probabilities, axis=0)\n\n    probabilities[i] = max_prob\n    backtrack_pointers[i] = argmax_prob\n\n  output_ids = [np.argmax(probabilities[-1]).item(),]\n\n  for i in range(l - 1, 0, -1):\n    current_idx = backtrack_pointers[i][output_ids[-1]].item()\n    output_ids.append(current_idx)\n\n  output_ids = output_ids[::-1]\n\n  output_tags = list(map(lambda x: tags[x], output_ids))\n\n\n  most_probable_tag_seq_dict = dict(\n      probability= np.max(probabilities[-1]).item(),\n      tag_sequence=output_tags\n  )\n\n  return(most_probable_tag_seq_dict)\nviterbi(\"I want to race\", transisition_probabilities, emission_probabilities)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Suraj_Giri_Vu_Duc_Manh.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd\nfrom itertools import product\n\n# reading the transition and emission probabilities from csv files to dataframes\ntransition_probabilities = pd.read_csv('transition_probabilities.csv', index_col=0)\nemission_probabilities = pd.read_csv('emission_probabilities.csv', delimiter=',', index_col=0)\n\ndef all_possible_tags(sentence, emission_probabilities):\n    all_possible_tags_dict = {}\n    # removing the leading and trailing spaces from the column names\n    emission_probabilities.columns = [col.strip() for col in emission_probabilities.columns]\n\n    # getting the list of all possible tags and words\n    tags = emission_probabilities.index.tolist()\n    words = emission_probabilities.columns.tolist()\n    possible_tags_words = []\n\n    # getting the list of possible tags for each word in the sentence\n    for word in sentence.split():\n        possible_tags_word = []\n        for tag in tags:\n            # we check if the word is not in the list of words or if the emission probability of the word for the tag is greater than 0\n            if (word not in words) or (emission_probabilities.loc[tag,word] > 0):\n                possible_tags_word.append(tag)\n        possible_tags_words.append(possible_tags_word)\n\n    # generating all possible tag sequences\n    all_combinations = list(product(*possible_tags_words))\n    # converting the list of tuples to a dictionary\n    all_possible_tags_dict = {str(i+1): list(combination) for i, combination in enumerate(all_combinations)}\n\n    return(all_possible_tags_dict)\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    # removing the leading and trailing spaces from the column names\n    emission_probabilities.columns = [col.strip() for col in emission_probabilities.columns]\n    transisition_probabilities.columns = [col.strip() for col in transisition_probabilities.columns]\n    words = sentence.split()\n\n    # calculating the probability of each tag sequence\n    for index, possible_tag_list in all_possible_tags_dict.items():\n        # calculating the probability of the first tag which has the initial tag as <s>\n        probability = transisition_probabilities.loc['<s>', possible_tag_list[0]] * emission_probabilities.loc[possible_tag_list[0], words[0]]\n\n        # calculating the probability of the rest of the tags\n        for i in range(1, len(possible_tag_list)):\n            transition_probability = transition_probabilities.loc[possible_tag_list[i-1], possible_tag_list[i]]\n            emission_probability = emission_probabilities.loc[possible_tag_list[i], words[i]]\n            probability = probability + (transition_probability * emission_probability)\n        all_possible_tags_probability_dict[index] = probability\n\n    # getting the most probable tag sequence\n    max_key = max(all_possible_tags_probability_dict, key=all_possible_tags_probability_dict.get)\n    most_probable_tag_seq_dict['tag_sequence'] = all_possible_tags_dict[max_key]\n    most_probable_tag_seq_dict['probability'] = all_possible_tags_probability_dict[max_key]\n\n    return(all_possible_tags_probability_dict, most_probable_tag_seq_dict)\n\nall_possible_tags_dict  = all_possible_tags('I want to race', emission_probabilities)\nprint(\"all possible tags dict: \\n\", all_possible_tags_dict)\nprint(\"possible tags probabilities and the most probable tag sequence: \\n\")\nprint(all_possible_tags_probability('I want to race', all_possible_tags_dict, transition_probabilities, emission_probabilities))"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transisition_probabilities, emission_probabilities):\n    most_probable_tag_seq_dict = {}\n    emission_probabilities.columns = [col.strip() for col in emission_probabilities.columns]\n    transisition_probabilities.columns = [col.strip() for col in transisition_probabilities.columns]\n    states =  emission_probabilities.index.tolist()\n    words = sentence.split()\n\n    return(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Mohammad_Mehdi_Balouchi.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Yagmur_Caglar.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import csv\n\ndef read_csv_column(file_path):\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file, delimiter=',')\n        columns = list(zip(*reader))\n        print(columns)\n        data = {head[0].replace(\" \", \"\") : list(head[1:]) for head in columns}\n    return data\n\ndef read_csv_row(file_path):\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file, delimiter=',')\n        data = {row[0].replace(\" \", \"\") : [prob.replace(\" \", \"\") for prob in row[1:]] for row in reader}\n    return data\n\ntransition_probabilities = read_csv_row(file_path + \"transition_probabilities.csv\")\nemission_probabilities = read_csv_row(file_path + \"emission_probabilities.csv\")\nsentence = \"I want to race\"\n\nprint(emission_probabilities)\nprint(transition_probabilities)\nfrom itertools import product\n\ndef all_possible_tags(sentence, emission_probabilities):\n\n    all_possible_tags_dict = {}\n    #Here comes your code\n\n    all_tags = list(emission_probabilities.keys())[1:]\n    len_dict = 1\n    word_combinations = product(all_tags, repeat=len(all_tags))\n\n    for comb in word_combinations:\n        all_possible_tags_dict[str(len_dict)] = list(comb)\n        len_dict += 1\n\n    return(all_possible_tags_dict)\n\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n    tag_place = {'VB': 0, 'TO': 1, 'NN': 2, 'PPSS': 3}\n\n    max_prob = 0\n\n    for k, v in all_possible_tags_dict.items():\n      prob = 1\n      prev_pos = '<s>'\n      for pos, tag in enumerate(v):\n        prob *= float(emission_probabilities[tag][pos]) * float(transition_probabilities[prev_pos][tag_place[tag]])\n        prev_pos = tag\n        if prob == 0:\n          break\n\n      all_possible_tags_probability_dict[k] = prob\n      if prob > max_prob:\n        most_probable_tag_seq_dict['tag sequence'] = v\n        most_probable_tag_seq_dict['probability'] = prob\n        max_prob = prob\n\n\n    return(all_possible_tags_probability_dict, most_probable_tag_seq_dict)\nall_possible_tags_dict = all_possible_tags(sentence, emission_probabilities)\nprint(all_possible_tags_dict)\n\n#I include all possible combinations resulting in 256 different positions. Most of them are with probability 0.\nall_possible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities)\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Yagmur_Caglar.ipynb",
        "question": "### Task 2 (7 points)\n)))\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "import numpy as np\n\ndef viterbi(sentence, transition_probabilities, emission_probabilities):\n\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    words = sentence.split()\n    num_words = len(words)\n    num_tags = len(list(emission_probabilities.keys())[1:])\n\n    viterbi_matrix = [[0.0] * num_tags for _ in range(num_words)]\n    backpointer_matrix = [[0] * num_tags for _ in range(num_words)]\n\n    #initialization step\n    for i in range(num_tags):\n        viterbi_matrix[0][i] = np.pi * float(emission_probabilities[i].get(words[0], 0.0))\n        backpointer_matrix[0][i] = 0\n\n    #recursion step\n    for t in range(2, num_words):\n        for i in range(1, num_tags):\n            max_prob = 0.0\n            max_prob_index = 0\n            for j in range(num_tags):\n                prob = viterbi_matrix[t - 1][j] * float(transition_probabilities[j].get(str(i), 0.0)) * float(emission_probabilities[i].get(words[t], 0.0))\n                if prob > max_prob:\n                    max_prob = prob\n                    max_prob_index = j\n\n            viterbi_matrix[t][i] = max_prob\n            backpointer_matrix[t][i] = max_prob_index\n\n    #termination step\n    max_prob = max(viterbi_matrix[num_words - 1])\n    max_prob_index = viterbi_matrix[num_words - 1].index(max_prob)\n\n    #backtrace to find the most probable tag sequence\n    tag_sequence = [max_prob_index]\n    for t in range(num_words - 1, 0, -1):\n        max_prob_index = backpointer_matrix[t][max_prob_index]\n        tag_sequence.insert(0, max_prob_index)\n\n    return(most_probable_tag_seq_dict)\nviterbi(sentence, transition_probabilities, emission_probabilities)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Yagmur_Caglar.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Niklas_Dobberstein.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "import pandas as pd\nimport itertools\nimport numpy as np\ndef load_probabilities(file_path):\n    probabilities_df = pd.read_csv(file_path, index_col=0)\n    return probabilities_df\n\ndef all_possible_tags(sentence, emission_probabilities):\n    words = sentence.split()\n    all_possible_tags_dict = {}\n    tags = emission_probabilities.index.tolist()\n    # NOTE: Here are also unrealistic tags sequences like VB,VB,VB,VB etc., \n    # but I understood that we should really have all possible combinations \n    all_possible = itertools.product(tags, repeat=len(words))\n    for idx, possibility in enumerate(all_possible): \n            all_possible_tags_dict[str(idx)] = possibility\n\n    return all_possible_tags_dict\n\ndef remove_spaces_and_save(input_file):\n    with open(input_file, 'r') as file:\n        content = file.read()\n\n    content_without_spaces = content.replace(' ', '')\n\n    output_file = f\"mod_{input_file}\"\n    with open(output_file, 'w') as file:\n        file.write(content_without_spaces)\n\n    print(f\"Spaces removed and data saved to {output_file}\")\n    return output_file\n\n\n\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities):\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    words = sentence.split()\n    for idx, tag_combinations in all_possible_tags_dict.items():\n        probability = 0.0\n        for i in range(0,len(tag_combinations)):\n            current_tag = tag_combinations[i]\n            if i == 0:\n                 prev_tag = \"<s>\"\n            else:\n                prev_tag = tag_combinations[i - 1]\n            transition_prob = transition_probabilities.at[prev_tag ,current_tag]\n            emission_prob = emission_probabilities.at[current_tag,words[i]] \n            # NOTE: Add an eps so that we dont have log(0)\n            probability += np.log(transition_prob + 1e-9) + np.log(emission_prob + 1e-9)\n\n        all_possible_tags_probability_dict[idx] = probability\n\n    most_probable_tag_seq_idx = max(all_possible_tags_probability_dict, key=all_possible_tags_probability_dict.get)\n    most_probable_tag_seq = all_possible_tags_dict[most_probable_tag_seq_idx]\n\n    most_probable_tag_seq_dict[\"tag_sequence\"] = most_probable_tag_seq\n    # NOTE: Convert back to prob\n    most_probable_tag_seq_dict[\"probability\"] = np.exp(all_possible_tags_probability_dict[most_probable_tag_seq_idx])\n    most_probable_tag_seq_dict[\"index\"] = int(most_probable_tag_seq_idx)\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\n\ntransition_probabilities = load_probabilities(remove_spaces_and_save(\"transition_probabilities.csv\"))\nemission_probabilities = load_probabilities(remove_spaces_and_save(\"emission_probabilities.csv\"))\nsentence = \"I want to race\"\nall_possible_tags_dict = all_possible_tags(sentence, emission_probabilities)\nall_possible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(sentence, all_possible_tags_dict, transition_probabilities, emission_probabilities)\n\nprint(\"All Possible Tag Sequences:\")\nprint(all_possible_tags_dict)\nprint(\"Tag Sequences Probability:\")\nprint(all_possible_tags_probability_dict)\nprint(\"Most Probable Tag Sequence:\")\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Niklas_Dobberstein.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transition_probabilities, emission_probabilities):\n    words = sentence.split()\n    tags = emission_probabilities.index.tolist()\n    num_words = len(words)\n    num_tags = len(tags)\n\n    # Initialization step\n    viterbi_matrix = np.zeros((num_tags, num_words))\n    backpointer_matrix = np.zeros((num_tags, num_words), dtype=int)\n    eps = 1e-9\n    for i in range(num_tags):\n        viterbi_matrix[i, 0] = np.log(transition_probabilities.at[\"<s>\", tags[i]] + eps) + np.log(emission_probabilities.at[tags[i], words[0]] + eps)\n        backpointer_matrix[i, 0] = 0\n\n    # Recursion step\n    for t in range(1, num_words):\n        for i in range(num_tags):\n            max_prob = float('-inf')\n            max_prob_idx = 0\n\n            for j in range(num_tags):\n                transition_prob = np.log(transition_probabilities.at[tags[j], tags[i]] + eps)\n                emission_prob = np.log(emission_probabilities.at[tags[i], words[t]] + eps)\n                current_prob = viterbi_matrix[j, t - 1] + transition_prob + emission_prob\n\n                if current_prob > max_prob:\n                    max_prob = current_prob\n                    max_prob_idx = j\n\n            viterbi_matrix[i, t] = max_prob\n            backpointer_matrix[i, t] = max_prob_idx\n\n    # Termination step\n    max_last_prob = float('-inf')\n    max_last_prob_idx = 0\n    for i in range(num_tags):\n        last_prob = viterbi_matrix[i, num_words - 1]\n        if last_prob > max_last_prob:\n            max_last_prob = last_prob\n            max_last_prob_idx = i\n\n    # Backtrack to find the most probable tag sequence\n    most_probable_tag_seq = [tags[max_last_prob_idx]]\n    for t in range(num_words - 1, 0, -1):\n        max_last_prob_idx = backpointer_matrix[max_last_prob_idx, t]\n        most_probable_tag_seq.insert(0, tags[max_last_prob_idx])\n\n    most_probable_tag_seq_dict = {\n        \"tag_sequence\": most_probable_tag_seq,\n        \"probability\": np.exp(max_last_prob)\n    }\n\n    return most_probable_tag_seq_dict\n\nsentence = \"I want to race\"\nmost_probable_tag_seq_dict = viterbi(sentence, transition_probabilities, emission_probabilities)\n\nprint(\"Most Probable Tag Sequence:\")\nprint(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Niklas_Dobberstein.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Carl_Jacobsen.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "def all_possible_tags(sentence, emission_probabilities):\n\n    all_possible_tags_dict = {}\n    #Here comes your code\n    #We only need here the number of words in the sentecne.\n    #Further I assume we only deal with easy senteces so.\n    length = len(sentence.split(\" \"))\n\n    #This finds all tags.\n    tags = []\n    for row in emission_probabilities:\n        tags.append(row[\"\"])\n    \n    #Now just some combinatorics to get all possible tags.\n    for i in range(len(tags)**length):\n        all_possible_tags_dict[str(i)] = [tags[i//len(tags)**k%len(tags)] for k in range(length)]\n    return(all_possible_tags_dict)\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    def probability(tags):\n        pass\n    for key in all_possible_tags_dict.keys:\n        all_possible_tags_probability[i] = []\n    return(possible_tags_probability_dict, most_probable_tag_seq_dict)\n#Just a test.\nL = []\nimport csv\nwith open(\"emission_probabilities.csv\", mode=\"r\") as file:\n    csv_file = csv.DictReader(file)\n    for x in csv_file:\n        L.append(x)\nprint(L)\nprint(all_possible_tags(\"Im a dog\", L))"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Carl_Jacobsen.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transisition_probabilities, emission_probabilities):\n\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    return(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_Carl_Jacobsen.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    },
    {
        "file_name": "Intro2NLP_Assignment_5_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 1 (6 points)\nUsing the files `transisition_probabilities.csv` and `emission_probabilities.csv`, compute the probability of POS tags for the following word sequence:\n| I | want | to | race |\n| --|------|----|------|\nImport the given files and:\n**a)** Define a function (all_possible_tags) to retrieve **ALL possible tag sequences** for the given word sequence.\n**Input:**  Sentence (\"I want to race\") and the files above.\n**Output:** All possible tag sequences. >> `all_possible_tags_dict = {\"1\": [VB, TO, NN, PPSS], \"2\": [NN, TO, VB, PPSS], \"3\": ...}`)\n**b)** Define a function (all_possible_tags_probability) to select the most probable tag sequence.\n**Input:** All possible tag sequences retrieved in (a) and the files above\n**Output:** The probability for each possible tag sequence as well as the most probable tag sequence with its probability. >> `all_possible_tags_probability_dict = {\"1\": 0.00123, \"2\": 0.00234, \"3\": ...}` and `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`\n**Notes:**\n1. The indices of the output in (b) should represent the same indices of the output in (a).\n2. The tag sequences and the probabilities, shown above in (a) and (b), are only some random examples.",
        "answer": "# read transition_probabilities.csv and emission_probabilities.csv\n\nimport pandas as pd\n\ntransisition_probabilities = pd.read_csv(\"transition_probabilities.csv\", sep=\",\", index_col=0)\nemission_probabilities = pd.read_csv(\"emission_probabilities.csv\", sep=\",\", index_col=0)\n\nprint(transisition_probabilities.head())\nprint(emission_probabilities.head())\ndef all_possible_tags_rec(sentence, emission_probabilities):\n\n    all_possible_tags_list = []\n    #Here comes your code\n\n    # get the current word\n    curr_word = sentence[0]\n\n    # get the emission probabilities of the current word\n    curr_word_emission_probabilities = emission_probabilities[curr_word]\n\n    # iterate over all tags\n    for tag in curr_word_emission_probabilities.index:\n\n        # check if the current tag has a non-zero emission probability for the current word\n        if emission_probabilities.loc[tag, curr_word] > 0:\n\n            # base case, if the sentence has only one word left\n            if len(sentence) == 1:\n                all_possible_tags_list.append([tag])\n                continue\n\n            # recursive case, if the sentence has more than one word left\n            next_seq = all_possible_tags_rec(sentence[1:], emission_probabilities)\n\n            # add current tag to the beginning of each sequence\n            for i in range(len(next_seq)):\n                next_seq[i].insert(0, tag)\n\n            # add the sequences to the list\n            all_possible_tags_list += next_seq\n\n    return all_possible_tags_list\n\n\ndef all_possible_tags(sentence, emission_probabilities):\n\n    all_possible_tags_dict = {}\n\n    # get all possible tag sequences as a list\n    all_possible_tags_list = all_possible_tags_rec(sentence, emission_probabilities)\n\n    # convert the list to a dictionary\n    for i in range(len(all_possible_tags_list)):\n        all_possible_tags_dict[i] = all_possible_tags_list[i]\n\n    return all_possible_tags_dict\n\n\ndef all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities):\n\n    all_possible_tags_probability_dict = {}\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    for i in all_possible_tags_dict.keys():\n\n        # get the current tag sequence\n        seq = all_possible_tags_dict[i]\n\n        # add <s> to the end of the sequence\n        seq.append('<s>')\n\n        # initialize probability of the sequence\n        prob_seq = 1\n\n        # calculate the probability of the sequence\n        for j in range(len(seq) - 1):\n\n            # get the current and next tag\n            curr_tag = seq[j]\n            next_tag = seq[j + 1]\n\n            # calculate the probability of the next tag\n            prob_seq *= transisition_probabilities.loc[next_tag, ' ' + curr_tag]\n\n        # add the probability of the sequence to the dictionary\n        all_possible_tags_probability_dict[i] = prob_seq\n\n    # get the most probable tag sequence\n    most_probable_tag_seq_dict[\"tag_sequence\"] = all_possible_tags_dict[max(all_possible_tags_probability_dict, key=all_possible_tags_probability_dict.get)]\n    most_probable_tag_seq_dict[\"probability\"] = all_possible_tags_probability_dict[max(all_possible_tags_probability_dict, key=all_possible_tags_probability_dict.get)]\n\n    return all_possible_tags_probability_dict, most_probable_tag_seq_dict\n\n\nsentence = [' I', \" want\", \" to \", ' race']\n\nall_possible_tags_dict = all_possible_tags(sentence, emission_probabilities)\nprint(all_possible_tags_dict)\n\nall_possible_tags_probability_dict, most_probable_tag_seq_dict = all_possible_tags_probability(sentence, all_possible_tags_dict, transisition_probabilities, emission_probabilities)\nprint(all_possible_tags_probability_dict, most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "### Task 2 (7 points)\nImplement the following algorithm from scratch, using the files given for Task 1:\nYou should define a function (viterbi) that:\n**a)** Computes the initialization step.\n**b)** Computes the recursion step.\n**c)** Computes the termination step.\n**Input:** Sentence (\"I want to race\"), transition and emission files.\n**Output:** Most probable tag sequence and its probability. >> `most_probable_tag_seq_dict = {tag_sequence: [VB, TO, NN, PPSS], probability: 0.00123}`",
        "answer": "def viterbi(sentence, transisition_probabilities, emission_probabilities):\n\n    most_probable_tag_seq_dict = {}\n    #Here comes your code\n\n    return(most_probable_tag_seq_dict)"
    },
    {
        "file_name": "Intro2NLP_Assignment_5_AlekseiZhuravlev_AffanZafar.ipynb",
        "question": "## Task 3 (7 points)\nGiven a hidden markov model with the following probabilities:\n**Transition Probabilities:**\n |            | Sunny (S) | Cloudy (C) | Rainy (R)       |\n-------------|------------|-----------|---------------\n|Sunny (S)    | 0.7       | 0.2        | 0.1   |\n|Cloudy (C)   | 0.3       | 0.5        | 0.2   |\n|Rainy (R)    | 0.1       | 0.4        | 0.5   |\n**Emission Probabilities:**\n |            | Dry (D)    | Wet (W) |\n-------------|------------|------------\n|Sunny (S)    | 0.9        | 0.1    |\n|Cloudy (C)   | 0.6        | 0.4     |\n|Rainy (R)    | 0.2        | 0.8     |\n**Initial Probability Distribution:**\nSunny (S): 0.4\nCloudy (C): 0.3\nRainy (R): 0.3\nrepresented in following graph: the colored, straight arrows represent values of Transition Matrix, the dotted arrows represent the Emission Matrix while the arrows from the start node to the hidden states (Sunny, Cloudy, Rainy) represent the initial probabilities.\n**1)** Compute the probabilities of observing the sequence Dry, Wet, Wet, Dry given this hidden state sequence Sunny, Cloudy, Rainy, Sunny.\n**2)**  How would the hidden Markov model (HMM) be affected if the emission probabilities for the observation symbols 'Dry' and 'Wet' in the Sunny state were both 0.5? Discuss the potential implications on the model's behavior and the interpretation of the observation sequence in part 1.\n**Note:**  You can type your solution directly in the following Markdown or add a handwritten image to it. Don't forget to submit the image if you choose that option.",
        "answer": ""
    }
]