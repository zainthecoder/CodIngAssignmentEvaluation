[
    {
        "question": "### Task 1.1 (3 points)\n\nWrite a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n1. num_words: The number of words in string\n2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "def extract_words_tokens(any_string):\n    \"\"\"\n    Args:\n      any_string: Python String\n\n    Returns:\n      Python String\n    \"\"\"\n    # using split method to get a list of words present in the sentence.\n    word_list = any_string.split(\" \")\n\n    #here len method returns the number of words present in the sentence.\n    num_words = len(word_list)\n\n    #here len method returns the number of characters present in the sentence.\n    num_tokens = len(any_string)\n\n    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\nextract_words_tokens(\"my name is zain ul\")"
    },
    {
        "question": "### Task 1.2 (4 points)\n\nWrite a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n\n**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters.",
        "answer": "def lemmatize(any_string, file_name):\n\n    vocab = dict() # declaring our vocabulary of words and lemmas with words as keys and lemma as values\n    dictionary_of_lemmatized_words = dict() # a map to store the lemmatized words\n\n    with open(file_name, 'r',encoding='utf-8-sig') as file:\n      for line in file:\n        key,value = line.strip().split('\\t') # stripping(removing any extra spaces at the beginning or end) and splitting the line on the basis of tab\n        vocab[value.lower()] = key.lower() # storing the word (lowercase) as key and lemma(lowercase) as value\n\n    for string in any_string.split(' '):\n      if string.lower() in vocab: # checking whether the word of a sentence(in lowercase) is present in the lemma vocabulary\n        dictionary_of_lemmatized_words[string] = vocab[string.lower()] # if present, assigned lemma of it\n      else:\n        dictionary_of_lemmatized_words[string] = string # else the word is lemma itself\n\n    return(print(dictionary_of_lemmatized_words))\nlemmatize('The striped bats are hanging on their feet for best','lemmatization-en.txt')"
    },
    {
        "question": "### Task 1.3 (3 points)\n\nWrite a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\n\nCreate rules for the following forms of the verbs, Here is one example:\n\n- (Infinitive form) >> study - studi\n- (Present simple tense: Third person) >> studies - studi\n- (Continuous tense) >> studying - studi\n- (Past simple tense) >> studied - studi\n\n**Hint:** The string can be a single word or a sentence and\n can contain some special charecters, such as: \"!\", \",\", \":\"",
        "answer": "import re\n\ndef stemmer(sentence):\n    # Define rules for stemming\n    rules = [\n        (r'y$', r'i'),             # Infinitive form\n        (r'ies$', r'i'),           # Present simple tense: Third person\n        (r'ying$', r'i'),           # Continuous tense\n        (r'ing$', r'i'),           # Continuous tense\n        (r'ied$', r'i'),            # Past simple tense\n    ]\n\n    # Split the sentence into words\n    words =  re.split(r'[,\\s!:]', sentence) # split on the basis of ' ','!',',',':'\n\n    # Apply rules to each word\n    stemmed_words = []\n    for word in words:\n      if len(word) == 0: # incase of empty strings after splitting\n        continue\n      for pattern, replacement in rules:\n          if len(word) <= 3: # assuming that words with length less than 3 are already stem\n            continue\n          word = re.sub(pattern, replacement, word)\n      stemmed_words.append(word)\n\n    # Join the stemmed words back into a sentence\n    stemmed_string = ' '.join(stemmed_words)\n\n    return(print(stemmed_string))\nstemmer(\"My boy is not studying and sleeping\")\nstemmer(\"My boy is eating\")"
    }
]