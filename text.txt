System Design: https://drive.google.com/file/d/1C4CK6erBh2rlCJuWjYT4SEuU6klwJrvv/view?usp=sharing


Objective:
We need to evaluate assignment using LLM.

Given Data:
We have assignments.

Method:
- Clean the data
- Think about model part


Reading about the implementation of CodeJudge:
1. Use prompting
    zero shot
    few shot

the prompts are present in evaluation/humaneval/prompts.py


A Survey on Evaluating Large Language Models in Code Generation Tasks
https://arxiv.org/pdf/2408.16498

- Generate Test Cases using LLM to evaluate the code.



Using LLMs for Evaluation
https://cameronrwolfe.substack.com/p/llm-as-a-judge

“While human evaluation is the gold standard for assessing human preferences, it is exceptionally slow and costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4, as a surrogate for humans.” - from [17]

- Fintuning LLM is also a options
- Several sources of bias exist that make this metric imperfect, so we should be sure to use LLM-as-a-Judge in tandem with human evaluation.
- Pairwise Scoring
- Pointwise Scoring
- COT, few shot
- low temperature is important for deterministic resutls


To Conclude:
Objective: We need to evaluate code using LLM.
    Need to think what novelty we are adding as evaluation of code using LLM is already been done.
    
Objective: Memory maintainance: as questions are inter linked 
    The complexity in extraction of answers is not contributing to the research question we have.
    Is the assignments right choice for our research question.

